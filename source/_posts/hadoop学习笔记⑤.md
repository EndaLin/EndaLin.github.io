---
title: Hadoop学习笔记⑤
copyright: true
date: 2019-06-29 09:21:33
tags: Hadoop
---

# 云计算

云计算是分布式计算、并行计算、效用计算、网络存储、虚拟化、负载均衡等计算机和网络技术发展融合的产物。

云计算主要包括3种类型： IaaS、PaaS 和SaaS。


# 云数据库

云数据库是部署和虚拟化在云计算环境中的数据库。

# MapReduce

思想: 分而治之， 把一个大的数据块拆分为多个小的数据块在不同的机器上并行处理。

<!--more-->

Map - Reduce 任务： 通常运行为数据存储节点上， 由此可使计算任务和数据存储在同一个节点之上， 无需额外的数据传输开销， 当Map 任务结束之后， 会生成以<K, V> 形式的中间结果， 然后分配给多个Reduce 任务去执行， 具有相同的Key 的任务会被分配到同一个Reduce 任务， Reduce 任务会对中间结果进行处理， 得到最终结果， 然后输出的分布式配置文件中。

注意： 不同的Map 任务之间以及不同的Reduce 任务之间是不会发生任何的信息交换。

#### 工作流程

- 首先使用InputFormat 模块做Map 前的预处理， 然后将文件切分为**逻辑上多个InputSplit**， InputSplit 是Map Reduce 对文件进行处理和运算的处理单位， 只是一个逻辑概念， 并没有进行实际的切分， 只是记录了需要处理的数据的位置和长度。

- 由于InputSplit 是逻辑分而不是物理分， 所以还需要通过RecordReader 根据InputSplit 中的信息来处理InputSplit 中的具体信息， 加载数据并转换成适合Map 任务读取的键值对.

- Map 任务会根据用户自定义的映射规则， 输出一系列的<K, V> 作为中间结果

- 在将Map 任务输出的中间结果交给Reduce 任务处理之前， 需要经过**shuffle** 处理， 从无序的<key, value> 到有序的<key, value-list>。

- Reduce 以一系列<key, value-list> 作为输入， 按照用户定义的逻辑， 将处理结果输出到OutFormat 模块。

- OutputFormat 会验证输出目录是否存在, 或者输出类型是否符合配置文件中的配置类型, 如果都满足则输出到HDFS 中.

![image.png](https://upload-images.jianshu.io/upload_images/13918038-ad73fa8729eb659d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### Shuffle 过程详解
Shuffle 是整个MapReduce 的核心， 对Map 输出结果进行分区、排序以及合并等处理并交给Reduce 的过程， Shuffle 分为Map 端操作和Reduce 端操作。


**在Map 端的Shuffle 过程**

每个Map 任务都会被分配到一个缓存， 默认大写是100M


Map 的输出结果会暂时被写入缓存中， 当缓存满时， 会启动溢写操作， 把缓存中的数据进行分区， 并对同一个分区里面的数据进行合并（合并是指对有着相同Key 值的数据加起来， 以减少需要溢写到磁盘的数据量）， 然后写入磁盘文件（此举可以减少对磁盘的I/O 操作）， 并清空缓存， 每次溢写操作， 都会生成一个溢写文件， 当Map 任务结束之后， 这些溢写文件就会被合并成一个大的磁盘文件(此时会对所有的数据进行合并操作)， 最终会生成一个统一的大文件， 但是这个大文件是被分区的， 然后通知Reduce 来领取属于自己的处理数据， 不同分区的数据会被分发到不同的Reduce 上执行。

为了使得Map 的写入操作可以持续进行， 需要让缓存留有一定的空间， 而不是等缓存满了之后在进行溢写操作。此处会有一个溢写比例。

合并例子： <k1, v1> 与<k1, v2> 合并成<k1, <v1, v2>>

**在Reduce 端的Shuffle 过程**
Reduce 任务从Map 端的不同机器领取属于自己的那一份数据， 然后对数据进行合并， 接着进行处理。

两个流程：
- 领取数据： Reduce会开多个线程去向多个Map 任务所在的机器去领取数据
- 归并数据： 从Map 端领取回来的数据会先保存在缓存中， 如果缓存占满了， 会触发溢写操作（此时也会对数据进行合并）， 将数据写到磁盘中， 每次溢写都会产生一个溢写文件， 最后当所有数据都被领取完毕之后， 就会对所有的溢写文件进行归并排序
- 将数据输入到Reduce 任务：经过多轮归并会生成多个大文件（Map 端是一个）， 这几个大文件不会继续归并成一个新的大文件， 而是直接输入到Reduce 任务中， 接下来Reduce 任务会执行操作， 将处理结果输出到HDFS 中。
