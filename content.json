{"meta":{"title":"Enda Lin","subtitle":"keep foolish, keep sharp","description":"所有的伟大都是从零开始","author":"Enda Lin","url":"https://wt-git-repository.github.io"},"pages":[{"title":"404","date":"2019-03-06T09:36:02.000Z","updated":"2019-07-08T01:15:52.653Z","comments":true,"path":"404/index.html","permalink":"https://wt-git-repository.github.io/404/index.html","excerpt":"","text":""},{"title":"All categories","date":"2019-03-06T09:18:23.000Z","updated":"2019-07-08T01:15:52.740Z","comments":true,"path":"categories/index.html","permalink":"https://wt-git-repository.github.io/categories/index.html","excerpt":"","text":""},{"title":"All tags","date":"2019-03-06T09:16:39.000Z","updated":"2019-07-08T01:15:52.741Z","comments":true,"path":"tags/index.html","permalink":"https://wt-git-repository.github.io/tags/index.html","excerpt":"","text":""},{"title":"Enda Lin","date":"2019-03-06T09:36:26.000Z","updated":"2019-10-02T12:29:46.373Z","comments":true,"path":"about/index.html","permalink":"https://wt-git-repository.github.io/about/index.html","excerpt":"","text":"13727782882 &nbsp;&nbsp;&nbsp;endawt.lin@gmail.com广东省东莞市松山湖高新技术产业开发区大学路1号 求职意向: Java 开发工程师教育背景 2016年 - 2020年&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;东莞理工学院&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本科(一本专业)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;软件工程(卓越计划班) 在校成绩 专业绩点稳定班级前十, 曾夺绩点专业第一 个人技能 熟悉Java， 了解Python 和C\\C++ 熟悉在Linux环境下编程，熟悉使用Docker基本命令，熟悉编写Dockerfile与docker-compose.yml文件 熟悉常用的数据结构与算法 熟悉Mysql基本操作， 了解Oracle 熟悉Spring、Spring MVC、MyBatis等常用框架，了解消息队列等其它框架 熟悉Eureka、Zuul、Spring boot admin、Ribbon、 Hystrix等微服务框架的常用配置 熟悉使用Nginx 反向代理与负载均衡 熟悉git、GitHub、Gitlab的使用, 了解SVN 了解Hadoop 、Spark 等大数据技术 工作与实习经历2019年7月8日-2019年9月1日&nbsp;&nbsp;&nbsp;&nbsp;深圳市乐融软件技术有限公司&nbsp;&nbsp;&nbsp;&nbsp; Java开发实习生，技术部 独立完成门禁项目前置系统的开发，使用Netty 搭建网络通信框架，通过TCP 长连接与设备进行通信，完成心跳检测、反控、名单下发，记录传输、设置开门时间、设置设备参数等功能 参加项目评审会议，参与对门禁系统原型图的设计和系统业务流程的设计 负责与设备厂商对接，商定设备与门禁前置系统的通信协议 在校项目经验2019年9月10日 - 至今&nbsp;&nbsp;&nbsp;&nbsp;学生网络行为画像监测系统&nbsp;&nbsp;&nbsp;&nbsp;系统开发主要负责人描述： 基于大数据分析技术对学生上网数据进行分析和挖掘，建立学生网络行为的相关模型和行为标签 通过分析上网数据对学生网络行为进行画像，为学生管理工作提供决策支撑，促进学生管理与决策的科学化 2019年1月20日-2019年5月30日&nbsp;&nbsp;&nbsp;&nbsp;东莞理工学院心理测评系统&nbsp;&nbsp;&nbsp;&nbsp;系统开发主要负责人职责: 负责这个系统的开发流程，保证系统的开发进度 使用Spring、Spring MVC、Druid、MyBatis以及Redis搭建后台框架，对用户填写的问卷数据进行清洗，并将分析的结果返回给用户 基于Websocket实现前后端的实时通讯，让用户可以及时地获知问卷分析的进度 基于Druid搭建数据库连接池以及系统性能的监控平台 使用Docker将项目部署到CentOS7 服务器中 使用Redis对热点数据进行缓存，提高系统并发量和访问速度 使用JWT 对用户进行身份认证 2018年3月20日-2018年5月20日&nbsp;&nbsp;&nbsp;&nbsp;基于人脸识别的考勤系统&nbsp;&nbsp;&nbsp;&nbsp;后端开发主要负责人职责: 负责技术开发相关的总体事项，保证项目的进度 基予Servlet实现后端功能，为小程序提供人脸识别、基于人脸数据去定位学生信息、自动录入并更新考勤数据、录入课程表、录入学生脸部等一系列功能 搭建WEB后台管理，对考勤数据、学生信息管理、班级管理等提供一系列的支持 接入百度人脸识别API提供人脸识别技术 获奖经历（算法类） 2019年第十届蓝桥杯全国总决赛Java程序设计二等奖 2019年第十届蓝桥杯广东省赛区Java程序设计一等奖 2018年度软件工程系”微三云杯”程序设计竞赛(算法类)一等奖（JAVA） 2018年第九届蓝桥杯广东省赛区C/C++程序设计二等奖 2017年第八届蓝桥杯广东省赛区C/C++程序设计三等奖 相关主页 github（https://github.com/wt-git-repository） 个人博客（https://wt-git-repository.github.io/） 自我评价 参加过多个项目的开发，熟悉常见的业务处理和开发流程，能够快速地融入团队 逻辑思维能力强，自学能力强，对新技术有着强烈的好奇心 具有良好的英语阅读能力，能够阅读相关英文文档"}],"posts":[{"title":"操作系统面试题之进程与线程相关","slug":"操作系统面试题之进程与线程相关","date":"2019-11-14T06:53:35.000Z","updated":"2019-11-14T07:38:21.882Z","comments":true,"path":"2019/11/14/操作系统面试题之进程与线程相关/","link":"","permalink":"https://wt-git-repository.github.io/2019/11/14/操作系统面试题之进程与线程相关/","excerpt":"","text":"进程（线程 + 内存 + 文件/网络句柄） 内存：逻辑内存 文件/网络句柄：为所有线程所共有 线程：进程所创建的，进程是线程的容器 孤儿进程指的是父进程死掉的子进程，这些孤儿进程会被init进程（进程号为1）所收养，由init 进程对它们完成状态收集工作 僵尸进程一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 wait() 或 waitpid() 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 wait() 或 waitpid()，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。 线程（栈 + PC + TLS） 栈 PC（程序计数器）：PC就是指向当前的指令，而这个指令是放在内存中。 每个线程都有一串自己的指针，去指向自己当前所在内存的指针 TLS（Thead Local Storage）: 存储线程所独有的数据 进程与线程的区别 进程是资源调度的最小单位，线程是系统调用的最小单位 进程有自己独立的内存空间，线程可以共享进程的内存空间 死锁原因 系统资源不足 进程推进的顺序不合适 资源分配不当 导致死锁的四个必要条件 一个进程一次只能访问一个资源，其它进程不能访问已经分配的资源 一个进程在等待其它资源的时候，继续占有已经抢占到的资源 一个进程不能强行抢占另一个进程的资源 存在一个封闭的进程链，导致每一个进程都在等待下一个进程的资源 银行家算法","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://wt-git-repository.github.io/tags/操作系统/"}]},{"title":"操作系统面试题之存储器相关","slug":"操作系统面试题之存储器相关","date":"2019-11-06T06:39:19.000Z","updated":"2019-11-13T07:53:49.608Z","comments":true,"path":"2019/11/06/操作系统面试题之存储器相关/","link":"","permalink":"https://wt-git-repository.github.io/2019/11/06/操作系统面试题之存储器相关/","excerpt":"存储器的层次结构寄存器、高速缓存、主存储器、磁盘缓存、固定磁盘、可移动的存储介质 逻辑地址和物理地址用户能看到的程序代码、变量、堆找的地址称之为逻辑地址，它们所在的实际内存区域实际地址是物理地址 将逻辑地址和物理地址分开的好处 实现内存隔离 实现进程和内核保护 内核空间与用户空间内核空间： 程序地址空间中安排给操作系统使用的部分，又称之为虚拟存储器 用户空间： 程序地址空间中安排给用户程序使用的部分，又称为用户虚拟存储器 分页存储管理和页表 把装入模块（即链接后的可执行程序）切分为一系列等长的片段，每个片段称为一个页面(页面长度为512B、1KB、2KB、4KB等，小于动态分区的很多碎片），页号依次为0、1、2、…，(具有n-1个页面的进程大小是多少字节) 也把系统物理内存划分为一系列等长的片段，每个片段称为一个物理块（或一个页帧），物理块长度与页面长度相同,块号为0、1、2、… 用户程序运行时，操作系统将其各个页面装载到一系列任意的物理块（地址可能不连续）","text":"存储器的层次结构寄存器、高速缓存、主存储器、磁盘缓存、固定磁盘、可移动的存储介质 逻辑地址和物理地址用户能看到的程序代码、变量、堆找的地址称之为逻辑地址，它们所在的实际内存区域实际地址是物理地址 将逻辑地址和物理地址分开的好处 实现内存隔离 实现进程和内核保护 内核空间与用户空间内核空间： 程序地址空间中安排给操作系统使用的部分，又称之为虚拟存储器 用户空间： 程序地址空间中安排给用户程序使用的部分，又称为用户虚拟存储器 分页存储管理和页表 把装入模块（即链接后的可执行程序）切分为一系列等长的片段，每个片段称为一个页面(页面长度为512B、1KB、2KB、4KB等，小于动态分区的很多碎片），页号依次为0、1、2、…，(具有n-1个页面的进程大小是多少字节) 也把系统物理内存划分为一系列等长的片段，每个片段称为一个物理块（或一个页帧），物理块长度与页面长度相同,块号为0、1、2、… 用户程序运行时，操作系统将其各个页面装载到一系列任意的物理块（地址可能不连续） 进程页号与物理页号的管理页表：通常使用数组来组织页表，页表并不保存页号，只保存物理块号。 在分页系统中，地址转换的流程 从逻辑地址中提取页号 根据页号，查询页表，找到对应的物理块号 用物理块号和块内的地址定位到对应的数据 提高地址转换效率由于页表保存在内存中，所以1 次内存存取的操作需要两次访存开销（一次查页表，一次读取内存本身）， 会降低系统效率 解决方案： 将经常使用的页表项放到快表 中，快表比内存快5 倍以上 MMU 执行地址转换时，在快表和页表中查找物理块号，若快表命中，则立即获取物理块号，若快表不命中，再到主存页表中查找 由于程序具有局部性特征，设置少量快表项，即可大幅提高地址转换速度 空闲页表管理 位示图法 链表法 多级页表法 分段存储管理基本思想 把程序按照内容或者函数分成段，每段有自己的名字，每段各有一个连续的地址空间 段式管理可以将那些经常访问的段驻留在内存，将那些不常访问到的放到外存中 段管理为了能找到每个逻辑段对应的物理内存的中分区的位置，系统为每个进程建立了一个段映射表，简称“段表” 每个段在段表中均有一项，段表项中包括如下内容：段号、段的长度、段在内存中的起始地址 段页式存储管理方式地址空间划分：作业的地址空间仍按其逻辑结构分段。每个段又被进一步分成若干大小相同的页面。内存空间则分成与页面大小相等的物理块 虚拟存储器程序运行特性分析大型复杂程序通常由很多模块构成，其运行过程有一些特性：有时一次执行过程仅少量模块被调用执行(如powerpoint、matlab、QQ)，或某段时间内仅少量模块被调用执行(如ppt文字录入时)，把大量没有用到的程序代码载入主存，是一种浪费。 某段时间内仅将部分部分代码、数据模块载入内存，可提高内存利用率，缓解内存不足的问题，传统方法有： 覆盖(重叠，overlay)：将不会同时执行的模块装载到同一个内存块，但需预先给出模块间调用关系，增加程序员负担。 动态加载(dynamic loading)：不常用的程序模块以动态链接库(.DLL，.so)方式保存在外存，在需要调用它们的地方用动态装载函数(LoadLibrary, dlopen)加载，虽然可以实现模块共享，但大量的使用这些函数也给程序员带来负担。 最好是把这些模块选择与加载工作全部交给操作系统：由OS根据系统空闲内存状况，灵活决定将哪些模块加载到内存，哪些模块留在外存，并自动完成模块加载工作。 虚拟存储器的基本思想 把辅存的一部分拿来虚拟成实体存储器 部分加载方法：最近需要访问的代码、需要访问的数据加载到物理内存中，其余数据保存在虚拟存储器中（外存） 虚拟存储器技术通常与分页式存储管理（paging)或分段式存储管理联合使用 请求分页系统存储管理 应用程序在运行之前，仅须将那些当前要运行的少数页面或段，先装入内存便可运行，其余部分暂留在外存（虚拟存储器）上。 程序在运行时，如果它所要访问的页已调入内存，便可继续执行；但如果程序所要访问的页尚未调入内存（称为缺页），便发出缺页中断请求，由OS利用指定页面调入内存。 若此时内存已满，无法再装入新页，OS应将内存中暂时不用的页调至外存（虚拟存储器）上，腾出页帧后，再所需页面调入内存（页面置换），使程序得以继续执行 页表修改状态为P:判断页表是在内存中还是在外存中 缺页中断处理过程 页面置换算法请求分页存储系统可用内存不足时，若调入新的页面，就需选择一个内存中被替换页面，将新页面的内容写入该页面所在物理块。 最佳置换算法OPT： 总是选择以后永不使用的，或许是在最长(未来)时间内不再被访问的页面，进行替换 先进先出页面置换算法（FIFO）：当需要调入新的页面，但无可用空闲物理块时，总是选择在内存中驻留时间最长的页并予以淘汰，用腾出的物理块来载入新的页面先进先出算法没有考虑程序执行的局部性特征时间局部性：如果程序中的某条指令一旦执行，则不久以后该指令可能再次执行；如果某数据被访问过，则不久以后该数据可能再次被访问，像循环结构空间局部性：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，象数组结构 LRU 置换算法（淘汰最久未使用的页面）据程序局部性原理，认为程序最近一段时间内经常访问的地址，也会是不久将来经常访问的地址，认为最后一次访问时间距离当前时间最久的页面，是不久将来访问可能性比较小的页面。硬件对该算法的支持：移位寄存器、堆栈法、计数器法 虽然LRU算法较好，但要求较多的硬件支持，实现代价（页表存储开销、地址转换过程开销）高，阻碍其应用推广。实际应用中大多采用性能下降不大但实现代价小的LRU近似算法，Clock置换算法是典型代表 Clock 置换算法","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://wt-git-repository.github.io/tags/操作系统/"}]},{"title":"Java面试题之树相关","slug":"Java面试题之树相关","date":"2019-11-06T02:31:02.000Z","updated":"2019-11-06T06:35:52.889Z","comments":true,"path":"2019/11/06/Java面试题之树相关/","link":"","permalink":"https://wt-git-repository.github.io/2019/11/06/Java面试题之树相关/","excerpt":"","text":"B 树binary search tree，称为二叉搜索树、二叉排序树或者二叉查找树，简称BST B 树，即二叉搜索树 所有非叶子结点都有左右子节点 所有结点都会存储一个关键字 非叶子节点的左子节点小于其关键字，右子节点大于其关键字 AVL 树 AVL 树是自平衡二叉查找树，在AVL 树中，任何节点的两个子树的高度最大差别为1，查找、插入和删除的平均和最坏情况都是O(log n) B+ 树特点 所有关键字都出现在叶子节点中，叶子节点和叶子节点之间是以链表的方式来存储的 叶子节点是有序的 红黑树红黑树是满足以下性质的二叉搜索树 每个结点是红色的或者黑色的 根结点是黑色的 如果结点是红色的，其左右叶子结点都是黑色的 保证了从根节点到叶子节点的最长路径的长度不会超过任何其他路径的两倍 每个叶子结点都是黑色的 对于每个结点，从该结点到其所有子孙叶结点的路径中所包含的黑色结点数量必须相同。 红黑树和B 树的应用场景 红黑树多用在内部排序，即全放在内存中的，STL的map和set的内部实现就是红黑树。 B+树多用于外存上时，B+也被成为一个磁盘友好的数据结构。 红黑树并不追求“完全平衡”——它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能 为什么用二叉树 而不用散列表 第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在O(n)的时间复杂度内，输出有序的数据序列。 第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)。 第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比logn小，所以实际的查找速度可能不一定比O(logn)快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。 第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。 参考 B树、B+树、AVL树、红黑树 对B+树，B树，红黑树的理解","categories":[],"tags":[{"name":"树","slug":"树","permalink":"https://wt-git-repository.github.io/tags/树/"}]},{"title":"Java面试题之JVM相关","slug":"Java面试题之JVM相关","date":"2019-10-29T09:24:09.000Z","updated":"2019-11-05T02:04:37.968Z","comments":true,"path":"2019/10/29/Java面试题之JVM相关/","link":"","permalink":"https://wt-git-repository.github.io/2019/10/29/Java面试题之JVM相关/","excerpt":"JVM 堆在Java 中，堆被划分为两个不同的区域：新生代和老年代 新时代：被划分为三个区域：Eden、From survivor 和 To survivor 划分的目的是为了使JVM 更好地管理JVM 内存对象，包括内存的分配以及内存的回收 参考Java GC、新生代、老年代 GCGC RootsGC Roots 是tracing GC 的根集合，是一组必须活跃的应用 通过一系列名为“GC roots” 的对象为起始点，从这个被成为“GC roots” 的对象向下搜索，如果一个对象到GC roots 没有任何的引用链相连，则说明该对象不可用。也就是说，以给定的一个集合的引用作为根出发点，通过引用关系遍历对象图，能被遍历到的（可到达的对象）则被判定为存活，不可达的则被判定为死亡。 GC roots set 虚拟机栈（栈帧中的局部变量表）中应用的对象 方法区中的类静态属性应用的对象 方法区中常量引用的对象 本地方法栈中JNI (Native 方法) 应用的对象 四大垃圾回收算法 引用计数 复制拷贝 标记-清除算法（Mark-Sweep） （优：节约空间，缺：产生内存碎片） 标记 - 压缩 主要的垃圾回收器下图展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。Hotspot实现了如此多的收集器，正是因为目前并无完美的收集器出现，只是选择对具体应用最适合的收集器。 串行垃圾回收器（-XX:+UseSerialGC） 串行垃圾回收器通过持有应用程序所有的线程进行工作。它为单线程环境设计，只使用一个单独的线程进行垃圾回收，通过冻结所有应用程序线程进行工作，所以可能不适合服务器环境。它最适合的是简单的命令行程序（单CPU、新生代空间较小及对暂停时间要求不是非常高的应用）。是client级别默认的GC方式。 开启之后会使用，新时代和老年代都会使用串行回收收集器，新生代使用标记-复制算法，老年代使用标记-整理算法 ParNew 收集器（-XX:+UseParNewGC） 只对新生代起作用ParNew 回收器也要冻结用户的工作线程，在新时代中使用多线程来运行标记-复制算法来进行垃圾回收，在老年期还是使用单线程和标记-整理算法来进行垃圾回收注意： ParNew 和SerialOld 已经不再被推荐使用备注：-XX:ParallelGCThreads 限制线程数量，默认起与CPU 相同数目的线程 Parallel Scavenge 收集器（-XX:+UseParallelGC） 它是JVM的默认垃圾回收器。与串行垃圾回收器不同，它使用多线程进行垃圾回收。相似的是，当执行垃圾回收的时候它也会冻结所有的应用程序线程。新生代和老年代都用多个线程并行去运行 CMS并发标记扫描垃圾回收器（-XX:+UseConcMarkSweepGC） 用户线程和垃圾回收线程同时执行，适用于对响应时间有要求的公司只要在老年代开启了CMS，新生代会自动开启ParNew，Serial Old会作为CMS 出错的后备收集器 四个步骤: 初始化标记Initial Mark（冻结用户线程）：只是标记一下GC Roots能直接关联的对象，速度很快 并发标记Concurrent Mark（GC 线程和用户线程一起工作）：进行GC Roots跟踪过程，这是主要的标记过程，标记所有的对象 重新标记Remark（冻结用户线程）：为了修正正在并发标记期间，因用户程序继续运行而导致标记发生变动的那一部分对象的标记记录，任然需要暂停所有工作线程。（二次确认） 并发清除Concurrent sweep（GC 线程和用户线程一起工作） G1垃圾回收器（-XX:+UseG1GC） G1垃圾回收器适用于堆内存很大的情况，他将堆内存分割成不同的区域，并且并发的对其进行垃圾回收,不会产生许多内存碎片，G1 在停顿时间添加了预测机制，用户可以指定停顿时间G1 特点 G1 能充分利用CPU，多核环境的优势，尽量缩短STW G1 整体采用标记-整理算法，局部通过复制算法，不会产生内存碎片 宏观上看G1 不再区分年轻代和老年代，把内存划分为多个Region 区 G1 收集器在小范围内进行年轻代和老年代的区分，不再是物理隔离，是一部分Region 的集合而且不需要连续 如何选择合适的垃圾收集器 单CPU 或者小内存，单机程序 -XX:+UseSerialGC 多CPU，需要最大的吞吐量，如后台计算型应用 -XX:+UseParallelGC or -XX:+UseParallelOldGC 多CPU，追求低停顿时间，需要快速响应 -XX:+UseConcMarkSweepGC -XX:+ParNewGC 查看默认的垃圾回收器1234567F:\\IDEA\\Test&gt;java -XX:+PrintCommandLineFlags -version-XX:G1ConcRefinementThreads=8 -XX:GCDrainStackTargetSize=64 -XX:InitialHeapSize=267006528 -XX:MaxHeapSize=4272104448 -XX:+PrintCommandLineFlags -XX:ReservedCodeCacheSize=251658240 -XX:+SegmentedCodeCache -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC -XX:-UseLargePagesIndividualAllocationopenjdk version \"11.0.3\" 2019-04-16OpenJDK Runtime Environment (build 11.0.3+12-b304.10)OpenJDK 64-Bit Server VM (build 11.0.3+12-b304.10, mixed mode, sharing) 结果：-XX:+UseG1GC 概念补充 可控制的吞吐量 = 运行用户代码的时间/（运行用户代码的时间+垃圾收集器），高吞吐量意味着利用CPU 的效率高 参考浅析JAVA的垃圾回收机制（GC） 深入理解JVM(3)——7种垃圾收集器 JVM的垃圾回收机制 总结(垃圾收集、回收算法、垃圾回收器)","text":"JVM 堆在Java 中，堆被划分为两个不同的区域：新生代和老年代 新时代：被划分为三个区域：Eden、From survivor 和 To survivor 划分的目的是为了使JVM 更好地管理JVM 内存对象，包括内存的分配以及内存的回收 参考Java GC、新生代、老年代 GCGC RootsGC Roots 是tracing GC 的根集合，是一组必须活跃的应用 通过一系列名为“GC roots” 的对象为起始点，从这个被成为“GC roots” 的对象向下搜索，如果一个对象到GC roots 没有任何的引用链相连，则说明该对象不可用。也就是说，以给定的一个集合的引用作为根出发点，通过引用关系遍历对象图，能被遍历到的（可到达的对象）则被判定为存活，不可达的则被判定为死亡。 GC roots set 虚拟机栈（栈帧中的局部变量表）中应用的对象 方法区中的类静态属性应用的对象 方法区中常量引用的对象 本地方法栈中JNI (Native 方法) 应用的对象 四大垃圾回收算法 引用计数 复制拷贝 标记-清除算法（Mark-Sweep） （优：节约空间，缺：产生内存碎片） 标记 - 压缩 主要的垃圾回收器下图展示了7种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用。虚拟机所处的区域，则表示它是属于新生代收集器还是老年代收集器。Hotspot实现了如此多的收集器，正是因为目前并无完美的收集器出现，只是选择对具体应用最适合的收集器。 串行垃圾回收器（-XX:+UseSerialGC） 串行垃圾回收器通过持有应用程序所有的线程进行工作。它为单线程环境设计，只使用一个单独的线程进行垃圾回收，通过冻结所有应用程序线程进行工作，所以可能不适合服务器环境。它最适合的是简单的命令行程序（单CPU、新生代空间较小及对暂停时间要求不是非常高的应用）。是client级别默认的GC方式。 开启之后会使用，新时代和老年代都会使用串行回收收集器，新生代使用标记-复制算法，老年代使用标记-整理算法 ParNew 收集器（-XX:+UseParNewGC） 只对新生代起作用ParNew 回收器也要冻结用户的工作线程，在新时代中使用多线程来运行标记-复制算法来进行垃圾回收，在老年期还是使用单线程和标记-整理算法来进行垃圾回收注意： ParNew 和SerialOld 已经不再被推荐使用备注：-XX:ParallelGCThreads 限制线程数量，默认起与CPU 相同数目的线程 Parallel Scavenge 收集器（-XX:+UseParallelGC） 它是JVM的默认垃圾回收器。与串行垃圾回收器不同，它使用多线程进行垃圾回收。相似的是，当执行垃圾回收的时候它也会冻结所有的应用程序线程。新生代和老年代都用多个线程并行去运行 CMS并发标记扫描垃圾回收器（-XX:+UseConcMarkSweepGC） 用户线程和垃圾回收线程同时执行，适用于对响应时间有要求的公司只要在老年代开启了CMS，新生代会自动开启ParNew，Serial Old会作为CMS 出错的后备收集器 四个步骤: 初始化标记Initial Mark（冻结用户线程）：只是标记一下GC Roots能直接关联的对象，速度很快 并发标记Concurrent Mark（GC 线程和用户线程一起工作）：进行GC Roots跟踪过程，这是主要的标记过程，标记所有的对象 重新标记Remark（冻结用户线程）：为了修正正在并发标记期间，因用户程序继续运行而导致标记发生变动的那一部分对象的标记记录，任然需要暂停所有工作线程。（二次确认） 并发清除Concurrent sweep（GC 线程和用户线程一起工作） G1垃圾回收器（-XX:+UseG1GC） G1垃圾回收器适用于堆内存很大的情况，他将堆内存分割成不同的区域，并且并发的对其进行垃圾回收,不会产生许多内存碎片，G1 在停顿时间添加了预测机制，用户可以指定停顿时间G1 特点 G1 能充分利用CPU，多核环境的优势，尽量缩短STW G1 整体采用标记-整理算法，局部通过复制算法，不会产生内存碎片 宏观上看G1 不再区分年轻代和老年代，把内存划分为多个Region 区 G1 收集器在小范围内进行年轻代和老年代的区分，不再是物理隔离，是一部分Region 的集合而且不需要连续 如何选择合适的垃圾收集器 单CPU 或者小内存，单机程序 -XX:+UseSerialGC 多CPU，需要最大的吞吐量，如后台计算型应用 -XX:+UseParallelGC or -XX:+UseParallelOldGC 多CPU，追求低停顿时间，需要快速响应 -XX:+UseConcMarkSweepGC -XX:+ParNewGC 查看默认的垃圾回收器1234567F:\\IDEA\\Test&gt;java -XX:+PrintCommandLineFlags -version-XX:G1ConcRefinementThreads=8 -XX:GCDrainStackTargetSize=64 -XX:InitialHeapSize=267006528 -XX:MaxHeapSize=4272104448 -XX:+PrintCommandLineFlags -XX:ReservedCodeCacheSize=251658240 -XX:+SegmentedCodeCache -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseG1GC -XX:-UseLargePagesIndividualAllocationopenjdk version \"11.0.3\" 2019-04-16OpenJDK Runtime Environment (build 11.0.3+12-b304.10)OpenJDK 64-Bit Server VM (build 11.0.3+12-b304.10, mixed mode, sharing) 结果：-XX:+UseG1GC 概念补充 可控制的吞吐量 = 运行用户代码的时间/（运行用户代码的时间+垃圾收集器），高吞吐量意味着利用CPU 的效率高 参考浅析JAVA的垃圾回收机制（GC） 深入理解JVM(3)——7种垃圾收集器 JVM的垃圾回收机制 总结(垃圾收集、回收算法、垃圾回收器) JVM查看当前运行程序的配置12345jinfo -flag 配置项 进程编号orjinfo -flags XX 参数 Boolean 类型：(+ or - ) 某参数 case: 是否打印GC 收集细节：-XX:+PrintGCDetails 123456&gt; F:\\IDEA\\Test\\src&gt;jps -l&gt; 2380 Test&gt;&gt; F:\\IDEA\\Test\\src&gt;jinfo -flag PrintGCDetails 2380&gt;-XX:+PrintGCDetails&gt; KV 设置类型：-XX:key=value case: -XX:MetaspaceSize=128m 注意： -Xms 等价于 -XX:InitialHeapSize -Xmx 等价于 -XX:MaxHeapSize 引用 强引用（Reference）在Java 中，把一个对象赋值给另一个引用变量，这个引用变量就是一个强应用。当一个对象被强引用变量引用时，它处于可达状态，不会被GC 回收，因此，强应用是造成Java 内存泄漏的主要原因。 软引用（SoftReference）对于软引用对象来说： 当系统内存充足时，不会被回收 当系统内存不足时，会被回收掉 软引用通常会应用在对内存敏感的程序中 Case: JVM OPTIONS: -Xms5m -Xmx5m -XX:+PrintGCDetails 123456789101112131415161718192021 Object object = new Object(); SoftReference&lt;Object&gt; softReference = new SoftReference&lt;&gt;(object); // 解除强引用和弱引用的联系 object = null; try &#123; /** * 模拟 OOM，迫使GC */ byte[] bytes = new byte[30 * 1024 * 1024]; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println(object); System.out.println(softReference.get()); &#125;输出：nullnullException in thread \"main\" java.lang.OutOfMemoryError: Java heap space at Test.main(Test.java:23) 弱引用(WeakReference)只要垃圾回收一运行，无论内存是否足够，GC 一律回收弱引用 弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。 1234567891011 public static void main(String[] args) throws Exception &#123; Object object = new Object(); WeakReference&lt;Object&gt; weakReference = new WeakReference&lt;&gt;(object); object = null; // 手动触发GC System.gc(); System.out.println(weakReference.get()); &#125;输出：null WeakHashMap当Key 值变成弱引用对象时，GC 会对其进行清除 1234567891011 WeakHashMap&lt;Object, Object&gt; weakHashMap = new WeakHashMap&lt;&gt;(1); Object o1 = new Object(); Object o2 = new Object(); weakHashMap.put(o1, o2); // 让key 值对象的内存块变成弱引用对象 o1 = null; System.gc(); System.out.println(weakHashMap);输出：&#123;java.lang.Object@4554617c=java.lang.Object@74a14482&#125; 虚引用（PhantomReference）虚引用主要用来跟踪对象被垃圾回收器回收的活动。 虚引用必须和引用队列(ReferenceQueue)联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。 12345678910111213141516171819202122232425262728public static void main(String[] args) throws Exception &#123; Object o1 = new Object(); ReferenceQueue&lt;Object&gt; referenceQueue = new ReferenceQueue&lt;&gt;(); PhantomReference&lt;Object&gt; phantomReference = new PhantomReference&lt;&gt;(o1, referenceQueue); System.out.println(o1); System.out.println(phantomReference.get()); System.out.println(referenceQueue.poll()); // GC o1 = null; System.gc(); System.out.println(\"================\"); System.out.println(o1); System.out.println(phantomReference.get()); System.out.println(referenceQueue.poll()); &#125;输出：java.lang.Object@4554617cnullnull[GC (System.gc()) [PSYoungGen: 1383K-&gt;496K(1536K)] 1503K-&gt;800K(5632K), 0.0008890 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [Full GC (System.gc()) [PSYoungGen: 496K-&gt;0K(1536K)] [ParOldGen: 304K-&gt;640K(4096K)] 800K-&gt;640K(5632K), [Metaspace: 3440K-&gt;3440K(1056768K)], 0.0065375 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] ================nullnulljava.lang.ref.PhantomReference@74a14482 参考理解Java的强引用、软引用、弱引用和虚引用","categories":[],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://wt-git-repository.github.io/tags/JVM/"}]},{"title":"Java面试题之线程池","slug":"Java面试题之线程池","date":"2019-10-29T02:34:06.000Z","updated":"2019-10-29T09:31:45.949Z","comments":true,"path":"2019/10/29/Java面试题之线程池/","link":"","permalink":"https://wt-git-repository.github.io/2019/10/29/Java面试题之线程池/","excerpt":"Runnable and Callable Runnable 没有返回值 Callable 有返回值 123456789101112131415161718192021222324class MyThread implements Runnable &#123; @Override public void run () &#123; &#125;&#125;class MyThread2 implements Callable&lt;Integer&gt; &#123; @Override public Integer call() &#123; return 123; &#125;&#125;public class Test &#123; public static void main(String[] args) throws Exception &#123; FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(new MyThread2()); Thread thread = new Thread(futureTask, \"aa\"); thread.start(); // 阻塞等待线程执行结果 System.out.println(futureTask.get()); &#125;&#125;","text":"Runnable and Callable Runnable 没有返回值 Callable 有返回值 123456789101112131415161718192021222324class MyThread implements Runnable &#123; @Override public void run () &#123; &#125;&#125;class MyThread2 implements Callable&lt;Integer&gt; &#123; @Override public Integer call() &#123; return 123; &#125;&#125;public class Test &#123; public static void main(String[] args) throws Exception &#123; FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(new MyThread2()); Thread thread = new Thread(futureTask, \"aa\"); thread.start(); // 阻塞等待线程执行结果 System.out.println(futureTask.get()); &#125;&#125; 线程池线程池优势 降低资源消耗。 通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。 当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。 线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性， 使用线程池可以进行统一的分配，调优和监控。 线程池的创建Executors Executors.newFixThreadPool(n) : 创建指定个数的线程池 Executors.newSingleThreadExecutor: 创建只有一个线程的线程池 Executors.newCacheThreadPool: 适用于不定个数的短期任务的执行，通过内部的调度，根据实际的情况动态地调整线程池中的线程数量 部分底层创建源码（ThreadPoolExecutor）123456789101112131415161718public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125;public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125;public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 注意 submit有返回值，而execute没有ExecutorService的execute和submit方法 ThreadPoolExecutor(阿里推荐)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 /** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @param threadFactory the factory to use when the executor * creates a new thread * @param handler the handler to use when execution is blocked * because the thread bounds and queue capacities are reached * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; * or &#123;@code threadFactory&#125; or &#123;@code handler&#125; is null */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 七大参数 corePoolSize: 线程池中常驻核心线程数 maximumPoolSize: 线程池能够容纳同时执行的最大线程数，此值必须大于等于1 keepAliveTime：多余的空闲线程的存活时间，当前的线程池的数量超过corePoolSize 时，当空闲时间达到keepAliveTime 值时，多余的空闲线程会被销毁直到剩下corePoolSize 个线程为止。 unit：keepAliveTime 的单位 workQueue：阻塞任务队列，被提交但尚未被执行的任务（当任务队列满时，线程池会扩充线程数，扩充后的线程数会小于maximumPoolSize） threadFactory：表示生成线程的线程工厂，一般使用默认便可 RejectedExecutionHandler：拒绝策略，当阻塞队列已满而且活跃线程数已经达到了maximumPoolSize 后，会启动拒绝策略，拒绝请求的加入 AbortPolicy(默认)：该策略直接抛出RejectedExecutionException 异常阻止系统的正常运行 CallerRunsPolicy：该策略不会抛弃任务也不会抛出异常，而是将任务退回到调用者 DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入到队列中再次提交该任务 DiscardPolicy：直接丢弃任务，不给予任何的处理，也不抛出异常。 12345678ExecutorService executorService = new ThreadPoolExecutor( 2, 5, 5L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); 线程池的工作流程 在创建了线程池后，等待提交过来的任务请求 当调用了execute() 方法添加了一个请求任务后，线程池会做如下判断： 如果正在运行的线程池数量小于corePoolSize，那么马上创建线程运行这个任务 如果正在运行的线程数量大于或等于corePoolSize时，那么会将请求任务放到阻塞队列中 如果阻塞队列已经满了，而且正在运行的线程数量小于maximumPoolSize，那么会创建非核心线程来立即运行这个任务 如果阻塞队列已经满了，而且正在运行的线程数量大于或等于maximumPoolSize，那么线程池会启动拒绝策略 当一个线程执行完任务时，会从队列中取出下一个任务出来继续执行 当一个线程的空闲时间大于kiveAliveTime 时，会关闭一些线程，直到线程数达到corePoolSize 为止 几张参考图 线程池线程数的合理配置CPU 密集型CPU 密集型需要大量的计算，没有阻塞，CPU 一直在全速运行 一个公式：CPU 核数 + 1个线程的线程池 IO 密集型IO 密集型，需要大量的IO 操作，即大量的阻塞 在单线程上运行IO 密集型的任务会导致浪费大量CPU 运算能力浪费在等待上，所以在IO 密集型任务中运行多线程k可以大大加速程序的运行 参考公式：CPU 核数／（１－阻塞系数），阻塞系数在0.8-0.9 之间 死锁的编码以及定位分析1234567891011121314151617181920212223242526272829class Resource implements Runnable&#123; private String lockA; private String lockB; public Resource (String lockA, String lockB) &#123; this.lockA = lockA; this.lockB = lockB; &#125; @Override public void run () &#123; synchronized (lockA) &#123; System.out.println(Thread.currentThread().getName() + \" 获得\" + lockA); synchronized (lockB) &#123; System.out.println(Thread.currentThread().getName() + \" 获得\" + lockB); &#125; &#125; &#125;&#125;public class Test &#123; public static void main(String[] args) throws Exception &#123; String lockA = \"lockA\"; String lockB = \"lockB\"; new Thread(new Resource(lockA, lockB), \"A\").start(); new Thread(new Resource(lockB, lockA), \"B\").start(); &#125;&#125; 定位分析(jps + jstack)1234567891011121314Java stack information for the threads listed above:===================================================\"B\": at Resource.run(Test.java:20) - waiting to lock &lt;0x000000076b316f18&gt; (a java.lang.String) - locked &lt;0x000000076b316f50&gt; (a java.lang.String) at java.lang.Thread.run(Thread.java:748)\"A\": at Resource.run(Test.java:20) - waiting to lock &lt;0x000000076b316f50&gt; (a java.lang.String) - locked &lt;0x000000076b316f18&gt; (a java.lang.String) at java.lang.Thread.run(Thread.java:748)Found 1 deadlock.","categories":[],"tags":[{"name":"线程池","slug":"线程池","permalink":"https://wt-git-repository.github.io/tags/线程池/"}]},{"title":"Java面试题之阻塞队列","slug":"Java面试题之阻塞队列","date":"2019-10-28T13:45:23.000Z","updated":"2019-10-29T09:32:05.543Z","comments":true,"path":"2019/10/28/Java面试题之阻塞队列/","link":"","permalink":"https://wt-git-repository.github.io/2019/10/28/Java面试题之阻塞队列/","excerpt":"阻塞队列（BlockingQueue） 当阻塞队列为空的时候，从队列中获取元素的操作会被阻塞 当阻塞队列为满的时候，向队列中添加元素的操作会被阻塞 种类 ArrayBlockingQueue: 由数组结构组成的有界阻塞队列 LinkedBlockingQueue: 由链表结构组成的有界阻塞队列（大小值默认为Integer.MAX_VALUE） PriorityBlockingQueue: 支持优先级排序的无界阻塞队列 DelayQueue：使用优先级队列实现的延迟无界阻塞队列 SynchronousQueue: 不存储元素的阻塞队列，也即有且只有一个元素的队列（一个put 必须要对应一个take） LinkedTransferQueue：由链表结构组成的无界阻塞队列 LinkedBlockingDeque：由列表结构组成的双向阻塞队列","text":"阻塞队列（BlockingQueue） 当阻塞队列为空的时候，从队列中获取元素的操作会被阻塞 当阻塞队列为满的时候，向队列中添加元素的操作会被阻塞 种类 ArrayBlockingQueue: 由数组结构组成的有界阻塞队列 LinkedBlockingQueue: 由链表结构组成的有界阻塞队列（大小值默认为Integer.MAX_VALUE） PriorityBlockingQueue: 支持优先级排序的无界阻塞队列 DelayQueue：使用优先级队列实现的延迟无界阻塞队列 SynchronousQueue: 不存储元素的阻塞队列，也即有且只有一个元素的队列（一个put 必须要对应一个take） LinkedTransferQueue：由链表结构组成的无界阻塞队列 LinkedBlockingDeque：由列表结构组成的双向阻塞队列 BlockingQueue 的核心方法抛出异常组 add 、remove and element 当阻塞队列为满时，继续add 会抛出IllegalStateException:Queue full 当阻塞队列为空时，继续remove 会抛出NoSuchElementException element 返回队列第一位，但是不会清除 返回boolean 值组与超时控制 offer、poll and peek 当阻塞队列为满时，继续offer 会返回false 当阻塞队列为空时，继续poll 会返回fase peek 返回队列的第一位 阻塞（官方推荐） put、take 当阻塞队列为满时，继续put 会阻塞等待，队列会一直阻塞生产线程，直到成功put 数据or 中断退出 当阻塞队列为空时，继续take 会阻塞等待，直到队列可用","categories":[],"tags":[{"name":"阻塞队列","slug":"阻塞队列","permalink":"https://wt-git-repository.github.io/tags/阻塞队列/"}]},{"title":"Java 面试题之锁相关","slug":"锁相关","date":"2019-10-27T02:49:30.000Z","updated":"2019-10-29T01:52:34.452Z","comments":true,"path":"2019/10/27/锁相关/","link":"","permalink":"https://wt-git-repository.github.io/2019/10/27/锁相关/","excerpt":"","text":"公平锁/非公平锁并发包中的ReentrantLock 可以创建公平锁或者非公平锁，默认是非公平锁 Java 多线程值ReentrantLock 与Condition 公平锁指多个线程按照申请锁的顺序来获取锁 在并发环境中，每个线程都会先查看锁的FIFO 等待队列，若为空，就占有锁，否则，则进队 非公平锁指多个线程获取锁的顺序不是按照申请锁的顺序来的，在高并发情况下，有可能会造成优先级反转或者饥饿现象 线程一上来就尝试占有锁，若尝试失败，则再采用类似公平锁的那种策略 优点：非公平锁的吞吐量比公平锁大 对于Synchronized 而言，也是一种非公平锁 可重入锁（递归锁）指的是同一线程在外层函数获得锁之后，内层的递归函数依然可以获得该锁 换句话说：线程可以进入任何一个它已经拥有的锁同步着的代码块 ReentrantLock/Synchronized 是一个典型的可重入锁 可重入锁作用：避免死锁 自旋锁指的是在尝试获取锁的时候不会立即阻塞，而是采用循环的方式去尝试获取锁 优点：减少上下文的切换 缺点：循环会消耗CPU 典型例子：CAS(AtomicInteger) 读写锁 读-读共存 读-写不共存 写-写不共存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class MyCache &#123; private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(16); private ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); public Object get(String key) &#123; lock.readLock().lock(); System.out.println(Thread.currentThread().getName() + \"号线程读数据\"); Object result = map.get(key); System.out.println(Thread.currentThread().getName() + \"号线程读数据完毕， \" + result); lock.readLock().unlock(); return result; &#125; public void put(String key, Object object) &#123; lock.writeLock().lock(); System.out.println(Thread.currentThread().getName() + \"号线程写数据\"); map.put(key, object); System.out.println(Thread.currentThread().getName() + \"号线程写数据完毕\"); lock.writeLock().unlock(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; MyCache myCache = new MyCache(); for (int i = 0;i &lt; 5;i++) &#123; final int finalI = i; new Thread(() -&gt; &#123; myCache.put(String.valueOf(finalI), String.valueOf(finalI)); &#125;, String.valueOf(i)).start(); &#125; for (int i = 0;i &lt; 5;i++) &#123; final int finalI = i; new Thread(() -&gt; &#123; myCache.get(finalI + \"\"); &#125;, String.valueOf(i)).start(); &#125; &#125;&#125;输出：1号线程写数据1号线程写数据完毕4号线程写数据4号线程写数据完毕0号线程写数据0号线程写数据完毕2号线程写数据2号线程写数据完毕3号线程写数据3号线程写数据完毕0号线程读数据0号线程读数据完毕， 02号线程读数据1号线程读数据1号线程读数据完毕， 13号线程读数据3号线程读数据完毕， 34号线程读数据4号线程读数据完毕， 42号线程读数据完毕， 2 CountDownLatch让一些线程阻塞，直到其余线程完成为止 1234567891011121314151617181920212223242526public class Test &#123; public static void main(String[] args) throws Exception&#123; CountDownLatch countDownLatch = new CountDownLatch(6); for (int i = 0;i &lt; 6;i++) &#123; int finalI = i; new Thread(() -&gt; &#123; System.out.println(finalI + \"号同学离开\"); countDownLatch.countDown(); &#125;, String.valueOf(i)).start(); &#125; // countDownWatch 达到0 时才能解锁 countDownLatch.await(); System.out.println(\"班长离开\"); &#125;&#125;输出：0号同学离开4号同学离开1号同学离开3号同学离开2号同学离开5号同学离开班长离开 CyclicBarrier123456789101112131415161718192021222324252627public class Test &#123; public static void main(String[] args) throws Exception &#123; CyclicBarrier cyclicBarrier = new CyclicBarrier(7, () -&gt; System.out.println(\"出发\")); for (int i = 0;i &lt; 7;i++) &#123; final int index = i; new Thread(() -&gt; &#123; System.out.println(index + \"号客人到达\"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125;输出：0号客人到达3号客人到达2号客人到达1号客人到达5号客人到达4号客人到达6号客人到达出发 Semaphore信号量，控制对共享资源的访问 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) throws Exception &#123; Semaphore semaphore = new Semaphore(3); for (int i = 0; i &lt; 6; i++) &#123; final int index = i; new Thread(() -&gt; &#123; try &#123; semaphore.acquire(); System.out.println(Thread.currentThread().getName() + \"进站\"); Thread.sleep(3000); System.out.println(Thread.currentThread().getName() + \"离开\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;, String.valueOf(i)).start(); &#125; &#125;&#125; AQS如果被请求的共享资源处于空闲状态，则当前请求资源的线程就会设置为工作线程，并且将当前资源设置为锁定状态；如果被请求的共享资源处于锁定状态，则需要一套线程阻塞以及线程被唤醒时锁分配的机制，这个机制是使用CLH 队列锁实现的，将暂时获取不到锁的线程放到队列中 典型例子：ReentrantLock, Semaphore, ReentrantReadWriteLock等等 AQS 定义两种资源的共享方式 Exclusive(独占)：只能有一个线程去独占共享资源，公平锁、非公平锁（Synchronized） Share(共享)：多个线程可以同时去执行，如Samephore、CycliBarrier、CountDownWatch","categories":[],"tags":[{"name":"锁","slug":"锁","permalink":"https://wt-git-repository.github.io/tags/锁/"}]},{"title":"flume","slug":"flume","date":"2019-10-11T01:11:50.000Z","updated":"2019-10-19T01:41:26.157Z","comments":true,"path":"2019/10/11/flume/","link":"","permalink":"https://wt-git-repository.github.io/2019/10/11/flume/","excerpt":"","text":"Flume 定义Flume 是一个高可用、高可靠、分布式的海量日志采集、聚合和传输的系统，它基于流式框架 Flume 作用在于实时读取服务器本地磁盘的文件夹，将数据写入HDFS 中","categories":[],"tags":[{"name":"flume","slug":"flume","permalink":"https://wt-git-repository.github.io/tags/flume/"}]},{"title":"kafka","slug":"kafka","date":"2019-10-08T09:21:03.000Z","updated":"2019-10-19T01:41:26.157Z","comments":true,"path":"2019/10/08/kafka/","link":"","permalink":"https://wt-git-repository.github.io/2019/10/08/kafka/","excerpt":"","text":"kafkakafka 是一个分布式消息队列，kafka 对消息保存是根据Topic 进行归类，发送者称为Producer，消息接受者称为Consumer，kafka 集群有多个kafka 实例组成，每个实例称为 broker 无论是kafka 集群还是consumer 都依赖Zookeeper 集群保存一些meta 信息， 来保证系统的可用性 在kafka 集群中，有leader 节点和follower 节点，follower 节点是备份数据用的，客户端只有访问leader 节点时才能得到响应，这个与Zookeeper 不同 在Zookeeper 集群中，客户端可以访问leader 节点也可以访问follower 节点，如果客户端访问follower 节点，如果是写请求，follower节点会把写请求转发到leader 节点中，leader 节点在执行完读写请求后，会把执行结果返回给follower 节点，follower 节点再把结果返回给客户端；如果是读请求，follower 会直接读 同一组的消费者不能同时消费同一个分区的kafka 部署1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162version: '3'services: kafka1: image: wurstmeister/kafka restart: always hostname: kafka1 container_name: kafka1 ports: - 9092:9092 environment: KAFKA_ADVERTISED_HOST_NAME: kafka1 KAFKA_ADVERTISED_PORT: 9092 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 KAFKA_LISTENERS: PLAINTEXT://kafka1:9092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092 volumes: - ./kafka1:/kafka extra_hosts: - \"zoo1:10.60.2.128\" - \"zoo2:10.60.2.128\" - \"zoo3:10.60.2.128\" kafka2: image: wurstmeister/kafka restart: always hostname: kafka2 container_name: kafka2 ports: - 9093:9092 environment: KAFKA_ADVERTISED_HOST_NAME: kafka2 KAFKA_ADVERTISED_PORT: 9092 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 KAFKA_LISTENERS: PLAINTEXT://kafka2:9092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9092 volumes: - ./kafka2:/kafka extra_hosts: - \"zoo1:10.60.2.128\" - \"zoo2:10.60.2.128\" - \"zoo3:10.60.2.128\" kafka3: image: wurstmeister/kafka restart: always hostname: kafka3 container_name: kafka3 ports: - 9094:9092 environment: KAFKA_ADVERTISED_HOST_NAME: kafka3 KAFKA_ADVERTISED_PORT: 9092 KAFKA_ZOOKEEPER_CONNECT: zoo1:2181,zoo2:2181,zoo3:2181 KAFKA_LISTENERS: PLAINTEXT://kafka3:9092 KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9092 volumes: - ./kafka3:/kafka extra_hosts: - \"zoo1:10.60.2.128\" - \"zoo2:10.60.2.128\" - \"zoo3:10.60.2.128\" kafka 生成过程分析写入方式producer 采用push 模式将消息发布到broker 中，每条消息都被追加append 到分区patition 中 分区(partition)消息发送时都会被发送到一个topic，其本质就是一个目录，而topic 是由一些partition logs（分区日志）组成 每一个分区内的消息都是有序的，生产的消息被不断追加到Partition 中，其中的每一个消息被赋予了唯一的offset 值 分区的原因： 方便在集群扩展，每个Partition 可以通过调整以适应它所在的机器，而一个Topic 又可以有多个partition 组成，因此整个集群就可以适应任意大小的数据了 提高并发，以partition 为单位读写 分区原则： 指定了partition， 则直接使用 未指定partition 但指定key，通过key 的value hash出一个partition partition 和key 都未指定，使用轮训选出一个partition 副本Replication同一个partition 可能会有多个replication，一旦broker 挂了，其上所有partition 的数据都不可被消费，同时也不能再往这些partition 中写入数据，此时，就会从partition 中的replication 中重新选举出一个leader ，producer 和consumer 只与这个leader 交互，其它fllower 从leader 中复制数据 Producer 写入流程 producer 先从broker-list 获取partition 的leader producer 将消息发送给该leader leader 将消息写入本地log followers 从leader pull 消息，followers 写完之后想leader 发送ack leader 写入成功之后，可以立即反馈给客户端，写操作结束，也可以等到followers 写完之后再反馈。而前者的效率是后者的十倍。 Cuscomerkafka 提供了两套consumer API:高级 API和低级API 高级API优点： 简单 不需要自行去管理offset，系统通过zookeeper 自行管理 不需要管理分区，副本等情况，系统自动管理 缺点： 不能控制offset，不能细化控制分区、副本等等 低级API 优点： 自主控制offset，管理分区、副本缺点： 太复杂","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://wt-git-repository.github.io/tags/kafka/"}]},{"title":"dubbo","slug":"dubbo","date":"2019-10-08T01:10:41.000Z","updated":"2019-10-08T10:53:19.069Z","comments":true,"path":"2019/10/08/dubbo/","link":"","permalink":"https://wt-git-repository.github.io/2019/10/08/dubbo/","excerpt":"","text":"分布式基础原理什么是分布式系统分布式系统是若干个独立计算机的集合，这个计算机对于用户来说就像单个相关的系统 演变路线： RPC远程过程调用，进程间的方法调用 Dubbo高性能的Java RPC 框架 Dubbo 官网 面向接口的高性能RPC 调用 智能负载均衡 服务自动注册与发现（zookeeper） 高度可扩展能力 运行期流量调度 可视化的服务治理与运维","categories":[],"tags":[{"name":"dubbo","slug":"dubbo","permalink":"https://wt-git-repository.github.io/tags/dubbo/"}]},{"title":"Zoopkeeper学习笔记","slug":"Zoopkeeper学习笔记","date":"2019-10-02T12:31:28.000Z","updated":"2019-10-08T10:53:19.069Z","comments":true,"path":"2019/10/02/Zoopkeeper学习笔记/","link":"","permalink":"https://wt-git-repository.github.io/2019/10/02/Zoopkeeper学习笔记/","excerpt":"Zoopkeeper 工作机制Zoopkeeper 是一个给予观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，接受观察者的注册，一旦数据状态发生了变化，Zoopkeeper 就负责通知已经在Zoopkeeper 上注册的那些观察者做出相应的反应。 Zoopeeker = 文件系统 + 通知机制 Zoopkeeper 特点 Zoopkeeper集群，一个leader, 多个跟随着组成集群 只要集群中半数以上节点存货，Zoopeekper 集群就能存活 全局数据的一致性，集群中每一个节点的数据都是一致的 集群对更新请求是顺序执行的，来自同一个client 的更新请求是按其发送顺序来顺序执行的 数据更新的原子性：一次数据要么更新成功，要么更新失败 实时性：在一定时间内，Client 总能读取到最新的数据","text":"Zoopkeeper 工作机制Zoopkeeper 是一个给予观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，接受观察者的注册，一旦数据状态发生了变化，Zoopkeeper 就负责通知已经在Zoopkeeper 上注册的那些观察者做出相应的反应。 Zoopeeker = 文件系统 + 通知机制 Zoopkeeper 特点 Zoopkeeper集群，一个leader, 多个跟随着组成集群 只要集群中半数以上节点存货，Zoopeekper 集群就能存活 全局数据的一致性，集群中每一个节点的数据都是一致的 集群对更新请求是顺序执行的，来自同一个client 的更新请求是按其发送顺序来顺序执行的 数据更新的原子性：一次数据要么更新成功，要么更新失败 实时性：在一定时间内，Client 总能读取到最新的数据 Zoopkeeper 应用场景统一命名服务在分布式环境下，需要对服务或者命名进行统一命名操作，同一个命名下，会有多个不同IP 的服务器统一对外提供相同的服务。 统一配置服务 在分布式环境下，统一集群中的配置文件，如kafka 集群 对配置文件修改后，能够迅速同步到各个节点上，此处，配置管理可以交给Zookeeper 上的一个Znode，各个客户端服务器监听这个Znode，一旦Znode 的数据发生了变化，Zookeeper 会通知所有节点 统一集群管理 在分布式环境中，Zoopkeeper 可以实时监控每一个节点的状态变化，节点信息可以存储在Zoopkeeper 中的一个ZNode 中 客户端能实时洞察到服务器上下线的状态变化 软负载均衡在Zoopkeeper 中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求 Zoopeekeer 的安装1234567891011121314151617181920212223242526272829303132333435363738394041424344version: '3.1'services: zoo1: image: zookeeper restart: always hostname: zoo1 ports: - 2181:2181 volumes: - ./zoo1/data:/data - ./zoo1/datalog:/datalog - ./zoo1/conf://apache-zookeeper-3.5.5-bin/conf environment: ZOO_MY_ID: 1 ZOO_SERVERS: server.1=0.0.0.0:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181 zoo2: image: zookeeper restart: always hostname: zoo2 ports: - 2182:2181 volumes: - ./zoo2/data:/data - ./zoo2/datalog:/datalog - ./zoo2/conf://apache-zookeeper-3.5.5-bin/conf environment: ZOO_MY_ID: 2 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=0.0.0.0:2888:3888;2181 server.3=zoo3:2888:3888;2181 zoo3: image: zookeeper restart: always hostname: zoo3 ports: - 2183:2181 volumes: - ./zoo3/data:/data - ./zoo3/datalog:/datalog - ./zoo3/conf://apache-zookeeper-3.5.5-bin/conf environment: ZOO_MY_ID: 3 ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=0.0.0.0:2888:3888;2181 配置传输解读 ZOO_MY_ID：表示第几号服务器 server.A=B:C:D A 是一个数字，表示这个是第几号服务器 B 是这个服务器的IP 地址 C 是这个服务器与集群中的Leader 服务器交换信息的端口 D 是万一集群中的Leader 服务器挂了，需要一个端口来重新进行选举，选举出一个新的Leader，这个端口是就是在选举时用来通信的端口 启动 1COMPOSE_PROJECT_NAME=zk_test docker-compose up docker 作为客户端连接1234567docker run -it --rm \\ --link zk_test_zoo1_1:zk1 \\ --link zk_test_zoo2_1:zk2 \\ --link zk_test_zoo3_1:zk3 \\ --net zktest_default \\ --name zkClient \\ zookeeper 进入容器执行1zkCli.sh -server zk1:2181,zk2:2181,zk3:2181 参考： docker - zookeeeper 集群部署 Zookeeper 内部原理选举机制paxos 半数机制：集群中半数以上的机器存活，集群可用，所有Zookeeper 适合安装奇数台服务器 Zookeeper 选举机制：在配置文件中，没有指定master 和Slave，但是Zookeeper 工作时，使用一个节点是Leader（通过内部选举机制临时产生的） 节点类型 持久型：客户端和服务器端断开连接后，创建的节点不删除 短暂型：客户端和服务器端断开连接后，创建的节点自己删除 持久化目录节点 持久化顺序编号目录节点 临时目录节点 临时顺序编号目录节点 /apache-zookeeper-3.5.5-bin/conf Zookeeper 监听器原理监听器原理详解 首先要有一个main() 线程 在main 线程中创建Zookeeper 客户端，这是就会创建两个线程，一个负责网络连接通信（connect），另一个负责监听（listener） 通过connect 线程将注册的监听时间发送给Zookeeper 在Zookeeper 的注册监听器列表中将注册的监听时间添加到列表中 Zookeeper 监听到有数据或者路径变化，就会将这个消息发送到listener 线程 listener 线程内部调用了process（） 方法 常见的监听 监听节点数据的变化 监听子节点增减的变化 Zookeeper 写数据流程 Client 向Zookeeper 的Server1 写数据，发出一个写请求 如果Server1 不是Leader，那么Server1 会把接收到的请求进一步转发给Leader，因为Zookeeper 的Server 里面有一个是Leader，这个Leader 会将写请求广播给各个Server，各个Server写成功之后就会通知Leader 当Leader 收到半数以上的Server 写成功之后，那就说明数据写成功了，写成功之后，Leader 会告诉Server1 写成功了 Server1 就会进一步通知Client 数据写成功了","categories":[],"tags":[{"name":"Zoopkeeper","slug":"Zoopkeeper","permalink":"https://wt-git-repository.github.io/tags/Zoopkeeper/"}]},{"title":"Spark学习笔记②","slug":"Spark学习笔记②","date":"2019-09-26T06:01:01.000Z","updated":"2019-10-16T15:24:55.826Z","comments":true,"path":"2019/09/26/Spark学习笔记②/","link":"","permalink":"https://wt-git-repository.github.io/2019/09/26/Spark学习笔记②/","excerpt":"","text":"Spark SQLSpark SQL 是Spark中的一个模块，主要用于结构化数据的处理，同时还可以作为分布式的SQL 查询引擎。 DataFrame 是以列的形式组织的，分布式的数据集合，可以通过很多源来构建，包括结构化的数据文件、Hive 中的表、外部的关系型数据库以及RDD #Spark Streaming StreamingContext 详解一个StreamingContext 定义之后，必须做一下几件事： 通过创建输入DStream 来创建输入数据源 通过对DStream 定义transformation 和output算子操作，来定义实时计算逻辑 调用StreamingContext 的start() 方法，来等待应用程序的终止（可以使用CTRL+C手动停止，或者让它持续不断地运行计算） 调用StreamingContext 的stop() 方法，来停止应用程序 需要注意几点： 只要一个StreamingContext 启动之后，就不能再往其中添加任何计算逻辑了，例如在start() 方法之后，再给某个DStream 执行一个算子 一个StreamingContext 不能重启，在stop 之后不能再Start 一个JVM 同时只能有一个StreamContext启动 调用stop 方法时，会同时停止内部的SparkContext，如果希望继续用SparkContext 创建其它类型的Context，则使用stop(false)","categories":[],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wt-git-repository.github.io/tags/Spark/"}]},{"title":"Spark学习笔记①","slug":"Spark学习笔记①","date":"2019-09-24T11:41:23.000Z","updated":"2019-10-02T12:29:46.369Z","comments":true,"path":"2019/09/24/Spark学习笔记①/","link":"","permalink":"https://wt-git-repository.github.io/2019/09/24/Spark学习笔记①/","excerpt":"SparkSpark 计算模式Spark 支持两种RDD 操作：transformation 和action。transformation 操作会针对已有的RDD 创建一个新的RDD，而action 主要是对RDD 进行最后的操作，例如遍历、reduce、保存到文件等等，并可以将结果返回给Driver 程序。 transformation 具有lazy 特性，一个程序只有transformation 是不会执行的，只有触发了action 操作，才会触发所有的transformation 操作。 Spark 工作原理分布式客户端Client 在本地编写Spark 程序，然后在本地将Spark 程序提交到Spark 集群中运行，Spark 从HDFS 中读出来的数据，会分布式存放在不同的Spark 节点上分布式处理。处理后的数据可能会被移动到别的Spark 节点中进行二次处理 所欲计算操作，都是针对多个计算节点上的数据，进行并行计算的 迭代式计算Spark 计算模型可以分为n 个阶段（MapReduce 只有两个阶段：map、reduce） RDD（弹性分布式数据集） 分布式：RDD 有多个分区，多个分区散落在不同的Spark 节点上 弹性：RDD 每个分区在Spark 存储时，默认都是在内存中的，如果内存放不下，会放部分数据到磁盘中，这些对用户都是透明的 容错性：当Spark 发现RDD 的某个分区的数据丢失后，会重新获取数据，重新计算","text":"SparkSpark 计算模式Spark 支持两种RDD 操作：transformation 和action。transformation 操作会针对已有的RDD 创建一个新的RDD，而action 主要是对RDD 进行最后的操作，例如遍历、reduce、保存到文件等等，并可以将结果返回给Driver 程序。 transformation 具有lazy 特性，一个程序只有transformation 是不会执行的，只有触发了action 操作，才会触发所有的transformation 操作。 Spark 工作原理分布式客户端Client 在本地编写Spark 程序，然后在本地将Spark 程序提交到Spark 集群中运行，Spark 从HDFS 中读出来的数据，会分布式存放在不同的Spark 节点上分布式处理。处理后的数据可能会被移动到别的Spark 节点中进行二次处理 所欲计算操作，都是针对多个计算节点上的数据，进行并行计算的 迭代式计算Spark 计算模型可以分为n 个阶段（MapReduce 只有两个阶段：map、reduce） RDD（弹性分布式数据集） 分布式：RDD 有多个分区，多个分区散落在不同的Spark 节点上 弹性：RDD 每个分区在Spark 存储时，默认都是在内存中的，如果内存放不下，会放部分数据到磁盘中，这些对用户都是透明的 容错性：当Spark 发现RDD 的某个分区的数据丢失后，会重新获取数据，重新计算 并行化创建RDD如果需要通过并行化创建RDD，需要针对程序中的集合，调用SparkContext 的parallelize 方法，将集合中的数据拷贝到集群上，形成一个分布式数据集合（RDD），相当于，集合中的一部分数据回到一个节点上，另一部分数据就到另一个节点上，然后使用并行的方式来操作这个分布式数据集合 调用parallelize 方法是有一个重要的参数是设置partition 的数量，Spark 默认会根据集群的情况来设置partition 的数量 使用本地文件和HDFS 创建RDD调用SparkContext 的textFile() 方法，可以针对本地文件或者HDFS 文件来创建RDD 注意： 如果是在Spark 集群上针对本地文件，那么需要将文件拷贝到所有Worker 节点上 Spark 的textFile 方法支持针对目录、压缩文件以及通配符来进行RDD 创建 Spark 默认为HDFS 文件的每一个block 创建一个partition Spark 常用算子Spark 常用算子","categories":[],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wt-git-repository.github.io/tags/Spark/"}]},{"title":"Spark-sample","slug":"Spark-sample","date":"2019-09-23T02:39:46.000Z","updated":"2019-10-02T12:29:46.368Z","comments":true,"path":"2019/09/23/Spark-sample/","link":"","permalink":"https://wt-git-repository.github.io/2019/09/23/Spark-sample/","excerpt":"","text":"Maven12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt;&lt;/dependency&gt; Start PointSpark 2.x ： 创建一个SparkSession 123456SparkSession spark = SparkSession .builder() .appName(\"Java Spark SQL basic example\") //.config(\"spark.some.config.option\", \"some-value\") .master(\"local\") .getOrCreate(); Create DataFrames from an existing RDDrecord.json1&#123;&quot;key&quot;:1,&quot;value&quot;:&quot;enda&quot;&#125; Application12345678910Dataset&lt;Row&gt; dataset = spark.read().json(\"./record.json\");dataset.show();// show// +---+-----+// |key|value|// +---+-----+// | 1| enda|// +---+-----+ 快速生成可执行Jar 包12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.1&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;!--运行jar包时运行的主类，要求类全名--&gt; &lt;mainClass&gt;com.dgut.App&lt;/mainClass&gt; &lt;!-- 是否指定项目classpath下的依赖 --&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;!-- 指定依赖的时候声明前缀 --&gt; &lt;classpathPrefix&gt;./lib/&lt;/classpathPrefix&gt; &lt;!--依赖是否使用带有时间戳的唯一版本号,如:xxx-1.3.0-20121225.012733.jar--&gt; &lt;useUniqueVersions&gt;false&lt;/useUniqueVersions&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!--把当前项目所有的依赖打包到target目录下的lib文件夹下--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;!--已存在的Release版本不重复copy--&gt; &lt;overWriteReleases&gt;false&lt;/overWriteReleases&gt; &lt;!--已存在的SnapShot版本不重复copy--&gt; &lt;overWriteSnapshots&gt;false&lt;/overWriteSnapshots&gt; &lt;!--不存在或者有更新版本的依赖才copy--&gt; &lt;overWriteIfNewer&gt;true&lt;/overWriteIfNewer&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 官方文档Spark SQL","categories":[],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://wt-git-repository.github.io/tags/Spark/"}]},{"title":"Spring in action 读书笔记①","slug":"Spring-in-action-1","date":"2019-09-20T01:30:57.000Z","updated":"2019-10-02T12:29:46.372Z","comments":true,"path":"2019/09/20/Spring-in-action-1/","link":"","permalink":"https://wt-git-repository.github.io/2019/09/20/Spring-in-action-1/","excerpt":"beanSpring 应用上下文Spring 自带了多种类型的应用上下文： AnnotationConfigApplicationContext： 从一个或者多个基于Java 的配置类中加载Spring 应用上下文。 AnnotationConfigWebApplicationContext：从一个或者多个基于Java 的配置类中加载Spring Web 应用上下文。 ClassPathXmlApplicationContext：从类路径下的一个或者多个XML 配置文件中加载上下文定义， 把应用上下文的定义文件作为类资源。 FileSystemXmlApplicationContext：从文件系统下的一个或者多个Xml 配置文件中加载上下文定义。 XmlWebApplicationContext：从Web应用下的一个或者多个Xml 文件中加载上下文定义。 Spring Boot 默认的类路径12345private static final String[] CLASSPATH_RESOURCE_LOCATIONS = &#123; \"classpath:/META-INF/resources/\", \"classpath:/resources/\", \"classpath:/static/\", \"classpath:/public/\" &#125;;","text":"beanSpring 应用上下文Spring 自带了多种类型的应用上下文： AnnotationConfigApplicationContext： 从一个或者多个基于Java 的配置类中加载Spring 应用上下文。 AnnotationConfigWebApplicationContext：从一个或者多个基于Java 的配置类中加载Spring Web 应用上下文。 ClassPathXmlApplicationContext：从类路径下的一个或者多个XML 配置文件中加载上下文定义， 把应用上下文的定义文件作为类资源。 FileSystemXmlApplicationContext：从文件系统下的一个或者多个Xml 配置文件中加载上下文定义。 XmlWebApplicationContext：从Web应用下的一个或者多个Xml 文件中加载上下文定义。 Spring Boot 默认的类路径12345private static final String[] CLASSPATH_RESOURCE_LOCATIONS = &#123; \"classpath:/META-INF/resources/\", \"classpath:/resources/\", \"classpath:/static/\", \"classpath:/public/\" &#125;; bean 的生命周期 @ComponentScan这个注解能够在Spring 中启用组件扫描， 如果没有其他配置， @ComponentScan 默认会扫描当前包及其子包下所有的组件。 12345// 指定基础包@ComponentScan(basePackages=&#123;'package_name'&#125;)// 指定包中的类或者接口@ComponentScan(basePackageClasses=&#123;'User.class'&#125;) or1&lt;context:component-scan base-package = 'package_name'/&gt; 使用JavaConfig 创建bean12345678910@Configurationpublic class MyConfig &#123; /** * 默认情况下，bean 的ID 与带有@Bean 注解的方法名是一样的 */ @Bean public MyBean myBean() &#123; return new MyBean(); &#125;&#125;","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://wt-git-repository.github.io/tags/Spring/"}]},{"title":"同步调用、同步回调和异步回调","slug":"同步调用、同步回调和异步回调","date":"2019-08-07T02:46:31.000Z","updated":"2019-11-14T10:30:10.920Z","comments":true,"path":"2019/08/07/同步调用、同步回调和异步回调/","link":"","permalink":"https://wt-git-repository.github.io/2019/08/07/同步调用、同步回调和异步回调/","excerpt":"","text":"I/O同步和异步说的是消息的通知机制，阻塞非阻塞说的是线程的状态 参考 一个经典例子让你彻彻底底理解java回调机制 同步调用、回调和异步调用区别 十分钟看懂Java NIO原理 Java NIO 系列文章之 浅析Reactor模式","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://wt-git-repository.github.io/tags/Java/"}]},{"title":"docker-Swarm","slug":"docker-Swarm","date":"2019-08-03T02:53:35.000Z","updated":"2019-09-10T12:12:41.559Z","comments":true,"path":"2019/08/03/docker-Swarm/","link":"","permalink":"https://wt-git-repository.github.io/2019/08/03/docker-Swarm/","excerpt":"Docker Swarm基本概念Swarm 是Docker 引擎内置原生的集群管理和编排工具 节点 节点分为管理节点 和工作节点 管理节点： 用于Swarm 集群的管理， 一个集群中可以拥有多个管理节点， 但是只有一个管理节点可以成为leader， leader 通过raft 协议实现 工作节点： 工作节点是任务执行节点，管理节点将服务下发到工作节点执行，管理节点也默认作为工作节点， 可以通过配置让服务只运行在管理节点 运行Docker 的主机可以主动初始化一个Swarm 集群或者加入一个已存在的Swarm 集群","text":"Docker Swarm基本概念Swarm 是Docker 引擎内置原生的集群管理和编排工具 节点 节点分为管理节点 和工作节点 管理节点： 用于Swarm 集群的管理， 一个集群中可以拥有多个管理节点， 但是只有一个管理节点可以成为leader， leader 通过raft 协议实现 工作节点： 工作节点是任务执行节点，管理节点将服务下发到工作节点执行，管理节点也默认作为工作节点， 可以通过配置让服务只运行在管理节点 运行Docker 的主机可以主动初始化一个Swarm 集群或者加入一个已存在的Swarm 集群 服务和任务 任务： Swarm 最小的调度单位，这里可以理解为一个单一的容器 服务： Services 是一组任务的集合， 服务模式有两种 replicated services ： 按照一定规则在各个工作节点上运行指定个数的任务 grobal services : 每个节点上运行指定个数的任务 Swarm 的创建与使用创建集群管理节点 1234567891011121314$ docker-machine create -d virtualbox manager$ docker-machine ssh managerdocker@manager:~$ docker swarm init --advertise-addr 192.168.99.100Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\ 192.168.99.100:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions. 工作节点123456789$ docker-machine create -d virtualbox worker1$ docker-machine ssh worker1docker@worker1:~$ docker swarm join \\ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\ 192.168.99.100:2377This node joined a swarm as a worker. 部署服务 在管理节点运行如下命令， 该服务默认在管理节点中也启动， 启动两个节点，一个在管理节点，一个在工作节点1docker service create --replicas 2 -p 80:80 --name nginx nginx:1.13.7-alpine 查看集群 在leader 中运行如下命令1docker node ls 使用docker-compose 创建集群docker service create： 每次只能启动一个服务 docker-compose： 一次可以启动多个服务 此处需要注意的是：需要在管理节点中运行docker-compose.yml 文件","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://wt-git-repository.github.io/tags/docker/"}]},{"title":"MySQL技术内幕与存储引擎①","slug":"MySQL技术内幕与存储引擎①","date":"2019-07-24T03:20:55.000Z","updated":"2019-11-07T02:21:14.058Z","comments":true,"path":"2019/07/24/MySQL技术内幕与存储引擎①/","link":"","permalink":"https://wt-git-repository.github.io/2019/07/24/MySQL技术内幕与存储引擎①/","excerpt":"基本概念数据库： 文件的集合， 是依照某种数据模型组织起来并存放于二级存储器中的数据集合 数据库实例：程序， 位于用户与操作系统之间的一层数据管理软件， 用户对数据库的任何操作， 包括数据库定义、数据查询、数据维护、数据库运行控制等都是在数据库实例下运行的， 应用程序只有通过数据库实例才能和数据库打交道","text":"基本概念数据库： 文件的集合， 是依照某种数据模型组织起来并存放于二级存储器中的数据集合 数据库实例：程序， 位于用户与操作系统之间的一层数据管理软件， 用户对数据库的任何操作， 包括数据库定义、数据查询、数据维护、数据库运行控制等都是在数据库实例下运行的， 应用程序只有通过数据库实例才能和数据库打交道 连接MySQLTCP/IP1mysql -h127.0.0.1 -u root -p InnoDB 体系架构 后台线程的主要作用是负责刷新内存池的数据，保证缓冲池中的内存缓存的是最近的数据。此外将已修改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下，InnoDB 能恢复到正常运行状态。 后台线程Master ThreadMaster Thread 是一个非常核心的后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性 IO Thread在InnoDB 存储引擎中大量使用AIO（Async IO） 来处理读写IO 请求，这样可以极大提高数据库的性能 Purge Thread事务被提交后，其所使用的undolog 可能不再需要，因此需要PurgeThread 来回收已经使用并分配的undo 页。 Page Cleaner Thread将之前版本中脏页的刷新操作都放入到单独的线程中 内存缓冲池InnoDB 存储引擎是基于磁盘存储的，并将其中的记录按照页的方式进行管理，在数据库系统中，由于CPU 速度与磁盘速度之间的鸿沟，基于磁盘的数据库系统通常使用缓冲池技术来提高数据库的整体性能。 在数据库中进行读取页的操作，首先将从磁盘读到的页放在缓冲池中，这个过程称为将页“FIX” 在缓冲池，下一次再读相同的页时，该页在缓冲池中被命中，直接读取该页，否则读取磁盘上的页。 在数据库中修改页的操作，则首先修改缓冲池的页，然后再以一定频率刷新到磁盘上。 文件 参数文件：配置参数，用来寻找数据库各种文件所在位置以及指定某些初始化参数，，my.cnf 什么是参数：可以把数据库参数看成一个键值对 参数类型：动态参数和静态参数 日志文件：记录MySQL 实例对某种条件作出响应时写入的文件，如错误日志、查询日志等等 错误日志：对MySQL 的启动、运行、关闭过程进行了记录 慢查询日志：将运行时间超过指定阀值的所有SQL 语句都记录到慢查询日志文件中，默认是10秒 查询日志：记录所有对MySQL 数据库请求的信息，无论这些请求是否得到了执行 二进制文件：记录了对MySQL 数据库执行更改的所有操作 socket 文件：使用UNIX 域套接字方式进行连接时需要的文件 pid 文件：MySQL 实例的进程ID 文件 MySQL 表结构文件：用来存放MySQL 表结构定义文件 存储引擎文件 表索引组织表在InnoDB 存储引擎中，表都是根据主键顺序组织存放的，这种存储方式的表称为索引组织表。 在InnoDB 存储引擎表中，每张表都有一个主键（Primary key），如果在创建表时没有显式地定义主键，那么InnoDB 存储引擎会按如下方式选择或者创建主键： 首先判断表中是否有非空的唯一索引，如果有，则该列为主键（选择第一个被定义的非空唯一索引） 如果不符合上述条件，InnoDB 存储引擎将选择表时第一个定义的非空唯一索引为主键。 InnoDB 逻辑存储结构 表空间： 表空间可以看做是InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。 段： 表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等等。 InnoDB 存储引擎表是索引组织的，因此数据即索引，那么数据段即为B+树的叶子节点（Leaf node segment） 索引段即为B+树的非索引节点（Non-leaf node segment） 区是由连续页组成的空间，在任何情况下，每个区的大小都为1 MB，在默认情况下，InnoDB 存储引擎页的大小为16KB，即一个区中一共有16 个连续的页。 页是InnoDB 磁盘管理的最小单位，在InnoDB 存储引擎中，默认每个页的大小为16KB 在InnoDB 存储引擎中，常见的页类型有： 数据页 undo 页 系统页 事务数据页 插入缓冲位图页 插入缓冲空闲列表页 未压缩的二进制大对象页 压缩的二进制大对象页 数据页存放的是完整的每行的记录 非数据页的索引页，存放的是键值及指向数据页的偏移量 行：InnoDB 存储引擎，数据是按行进行存放的 InnoDB 行记录格式InnoDB 存储引擎提供了Compact 和Redundant 两种格式来存放行记录数据 Compact 行记录格式代办 索引与算法InnoDB 存储引擎支持 B+ 树索引 全文索引 哈希索引 B+ 树索引并不能找到一个给定键值的具体行，B+树索引能找到的只是被查找数据行的所在的页，然数据库通过把页读到内存，再从内存中进行查找，最后得到要查找的数据。 聚集索引聚集索引是按照每张表的主键构造一课B+ 树，同时叶子节点存放的就是整张表的行记录数据，也将聚集索引的叶子节点成为数据页 聚集索引的存储并不是物理上连续，而是逻辑上连续 非聚集索引对于非聚集索引，叶子节点并不包含记录的全部数据，叶子节点除了包含键值对以外，每个叶子节点的索引行还包括一个书签，这个书签用来告诉InnoDB 引擎哪里可以找到与索引对应的行数据，由于InnoDB 存储引擎表是索引组织表，因此InnoDB 存储引擎的辅助索引的书签就是相应的行数据的聚集索引键 聚集索引和非聚集索引的关系非聚集索引的存在并不影响聚集索引的组织，因此每张表上可以有多个辅助索引。当通过非聚集索引来寻找数据的时候，InnoDB 存储引擎会遍历非聚集索引并通过叶级别的指针获得指向主键索引的主键，然后再通过主键索引来找到一个完整的行记录。 Cardinality 值什么时候添加B+ 树索引会比较合适对于列值具有高选择性的添加索引会有意义 高选择性是某个字段选取的值范围很广，几乎没有重复 如何查看索引是否是高选择性可以通过SHOW INDEX 的结果中的Cardinality 值来观察 什么是Cardinality 值Cardinality 值是一个预估值，表示索引中不重复记录数量的预估值 在实际应用中，Cardinality / n_rows_in_table 需要非常接近1，在访问高选择性属性的字段并从表中取出很少一部分数据的时候，对这个字段添加B+ 树索引是非常必要的。 InnoDB 是如何统计Cardinality在InnoDB 存储引擎中，Cardinality 统计信息的更新发生在两个操作中：INSERT and UPDATE。 如果一张表有50GB 的数据，那么统计一次Cardinality 会耗费非常多的时间，这样会增加数据库的负荷，因此，InnoDB 存储引擎内部对更新Cardinality 信息的策略是： 表中1/16 的数据已经发生过了变化 stat_modified_counter &gt; 2 000 000 000 (stat_modified_counter 是数据发生变化的次数) 锁lock and latchlatch 一般称之为轻量级的锁，其要求上锁的时间必须是非常短的，在InnoDB 存储引擎中，latch 又分为mutex 互斥锁和relock 读写锁，其目的都是用来保证并发线程操作临界资源的正确性。 lock 的对象一般是事务，用来锁定数据库中的对象，如表、页、行。一般lock 的对象是在事务commit 或者rollback 后进行释放。 InnoDB 存储引擎中的锁锁lock 的类型InnoDB 存储引擎实现了如下两种标准的行级锁： 共享锁S Lock： 允许事务读一行数据 排他锁X Lock: 允许事务删除或者更新一行数据 一致性非锁定读一致性非锁定读是指如果读取的行被加了X 锁，这时候读操作并不会等待行锁的释放，而是去读取行的快照，行的快照指的是该行之前版本的数据，该实现是通过undo 段来完成的，而undo 用来在事务中回滚数据的，读取快照数据也不需要上锁，因为没有事务需要对历史数据进行操作。 一致性锁定读死锁死锁避免 超时： 两个事务互相等待，当其中一个事务的等待时间超过了阈值，事务就会进行回滚，另一个等待的事务就会继续进行下去。 wait-for graph （等待图） 事务ACID 原子性：要么做，要么都不做 一致性：指的是事务将数据库从一种状态转移到下一种一致状态 隔离性：每个读写操作的事务对其它事务的操作不影响 持久性：事务一旦提交，其结果就是永久性的 事务的实现redo重做日志用来实现事务的持久性，即ACID 中的D，其由两部分组成，一是内存中的重做日志缓存，二是重做日志文件 重做日志在InnoDB 中是由两部分组成的：redo log和undo log, redo log保证事务的持久性，undo log 用来帮助事务回滚及MVCC 的功能 为了确保每次日志都写入了重做日志文件，在每次将重做日志缓冲写入到重做日志文件后，InnoDB 存储引擎都将调用一次fsync 操作 重做日志记录了事务的行为，可以很好地通过其对页进行重做操作 undo事务的回滚，需要用到undo 的信息将数据回滚到修改之前的样子。","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wt-git-repository.github.io/tags/MySQL/"}]},{"title":"Netty学习笔记","slug":"Netty学习笔记","date":"2019-07-18T05:39:42.000Z","updated":"2019-07-19T07:35:47.083Z","comments":true,"path":"2019/07/18/Netty学习笔记/","link":"","permalink":"https://wt-git-repository.github.io/2019/07/18/Netty学习笔记/","excerpt":"Netty in actionChannel、 EventLoop和 ChannelFuture Channel -&gt; socket EventLoop -&gt; 控制流、多线程处理、并发 ChannelFuture -&gt; 异步通知 Channel 接口基本的I/O 操作：（bind()、 connect()、 read()、 和write()） 在基于Java 的网络编程中， 其基本构造是class socket, Netty 的Channel 接口提供的API， 大大降低了直接使用socket 的难度。 此外， Channel 也是拥有许多预定义、 专门实现的广泛类层次结构的根 EmbeddedChannel LocalServerChannel NioDatagramChannel NioSctpChannel NioSocketChannel Channel 的生命周期 ChannelUnRegistered：Channel 已被创建， 但是还没有注册到EventLoop 中 ChannelRegistered：Channel 已经被注册到EventLoop 中 ChannelActive：Channel 处于活动状态， 已经连接到远程节点， 可以接收或者发送数据 ChannelInActive：Channel 没有连接到远程节点","text":"Netty in actionChannel、 EventLoop和 ChannelFuture Channel -&gt; socket EventLoop -&gt; 控制流、多线程处理、并发 ChannelFuture -&gt; 异步通知 Channel 接口基本的I/O 操作：（bind()、 connect()、 read()、 和write()） 在基于Java 的网络编程中， 其基本构造是class socket, Netty 的Channel 接口提供的API， 大大降低了直接使用socket 的难度。 此外， Channel 也是拥有许多预定义、 专门实现的广泛类层次结构的根 EmbeddedChannel LocalServerChannel NioDatagramChannel NioSctpChannel NioSocketChannel Channel 的生命周期 ChannelUnRegistered：Channel 已被创建， 但是还没有注册到EventLoop 中 ChannelRegistered：Channel 已经被注册到EventLoop 中 ChannelActive：Channel 处于活动状态， 已经连接到远程节点， 可以接收或者发送数据 ChannelInActive：Channel 没有连接到远程节点 EventLoopEventLoop 定义了Netty 的核心抽象， 用于处理连接的生命周期中所发生的事件。 一个EventLoopGroup 包含一个或者多个EventLoop 一个EventLoop 在它的生命周期内只有一个Thread 绑定 所有由EventLoop 处理的I/O 事件都由它专属的Thread 上被处理 一个Channel 在它的生命周期内只注册一个EventLoop 一个EventLoop 可能被分配给以一个或者多个Channel 注意：一个给定的Channel 的I/O 操作都是由相同的Thread 去执行的。 ChannelFutureNetty 所有操作都是异步的， Netty 提供了ChannelFuture 接口， 其addListener() 方法注册了一个ChannelFutureListener， 以便在一个操作完成之后作一个及时的反馈。 ChannelHandler 和 ChannelPipelineChannleHandlerChannelHandler 充当了所有处理入站和出站数据的应用程序逻辑的容器 例如： ChannelHandlerAdapter ChannelInboundHandler：处理入站数据以及各种状态变化， 其中一个子类是SimpleChannelInboundHandler ChannelOutboundHandlerAdapter：处理出站数据并且允许拦截所有的操作 ChannelDuplexHandler ChannelHandler 生命周期方法 handlerAdded：当ChannelHandler 添加到ChannelPipeline 时被调用 handlerRemoved：当从ChannelPipeline 中移除ChannelHandler 时被调用 exceptionCaught：当处理过程中， 在拦截链中遇到异常时被触发 ChannelInboundHandler 当某个 ChannelInboundHandler 的实现重写 channelRead()方法时，它将负责显式地释放与池化的 ByteBuf 实例相关的内存。 Netty 为此提供了一个实用方法 ReferenceCount\u0002Util.release(msg)来释放资源 SimpleChannelInboundHandler 会自动释放资源， ChannelOutboundHandler ChannelPromise与ChannelFuture ChannelOutboundHandler中的大部分方法都需要一个ChannelPromise参数，以便在操作完成时得到通知。ChannelPromise是ChannelFuture的一个子类，其定义了一些可写的方法，如setSuccess()和setFailure()，从而使ChannelFuture不可变 解码器和编码器所有由 Netty 提供的编码器/解码器适配器类都实现了 ChannelOutboundHandler 或者 ChannelInboundHandler 接口 对于入站数据来说，channelRead 方法/事件已经被重写了。对于每个从入站Channel 读取的消息，这个方法都将会被调用。随后，它将调用由预置解码器所提供的 decode()方法，并将已解码的字节转发给 ChannelPipeline 中的下一个 ChannelInboundHandler。出站消息的模式是相反方向的：编码器将消息转换为字节，并将它们转发给下一个ChannelOutboundHandler。 抽象类 SimpleChannelInboundHandlerT 是你要处理的消息的 Java 类型 ChannelPipeline(拦截过滤器)ChannelPipeline 提供了ChannelHandler 链的容器， 并定义了用于在该链上传播入站和出站事件流的API ChannelPipeline 持有所有将应用于入站和出站数据以及事件的 ChannelHandler 实例，这些 ChannelHandler 实现了应用程序用于处理状态变化以及数据处理的逻辑 当Channel 被创建之后， 它会被分配到专属的Channelpeline ChannelHandler 安装到ChannelPipeline 的过程如下： 一个ChannelInitializer 的实现被注册到了ServerBootstrap 中 当ChannelInitializer.initChannel() 方法被调用之后， ChannelInitializer 将会在ChannelPipeline 中安装一组自定义的ChannelHandler ChannelInitializer 将它自己从ChannelPipeline 中移除 在Netty 中， 有两种写数据的方式： 直接写到Channel： 这种方式会导致消息从ChannelPipeline 尾部开始流动 写到和ChannelHandler 相关联的ChannelHandlerContext 中： 这种方式会导致消息从ChannelPipeline 对应的下一个ChannelHandler 开始流动 ChannelHandlerContextChannelHandlerContext 代表了ChannelHandler 和 ChannelPipeline 之间的关联, 每当有ChannelHandler 添加到ChannelPipeline 中时，都会创建ChannelHandler\u0002Context ChannelHandlerContext 的主要功能是管理它所关联的ChannelHandler 和在同一个ChannelPipeline 中的其他ChannelHandler 之间的交互 注意：如果调用Channel 和ChannelPipeline 上面的方法， 它们会沿着这个ChannelPipeline 上进行传播 如果ChannelHandlerContext 上相同的方法， 只会从当前ChannelHandler 开始传播 通过 ChannelHandlerContext 获取到 Channel 的引用。调用Channel 上的 write()方法将会导致写入事件从尾端到头部地流经 ChannelPipeline 引导两种类型： 客户端（Bootstrap）与服务器端(ServerBootStrap) 传输 ByteBufNetty 的ByteBuf 用于替代Java NIO 的ByteBuff ByteBuf 维护了两个不同的索引： 一个用于读取、一个用于写入 ByteBuf 的使用模式堆缓冲区最常用的ByteBuf 模式是将数据存储在JVM 的堆空间里面， 这种模式被称为支撑数组， 它能在没有使用池化的情况下提供快速的分配和释放。 直接缓冲区直接缓冲区的内容将驻留在常规的会被垃圾回收的堆之外 直接缓冲区是网络数据传输的理想选择， 但不是数据处理的理想选择， 因为在直接缓冲区需要被处理的数据， 需要复制到堆中才能被处理 如果你的数据包含在一个在堆中分配的缓冲区， 在通过套接字发送之前， JVM 会将它复制到直接缓冲区， 然后再发送 复合缓冲区它为多个ByteBuf 提供一个聚合视图， 可以根据需要添加或者删除ByteBuf 实例 Netty 通过一个ByteBuf 子类（compositeByteBuf）实现这个模式， 它提供了一个将多个缓冲区表示为单个合并缓冲区的虚拟表示， 所以这里值得注意的是， CompositeByteBuf 实例可能同时包含直接内存和非直接内存。","categories":[],"tags":[{"name":"Netty","slug":"Netty","permalink":"https://wt-git-repository.github.io/tags/Netty/"}]},{"title":"MySQL必知必会","slug":"MySQL必知必会","date":"2019-07-15T00:59:33.000Z","updated":"2019-07-15T04:53:41.723Z","comments":true,"path":"2019/07/15/MySQL必知必会/","link":"","permalink":"https://wt-git-repository.github.io/2019/07/15/MySQL必知必会/","excerpt":"","text":"MySQL 必知必会 MySQL 线程池的最佳连接数 = （CPU 核心数 * 2） + 有效磁盘数 比较运算符能用 = 就不用 &lt;&gt;, 增加索引的利用率 如果只有一条查询结果， 请使用 “LIMIT 1” 为列选择合适的数据类型， 能用SMALLINT 就不用INT， 磁盘和内存消耗越小越好。 将大的DELETE、 UPDATE or INSERT 查询拆成多个小的查询 如果结果集允许重复， 则使用UNION ALL 代替 UNION 为了充分利用查询缓存， 请保持SQL 语句查询条件的一致性 尽量避免使用select * where、 Join and Order by 语句的的列尽量被索引 使用LIMIT 实现分页逻辑， 减少不必要的传输","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wt-git-repository.github.io/tags/MySQL/"}]},{"title":"hadoop学习笔记⑥","slug":"hadoop学习笔记⑥","date":"2019-07-13T02:30:34.000Z","updated":"2019-07-26T10:06:38.979Z","comments":true,"path":"2019/07/13/hadoop学习笔记⑥/","link":"","permalink":"https://wt-git-repository.github.io/2019/07/13/hadoop学习笔记⑥/","excerpt":"Shuffle and combiner","text":"Shuffle and combiner HDFS 2.0HDFS 1.0 只有一个名称节点， 存在单点故障问题。 HDFS 2.0， 采用HA 架构。 在一个典型的HA 集群中， 一般设置两个名称节点， 一个名称节点处于活跃状态， 另一个处于待命状态。 处于活跃状态的名称节点负责对外处理所有客户端的请求， 而处于待命状态的节点则作为备用节点。 待命的名称节点是活跃名称节点的“热备份”， 活跃名称节点的状态信息必须实时同步到待命节点， 两个名称节点的状态同步， 可以借助一个共享存储系统来实现（Zookeeper）， 活跃名称节点将更新数据写入到共享存储系统， 待命名称节点会一直监听该系统， 一旦有新的写入， 就立即从公共存储系统中读取这些数据并加载到自己的内存中， 从而保证与活跃名称节点状态的完全同步。 每个数据节点都要向集群中的所有名称节点注册， 并周期性地向名称节点发送“心跳”和块信息， 报告自己的状态， 同时也处理来自名称节点的指令。 YARN（新一代资源管理调度框架）YARN 把JobTracker（Hadoop1.0 中的资源管理调度功能） 这一组件的任务进行拆分， 拆成三个部分： 资源管理、任务调度和任务监控 YARN 包括ResourceManager（资源管理）、 ApplicationMaster（任务调度和监控）和 NodeManager（负责执行原TaskTracker（DataNode） 的任务） YARN 体系结构 ResourceManager 处理客户端请求 启动/监控ApplicationMaster 监控NodeManager 资源分配与调度 ApplicationMaster 为应用程序申请资源， 并分配给内部任务 任务调度、监控与容错 NodeManager 单个节点上的资源管理 处理来自ResourceManager 的命令 处理来自ApplicationManager 的命令 YARN 的部署 YARN 的工作流程 用户编写客户端应用程序， 向YARN 提交应用程序， 提交的内容包括ApplicationMaster程序、启动ApplicationMaster 的命令、用户程序等等。 YARN 中的ResourceManager 负责接受和处理来自客户端的请求， 为应用程序分配一个容器， ResourceManager 的应用程序管理器会与该容器所在的NodeManager 通信， 为该应用程序所在的容器启动一个ApplicationMaster ApplicationMaster 被创建后会先向ResourceManager 注册， 然后用户可以通过ResourceManager 来查看程序的运行状态。 以下是应用程序执行步骤 ApplicationManager 采用轮询的方式通过RPC 协议向ResourceMAnager 申请资源 ResourceManager 以“容器” 的形式向提出申请的ApplicationMAster 分配资源， 一旦申请到资源后， 就会与该容器所在的NodeManager 进行通信， 要求它启动任务 当ApplicationMaster 要求容器启动任务时， 它会为任务设置好运行环境， 然后将任务启动命令写到一个脚本中， 最后通过在容器中运行该脚本来启动任务。 各个任务通过某个RPC 协议向ApplicationMaster 汇报自己的状态和进度， 让ApplicationMaster 可以随时掌握各个任务的运行状态， 从而可以在任务失败时重启任务 应用程序运行完成后， ApplicationMaster 向ResourceManager 的应用管理器注销并关闭自己。 SparkSpark 同时支持一下三种类型： 复杂的批量数据处理： 时间跨度通常在数十分钟到数小时之间 基于历史数据的交互式查询： 时间跨度通常在数十秒到数分钟之间 基于实时数据流的数据处理： 时间跨度通常在数百毫秒到数秒之间 Spark 专注于数据的处理和分析， 数据的存储还是需要借助Hadoop 分布式文件系统HDFS 来实现。 Spark 生态系统主要包括Spark Core、 Spark SQL、 Spark Streaming、 MLlib和 GraphX 等组件 Spark Core： Spark Core 包含Spark 的基本功能， 如内存计算、任务调度、部署模式、故障恢复、存储管理等， 主要面向批数据处理。 Spark 建立在统一的抽象RDD 之上， 使其可以以基本一致的方式应对大数据处理场景。 Spark SQL： Spark SQL 允许开发人员直接处理RDD， 同时也可以查询Hive、 HBase等外部数据源。 Spark SQL 的一个重要特点是其能够统一处理关系表和RDD， 使得开发人员不需要自己编写Spark 应用程序， 开发人员可以轻松使用SQL 命令进行查询， 并进行更复杂的数据分析。 Spark Streaming： Spark Streaming 支持高吞吐量、 可容错处理的实时数据处理， 其核心思路是将流数据分解成一系列短小的批处理作业， 每个短小的批处理作业都可以使用Spark Core 进行快速处理， Spark Streaming 支持多种数据输入源， 如Kafka、 Flume和TCP 套接字等。 MLlib： 机器学习 GraphX（图计算） Spark 运行框架基本概念 RDD： 弹性分布式数据集， 是分布式内存的一个抽象概念， 提供了一个高度受限的共享内存模型 DAG： 有向无环图， 反映RDD 之间的依赖关系 Executor： 是运行在工作节点（Worker Node）上的一个进程， 负责运行任务， 并为应用程序存储数据 应用Application： 用户编写的Spark 应用程序 任务： 运行在Executor 上的工作单元 作业： 一个作业包含多个RDD 及作用于相应RDD 上的各种操作 阶段： 是作业的基本调度单位， 一个作业分为多个阶段， 一个阶段又有多个任务task 架构设计 集群资源管理器Cluster Manager（资源管理器可以使用YARN） 运行作业任务的工作节点Work Node 每个应用的任务控制节点Driver 每个工作节点上负责具体任务的执行进程Executor 当执行一个应用时， 任务控制节点会向集群管理器（Cluster Manager）申请资源， 启动Executor， 并向Executor 发送应用程序代码和文件， 然后在Executor 上执行任务， 运行结束后执行结果会返回任务控制节点， 或者写到HDFS 或其它数据库中。 Spark 运行的基本流程 当一个Spark 应用被提交时， 首先需要为这个应用构建起基本的运行环境， 即由任务控制节点（Driver） 创建一个SparkContext， 由SparkContext 负责和资源管理器Cluster Manager 的通信以及进行资源的申请、任务的分配等工作， 然后SparkContext 会向资源管理器（ResourceManager or Cluster Manager）注册并申请运行Executor 的资源 资源管理器为Executor 分配资源， 并启动Executor 进程， Executor 运行情况将随着“心跳” 发送到资源管理器上 SparkContext 根据RDD 的依赖关系构建DAG 图， DAG 图提交给DAG 调度器进行解析， 将DAG 图分解为多个“阶段”（每个阶段都是一个任务集）， 并且计算出各个阶段的依赖关系， 并且把一个个阶段中的任务集分发给Executor 运行， 同时SparkContext 将应用程序代码发放给Executor 任务在Executor 上运行， 把执行结果反馈给任务调度器， 然后反馈给DAG 调度器， 运行完毕之后写入数据并释放资源 注意 每个应用都有自己专属的Executor， 并且该进程在应用运行期间一直驻留， Executor 进程以多线程方式运行任务， 减少多进程任务频繁的启动开销， 使得任务执行变得非常高效和可靠 Spark 运行过程与资源管理器无关， 只要能够获取Executor 进程并保持通信即可 Executor 上有一个BlockManager 存储模块， 把内存和磁盘共同作为存储设备， 在处理迭代计算任务时， 不需要把中间结果写入到HDFS 等文件系统， 而是直接放在这个存储系统上， 后续有需要则直接读取， 在交互式查询的场景中， 也可以把表提前缓存到这个存储系统上， 提高读写IO性能 任务采用数据本地性和推测执行等优化机制 数据本地化：将计算移到数据所在的节点上进行， 因为移动计算比移动数据所占的网络资源要少得多。 延时调度机制： 实现执行过程优化 RDD 的设计与运行原理一个RDD 就是一个分布式对象集合， 一个RDD 可以分为多个分区， 一个分区就是一个数据集片段， 一个RDD 的不同分区可以被保存到集群中的不同节点上， 从而在集群中的不同节点上进行并行分析。 RDD 是只读的记录分区的集合， 不能直接修改， 只能基于稳定的物理存储中的数据集来创建RDD， 或者通过在其他RDD 上执行确定的转换操作来创建RDD。 RDD 提供了一组丰富的操作以支持常见的数据运算， 分为”行动Action” 和“转换Transformation” 两种类型， 前者用于执行计算并指定输出的形式， 后者指定RDD 之间的相互依赖关系。 转换操作： 接受一个RDD 并返回一个RDD 行动操作： 接受一个RDD 但返回的不是一个RDD Spark 使用Scala 语言实现RDD 的API： RDD 读入外部数据源进行创建。 RDD 经过一系列的“转换” 操作， 每一次都会产生不同的RDD， 供给下一次“转换” 使用。 最后一个RDD 经过“行动Action” 操作进行处理， 并输出到外部数据源。 Spark 部署和应用方式Spark 可运行在YARN 之上， 与Hadoop 进行统一部署， 资源管理和调度依赖YARN， 分布式存储依赖HDFS Spark 目前最好与Hadoop 进行统一部署","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://wt-git-repository.github.io/tags/hadoop/"}]},{"title":"Spring Secutiry 登陆源码分析","slug":"SpringSecurity","date":"2019-07-11T01:18:30.000Z","updated":"2019-07-15T06:37:41.607Z","comments":true,"path":"2019/07/11/SpringSecurity/","link":"","permalink":"https://wt-git-repository.github.io/2019/07/11/SpringSecurity/","excerpt":"CasAuthenticationFilter 简介|（版本：3.0.7）处理Cas service ticket Service Tickets一个服务票据是由一串加密的票证字符串组成， 服务票据是用户的浏览器通过Cas Server 认证后， 通过HTTP Redirect 到资源服务器中， 而服务票据是通过其中的请求参数中获取到的。 过滤器监视着Service URL 以致于它可以接收到服务票据并进行处理，The CAS server knows which service URL to use via the ServiceProperties.getService() method。 Processing the service ticket involves creating a UsernamePasswordAuthenticationToken(the principal and the opaque ticket string as the credentials) which uses CAS_STATEFUL_IDENTIFIER for the principal and the opaque ticket string as the credentials.(通过服务票据生成 UsernamePasswordAuthenticationToken) The configured AuthenticationManager is expected to provide a provider that can recognise UsernamePasswordAuthenticationTokens containing this special principal name, and process them accordingly by validation with the CAS server.（通过AuthenticationManager 依据Cas server 来处理识别该证书）","text":"CasAuthenticationFilter 简介|（版本：3.0.7）处理Cas service ticket Service Tickets一个服务票据是由一串加密的票证字符串组成， 服务票据是用户的浏览器通过Cas Server 认证后， 通过HTTP Redirect 到资源服务器中， 而服务票据是通过其中的请求参数中获取到的。 过滤器监视着Service URL 以致于它可以接收到服务票据并进行处理，The CAS server knows which service URL to use via the ServiceProperties.getService() method。 Processing the service ticket involves creating a UsernamePasswordAuthenticationToken(the principal and the opaque ticket string as the credentials) which uses CAS_STATEFUL_IDENTIFIER for the principal and the opaque ticket string as the credentials.(通过服务票据生成 UsernamePasswordAuthenticationToken) The configured AuthenticationManager is expected to provide a provider that can recognise UsernamePasswordAuthenticationTokens containing this special principal name, and process them accordingly by validation with the CAS server.（通过AuthenticationManager 依据Cas server 来处理识别该证书） By configuring a shared ProxyGrantingTicketStorage between the TicketValidator and the CasAuthenticationFilter one can have the CasAuthenticationFilter handle the proxying requirements for CAS. In addition, the URI endpoint for the proxying would also need to be configured (i.e. the part after protocol, hostname, and port).（代理相关） Spring Secutiry 登陆基本流程（源码分析）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226 /** * AbstractAuthenticationProcessingFilter */ public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest request = (HttpServletRequest)req; HttpServletResponse response = (HttpServletResponse)res; /* * Invokes the requiresAuthentication method to determine whether the request is for * authentication and should be handled by this filter.（如果是需要认证的话， 再进入此过滤类） */ if (!this.requiresAuthentication(request, response)) &#123; chain.doFilter(request, response); &#125; else &#123; if (this.logger.isDebugEnabled()) &#123; this.logger.debug(\"Request is to process authentication\"); &#125; Authentication authResult; try &#123; /* * If it is an authentication request, the attemptAuthentication will be invoked to perform the authentication. (调用了子类的方法) */ authResult = this.attemptAuthentication(request, response); if (authResult == null) &#123; return; &#125; // // 最终认证成功后，会处理一些与session相关的方法（比如将认证信息存到session等操作） this.sessionStrategy.onAuthentication(authResult, request, response); &#125; catch (InternalAuthenticationServiceException var8) &#123; this.logger.error(\"An internal error occurred while trying to authenticate the user.\", var8); this.unsuccessfulAuthentication(request, response, var8); return; &#125; catch (AuthenticationException var9) &#123; this.unsuccessfulAuthentication(request, response, var9); return; &#125; if (this.continueChainBeforeSuccessfulAuthentication) &#123; chain.doFilter(request, response); &#125; /* * 最终认证成功后的相关回调方法，主要将当前的认证信息放到SecurityContextHolder中 * 并调用成功处理器做相应的操作。 */ this.successfulAuthentication(request, response, chain, authResult); &#125; &#125; /** * CasAuthenticationFilter * * Performs actual authentication. * 1.Return a populated authentication token for the authenticated user, indicating * successful authentication * 2.Return null, indicating that the authentication process is still in progress. Before * returning, the implementation should perform any additional work required to complete the * process. * 3.Throw an AuthenticationException if the authentication process fails */ public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException, IOException &#123; if (this.proxyReceptorRequest(request)) &#123; this.logger.debug(\"Responding to proxy receptor request\"); CommonUtils.readAndRespondToProxyReceptorRequest(request, response, this.proxyGrantingTicketStorage); return null; &#125; else &#123; boolean serviceTicketRequest = this.serviceTicketRequest(request, response); String username = serviceTicketRequest ? \"_cas_stateful_\" : \"_cas_stateless_\"; String password = this.obtainArtifact(request); if (password == null) &#123; this.logger.debug(\"Failed to obtain an artifact (cas ticket)\"); password = \"\"; &#125; UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken(username, password); authRequest.setDetails(this.authenticationDetailsSource.buildDetails(request)); // 调用Authentication（ProviderManager） return this.getAuthenticationManager().authenticate(authRequest); &#125; &#125; /* * UsernamePasswordAuthenticationToken * * 保存用户信息， 并设置授权为false */ public UsernamePasswordAuthenticationToken(Object principal, Object credentials) &#123; super((Collection)null); this.principal = principal; this.credentials = credentials; this.setAuthenticated(false); &#125; /* * AuthenticationManager -&gt; ProviderManager * * 进行校验工作 */ public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; Class&lt;? extends Authentication&gt; toTest = authentication.getClass(); AuthenticationException lastException = null; Authentication result = null; boolean debug = logger.isDebugEnabled(); Iterator var6 = this.getProviders().iterator(); while(var6.hasNext()) &#123; AuthenticationProvider provider = (AuthenticationProvider)var6.next(); if (provider.supports(toTest)) &#123; if (debug) &#123; logger.debug(\"Authentication attempt using \" + provider.getClass().getName()); &#125; try &#123; result = provider.authenticate(authentication); // 如果有处理器成功处理， 则退出 if (result != null) &#123; this.copyDetails(authentication, result); break; &#125; &#125; catch (AccountStatusException var11) &#123; this.prepareException(var11, authentication); throw var11; &#125; catch (InternalAuthenticationServiceException var12) &#123; this.prepareException(var12, authentication); throw var12; &#125; catch (AuthenticationException var13) &#123; lastException = var13; &#125; &#125; &#125; // 如果没有处理器能够处理， 则调用父类的 if (result == null &amp;&amp; this.parent != null) &#123; try &#123; result = this.parent.authenticate(authentication); &#125; catch (ProviderNotFoundException var9) &#123; ; &#125; catch (AuthenticationException var10) &#123; lastException = var10; &#125; &#125; if (result != null) &#123; if (this.eraseCredentialsAfterAuthentication &amp;&amp; result instanceof CredentialsContainer) &#123; ((CredentialsContainer)result).eraseCredentials(); &#125; this.eventPublisher.publishAuthenticationSuccess(result); return result; &#125; else &#123; if (lastException == null) &#123; lastException = new ProviderNotFoundException(this.messages.getMessage(\"ProviderManager.providerNotFound\", new Object[]&#123;toTest.getName()&#125;, \"No AuthenticationProvider found for &#123;0&#125;\")); &#125; this.prepareException((AuthenticationException)lastException, authentication); throw lastException; &#125; &#125; /* * AuthenticationProvider -&gt; AbstractUserDetailsAuthenticationProvider */ public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; Assert.isInstanceOf(UsernamePasswordAuthenticationToken.class, authentication, this.messages.getMessage(\"AbstractUserDetailsAuthenticationProvider.onlySupports\", \"Only UsernamePasswordAuthenticationToken is supported\")); String username = authentication.getPrincipal() == null ? \"NONE_PROVIDED\" : authentication.getName(); boolean cacheWasUsed = true; UserDetails user = this.userCache.getUserFromCache(username); if (user == null) &#123; cacheWasUsed = false; try &#123; user = this.retrieveUser(username, (UsernamePasswordAuthenticationToken)authentication); &#125; catch (UsernameNotFoundException var6) &#123; this.logger.debug(\"User '\" + username + \"' not found\"); if (this.hideUserNotFoundExceptions) &#123; throw new BadCredentialsException(this.messages.getMessage(\"AbstractUserDetailsAuthenticationProvider.badCredentials\", \"Bad credentials\")); &#125; throw var6; &#125; Assert.notNull(user, \"retrieveUser returned null - a violation of the interface contract\"); &#125; /** * DaoAuthenticationProvider */ protected final UserDetails retrieveUser(String username, UsernamePasswordAuthenticationToken authentication) throws AuthenticationException &#123; UserDetails loadedUser; try &#123; loadedUser = this.getUserDetailsService().loadUserByUsername(username); &#125; catch (UsernameNotFoundException var6) &#123; if (authentication.getCredentials() != null) &#123; String presentedPassword = authentication.getCredentials().toString(); this.passwordEncoder.isPasswordValid(this.userNotFoundEncodedPassword, presentedPassword, (Object)null); &#125; throw var6; &#125; catch (Exception var7) &#123; throw new InternalAuthenticationServiceException(var7.getMessage(), var7); &#125; if (loadedUser == null) &#123; throw new InternalAuthenticationServiceException(\"UserDetailsService returned null, which is an interface contract violation\"); &#125; else &#123; return loadedUser; &#125; &#125; /* * DaoUserDetailsService 自定义 */ @Overridepublic UserDetails loadUserByUsername(final String username) throws UsernameNotFoundException &#123; try &#123; final User user = userRepository.loadUserByUsername(username); if (user == null) &#123; throw new Exception(\"用户名或密码错误\"); &#125; MyUserPrincipal myPrincipal = new MyUserPrincipal(user.getIsadmin(), user.getRealname(),user.getId(), user.getPassword(), MyUserPrincipal.getAuthorities(user.getRoles())); return myPrincipal; &#125; catch (final Exception e) &#123; throw new RuntimeException(e); &#125;&#125; 补充 addFilterBefore(filter, class) – adds a filter before the position of the specified filter class addFilterAfter(filter, class) – adds a filter after the position of the specified filter class addFilterAt(filter, class) – adds a filter at the location of the specified filter class addFilter(filter) – adds a filter that must be an instance of or extend one of the filters provided by Spring Security 官方文档 Spring Security 官方文档 CasAuthenticationFilter 官方文档 AbstractAuthenticationProcessingFilter 官方文档 简书上的相关解释 A Custom Filter in the Spring Security Filter Chain","categories":[],"tags":[{"name":"Spring Secutiry, Cas","slug":"Spring-Secutiry-Cas","permalink":"https://wt-git-repository.github.io/tags/Spring-Secutiry-Cas/"}]},{"title":"基于pageable实现分页操作","slug":"基于Pageable实现分页操作","date":"2019-07-10T03:49:23.000Z","updated":"2019-07-10T03:57:57.979Z","comments":true,"path":"2019/07/10/基于Pageable实现分页操作/","link":"","permalink":"https://wt-git-repository.github.io/2019/07/10/基于Pageable实现分页操作/","excerpt":"","text":"简介 Pageable 是Spring Data库中定义的一个接口，该接口是所有分页相关信息的一个抽象，通过该接口，我们可以得到和分页相关所有信息（例如pageNumber、pageSize等），这样，Jpa就能够通过pageable参数来得到一个带分页信息的Sql语句。 Page类也是Spring Data提供的一个接口，该接口表示一部分数据的集合以及其相关的下一部分数据、数据总数等相关信息，通过该接口，我们可以得到数据的总体信息（数据总数、总页数…）以及当前数据的信息（当前数据的集合、当前页数等） 通过参数获取Pageable1234567@RequestMapping(value = \"/params\", method=RequestMethod.GET)public Page&lt;Blog&gt; getEntryByParams(@RequestParam(value = \"page\", defaultValue = \"0\") Integer page, @RequestParam(value = \"size\", defaultValue = \"15\") Integer size) &#123; Sort sort = new Sort(Direction.DESC, \"id\"); Pageable pageable = new PageRequest(page, size, sort); return blogRepository.findAll(pageable);&#125; 直接获取Pageable 对象当Spring发现这个参数时，Spring会自动的根据request的参数来组装该pageable对象，Spring支持的request参数如下： page，第几页，从0开始，默认为第0页 size，每一页的大小，默认为20 sort，排序相关的信息，以property,property(,ASC|DESC)的方式组织，例如sort=firstname&amp;sort=lastname,desc表示在按firstname正序排列基础上按lastname倒序排列 12345@RequestMapping(value = \"\", method=RequestMethod.GET)public Page&lt;Blog&gt; getEntryByPageable(@PageableDefault(value = 15, sort = &#123; \"id\" &#125;, direction = Sort.Direction.DESC) Pageable pageable) &#123; return blogRepository.findAll(pageable);&#125; 参考 Spring Data Jpa: 分页和排序 Pageable 前台传参","categories":[],"tags":[{"name":"Spring Data Jpa","slug":"Spring-Data-Jpa","permalink":"https://wt-git-repository.github.io/tags/Spring-Data-Jpa/"}]},{"title":"Java自定义Annotation注解","slug":"Java自定义Annotation注解","date":"2019-07-10T01:35:39.000Z","updated":"2019-07-10T03:38:25.501Z","comments":true,"path":"2019/07/10/Java自定义Annotation注解/","link":"","permalink":"https://wt-git-repository.github.io/2019/07/10/Java自定义Annotation注解/","excerpt":"元注解元注解的作用在于： 负责注解其它注解。 @Target @Retention @Documented @Inherited @Target@Target 说明了Annotation 所修饰的对象的范围， 使用枚举类ElementType 来指定。 123456789101112131415161718192021222324252627282930313233343536373839public enum ElementType &#123; /** Class, interface (including annotation type), or enum declaration */ TYPE, /** Field declaration (includes enum constants) */ FIELD, /** Method declaration */ METHOD, /** Formal parameter declaration */ PARAMETER, /** Constructor declaration */ CONSTRUCTOR, /** Local variable declaration */ LOCAL_VARIABLE, /** Annotation type declaration */ ANNOTATION_TYPE, /** Package declaration */ PACKAGE, /** * Type parameter declaration * * @since 1.8 */ TYPE_PARAMETER, /** * Use of a type * * @since 1.8 */ TYPE_USE&#125;","text":"元注解元注解的作用在于： 负责注解其它注解。 @Target @Retention @Documented @Inherited @Target@Target 说明了Annotation 所修饰的对象的范围， 使用枚举类ElementType 来指定。 123456789101112131415161718192021222324252627282930313233343536373839public enum ElementType &#123; /** Class, interface (including annotation type), or enum declaration */ TYPE, /** Field declaration (includes enum constants) */ FIELD, /** Method declaration */ METHOD, /** Formal parameter declaration */ PARAMETER, /** Constructor declaration */ CONSTRUCTOR, /** Local variable declaration */ LOCAL_VARIABLE, /** Annotation type declaration */ ANNOTATION_TYPE, /** Package declaration */ PACKAGE, /** * Type parameter declaration * * @since 1.8 */ TYPE_PARAMETER, /** * Use of a type * * @since 1.8 */ TYPE_USE&#125; @Retention@Retention ： 用于描述注解的生命周期， 即被描述的注解在什么范围内有效。 取值范围有(在枚举类RetentionPolicy 中有说明)： SOURCE：在源文件中有效（即在源文件中保留） CLASS：在class 文件中有效（即在CLASS 文件中保留） RUNTIME：在运行时有效（即在运行时保留） 123456789101112131415161718192021public enum RetentionPolicy &#123; /** * Annotations are to be discarded by the compiler. */ SOURCE, /** * Annotations are to be recorded in the class file by the compiler * but need not be retained by the VM at run time. This is the default * behavior. */ CLASS, /** * Annotations are to be recorded in the class file by the compiler and * retained by the VM at run time, so they may be read reflectively. * * @see java.lang.reflect.AnnotatedElement */ RUNTIME&#125; @Documented@Documented 用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员。 @Inherited@Inherited 是一个标记注解， 阐述了某个被标注的类型是被继承的， 参考文档 深入理解Java：注解（Annotation）自定义注解入门 秒懂，Java 注解 （Annotation）你可以这样学 深入了解Java 发射机制","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"Hadoop学习笔记⑤","slug":"hadoop学习笔记⑤","date":"2019-06-29T01:21:33.000Z","updated":"2019-07-11T11:25:25.272Z","comments":true,"path":"2019/06/29/hadoop学习笔记⑤/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/29/hadoop学习笔记⑤/","excerpt":"云计算云计算是分布式计算、并行计算、效用计算、网络存储、虚拟化、负载均衡等计算机和网络技术发展融合的产物。 云计算主要包括3种类型： IaaS、PaaS 和SaaS。 云数据库云数据库是部署和虚拟化在云计算环境中的数据库。 MapReduce思想: 分而治之， 把一个大的数据块拆分为多个小的数据块在不同的机器上并行处理。","text":"云计算云计算是分布式计算、并行计算、效用计算、网络存储、虚拟化、负载均衡等计算机和网络技术发展融合的产物。 云计算主要包括3种类型： IaaS、PaaS 和SaaS。 云数据库云数据库是部署和虚拟化在云计算环境中的数据库。 MapReduce思想: 分而治之， 把一个大的数据块拆分为多个小的数据块在不同的机器上并行处理。 Map - Reduce 任务： 通常运行为数据存储节点上， 由此可使计算任务和数据存储在同一个节点之上， 无需额外的数据传输开销， 当Map 任务结束之后， 会生成以&lt;K, V&gt; 形式的中间结果， 然后分配给多个Reduce 任务去执行， 具有相同的Key 的任务会被分配到同一个Reduce 任务， Reduce 任务会对中间结果进行处理， 得到最终结果， 然后输出的分布式配置文件中。 注意： 不同的Map 任务之间以及不同的Reduce 任务之间是不会发生任何的信息交换。 工作流程 首先使用InputFormat 模块做Map 前的预处理， 然后将文件切分为逻辑上多个InputSplit， InputSplit 是Map Reduce 对文件进行处理和运算的处理单位， 只是一个逻辑概念， 并没有进行实际的切分， 只是记录了需要处理的数据的位置和长度。 由于InputSplit 是逻辑分而不是物理分， 所以还需要通过RecordReader 根据InputSplit 中的信息来处理InputSplit 中的具体信息， 加载数据并转换成适合Map 任务读取的键值对. Map 任务会根据用户自定义的映射规则， 输出一系列的&lt;K, V&gt; 作为中间结果 在将Map 任务输出的中间结果交给Reduce 任务处理之前， 需要经过shuffle 处理， 从无序的&lt;key, value&gt; 到有序的&lt;key, value-list&gt;。 Reduce 以一系列&lt;key, value-list&gt; 作为输入， 按照用户定义的逻辑， 将处理结果输出到OutFormat 模块。 OutputFormat 会验证输出目录是否存在, 或者输出类型是否符合配置文件中的配置类型, 如果都满足则输出到HDFS 中. Shuffle 过程详解Shuffle 是整个MapReduce 的核心， 对Map 输出结果进行分区、排序以及合并等处理并交给Reduce 的过程， Shuffle 分为Map 端操作和Reduce 端操作。 在Map 端的Shuffle 过程 每个Map 任务都会被分配到一个缓存， 默认大写是100M Map 的输出结果会暂时被写入缓存中， 当缓存满时， 会启动溢写操作， 把缓存中的数据进行分区， 并对同一个分区里面的数据进行合并（合并是指对有着相同Key 值的数据加起来， 以减少需要溢写到磁盘的数据量）， 然后写入磁盘文件（此举可以减少对磁盘的I/O 操作）， 并清空缓存， 每次溢写操作， 都会生成一个溢写文件， 当Map 任务结束之后， 这些溢写文件就会被合并成一个大的磁盘文件(此时会对所有的数据进行合并操作)， 最终会生成一个统一的大文件， 但是这个大文件是被分区的， 然后通知Reduce 来领取属于自己的处理数据， 不同分区的数据会被分发到不同的Reduce 上执行。 为了使得Map 的写入操作可以持续进行， 需要让缓存留有一定的空间， 而不是等缓存满了之后在进行溢写操作。此处会有一个溢写比例。 合并例子： &lt;k1, v1&gt; 与&lt;k1, v2&gt; 合并成&lt;k1, &lt;v1, v2&gt;&gt; 在Reduce 端的Shuffle 过程Reduce 任务从Map 端的不同机器领取属于自己的那一份数据， 然后对数据进行合并， 接着进行处理。 两个流程： 领取数据： Reduce会开多个线程去向多个Map 任务所在的机器去领取数据 归并数据： 从Map 端领取回来的数据会先保存在缓存中， 如果缓存占满了， 会触发溢写操作（此时也会对数据进行合并）， 将数据写到磁盘中， 每次溢写都会产生一个溢写文件， 最后当所有数据都被领取完毕之后， 就会对所有的溢写文件进行归并排序 将数据输入到Reduce 任务：经过多轮归并会生成多个大文件（Map 端是一个）， 这几个大文件不会继续归并成一个新的大文件， 而是直接输入到Reduce 任务中， 接下来Reduce 任务会执行操作， 将处理结果输出到HDFS 中。","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wt-git-repository.github.io/tags/Hadoop/"}]},{"title":"Hadoop学习笔记④","slug":"hadoop学习笔记④","date":"2019-06-28T01:05:05.000Z","updated":"2019-07-08T01:15:52.718Z","comments":true,"path":"2019/06/28/hadoop学习笔记④/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/28/hadoop学习笔记④/","excerpt":"NoSQL典型的NoSQL 数据库通常包括键值数据库、列族数据库、文档数据库和图数据库。 键值数据库（Redis）键值数据库会使用一个哈希表， 这个表有一个特定的Key 和一个指定特定的Value。 键值数据库可以划分为内存键值数据库和持久化键值数据库。 弱点： 条件查询、多表查询， 不支持回滚， 无法支持事务。","text":"NoSQL典型的NoSQL 数据库通常包括键值数据库、列族数据库、文档数据库和图数据库。 键值数据库（Redis）键值数据库会使用一个哈希表， 这个表有一个特定的Key 和一个指定特定的Value。 键值数据库可以划分为内存键值数据库和持久化键值数据库。 弱点： 条件查询、多表查询， 不支持回滚， 无法支持事务。 列族数据库（BigTable、HBase）列族数据库一般采用列族数据库模型， 数据库由多个行构成， 每行数据包括多个列族， 不同的行可以具有不同数量的列族， 属于同一列族的数据会被存储在一起。 文档数据库（MongoDB）在文档数据库中， 文档是数据库的最小单位。 虽然每一种文档数据库的部署有所不同， 但是大都假定文档以某种标准化格式封装并对数据进行加密， 同时用多种格式进行解密， 包括XML、 YAML 和JSON 等等。 文档数据库既可以根据键Key来构建索引， 也可以基于文档内容来构建索引。 图数据库图数据库是以图论为基础， 一个图是一个数学概念， 用来表示一个对象集合， 包括顶点以及连接顶点的边。 NoSQL的三大基石NoSQL 的三大基石包括CAP、 BASE 和最终一致性。 NewSQL 数据库（Spanner）NewSQL 不但具有NoSQL 对海量数据的存储管理能力， 还保持了传统数据库支持ACID 和SQL 等特性。 各类数据库的汇集","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wt-git-repository.github.io/tags/Hadoop/"}]},{"title":"Hadoop学习笔记③","slug":"hadoop学习笔记③","date":"2019-06-27T04:53:00.000Z","updated":"2019-09-10T12:12:41.590Z","comments":true,"path":"2019/06/27/hadoop学习笔记③/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/27/hadoop学习笔记③/","excerpt":"分布式数据库HBaseHBase 是BigTable的开源实现， 高可靠、高性能、面向列、可伸缩的分布式数据库， 主要用来存储非结构化数据和半结构化的松散数据， HBase支持超大规模数据存储， 可以通过水平扩展的方式， 利用廉价计算机集群处理超过10亿行数据和数百位列组成的数据表。 BigTableBigTable是一个分布式存储系统， 利用谷歌提出的MapReduce分布式并行计算模型来处理海量初级， 利用GFS作为底层数据存储， 并采用Chubby提供协同服务管理， 可以扩展到PB级别的数据和上千台机器。","text":"分布式数据库HBaseHBase 是BigTable的开源实现， 高可靠、高性能、面向列、可伸缩的分布式数据库， 主要用来存储非结构化数据和半结构化的松散数据， HBase支持超大规模数据存储， 可以通过水平扩展的方式， 利用廉价计算机集群处理超过10亿行数据和数百位列组成的数据表。 BigTableBigTable是一个分布式存储系统， 利用谷歌提出的MapReduce分布式并行计算模型来处理海量初级， 利用GFS作为底层数据存储， 并采用Chubby提供协同服务管理， 可以扩展到PB级别的数据和上千台机器。 HBase 利用Hadoop MapReduce来处理HBase中的海量数据， 实现高性能计算 利用Zookpper来作为协同服务， 实现稳定服务和失败恢复 利用HDFS作为高可靠的底层存储， 利用廉价集群提供海量数据存储能力 为了方便在HBase上进行数据处理， Sqoop为HBase提供了高效、便捷的RDBMS数据导入功能， Pig和Hive为HBase提供了高层语言支持。 HBase 数据类型HBase 是一个稀疏、多维度、排序的映射表， 这张表的索引是行健、列族、列限定符和时间戳， 每个值是一个未经解释的字符串byte[]， 没有数据类型。 用户在表中存储数据， 每一行都有一个可排序的行键和任意多的列。 表在水平方向有一个或者多个列族组成， 一个列族可以包含任意多个列， 同一个列族里面的数据存储在一起。 列族支持动态扩展， 无需预定列的数量以及类型， 所有列均以字符串形式存储， 同一张表里面的每一行数据都可以有截然不同的列。 在HBase 中执行更新操作时， 并不会删除数据旧的版本， 而是生成一个新的版本， 旧的版本仍然保留， HBase 可以对允许保留的版本的数量进行设置。 HBase 数据模型的相关概念 表 行 列族 列限定符 单元格 时间戳 HBase 使用坐标来定位表中数据， 四维坐标[行键， 列族， 列限定符， 时间戳]。 行式数据库和列式数据库行式数据库主要适合于小批量的数据处理 列式数据库适合于批量数据处理和即时查询， 可以降低I/O开销， 支持大量并发用户查询， 其数据处理速度比传统方法快100倍， 因为仅仅需要处理可以回答这些查询的列， 而不是分类整理与特定查询无关的数据行， 具有较高的数据压缩比。 HBase 的实现原理HBase 的功能组件： 库函数（链接到每个客户端） 一个Master主服务器： 负责管理和维护HBase 表的分区信息。 许多个Region服务器: 负责维护存储和维护分配给自己的Region， 处理来自客户端的读写请求。 客户端并不是直接从Master主服务器读取数据， 而是在获取Region 的存储位置信息后， 直接从Region上读取数据， HBase 客户端并不依赖Master 而是借助 Zookeeper 来获取Region的位置信息的。 表与Region 对于每一个HBase 表而言， 表中的行是根据行键的值的字段序进行维护的， 由于表中包含的行的数量可能非常庞大， 无法存储在一台机器上， 所有需要分布式存储， 需要根据行键的值对表中的行进行分区， 每个行区间构成一个Region， 它是负载均衡和数据分发的基本单位， 这些Region会被分发到不同的Region服务器。 Region 的定位 每个Region 都有一个Region ID（表名 + 开始主键 + Region ID）来标识它的唯一性。 Region 映射表（元数据表 or .META表）： Region 标识符 + Region服务器标识符。 -ROOT-表（根数据表）， 当.META表的条目非常多时， 也会被分裂成多个Rigion， 其映射表是根数据表-ROOT-表。 HBase 运行机制访问流程： 首先访问Zookeeper。 获取-ROOT-表的位置信息， 然后访问-ROOT-表， 获取.META表的信息， 接着访问.META表， 找到所需的Region具体位于哪个Region服务器， 最后找到该Region服务器读取数据。（客户端会缓存位置信息， 提高寻址效率） Zoopkeeper 中保存了-ROOT-表的地址和Master 的地址， 客户端可以通过访问ZoopKeeper 获得-ROOT-表的地址， 最后通过三级寻址来找到所需要的地址。 HBase自身不具备数据复制和维护数据副本的功能， 而HDFS可以为HBase提供这些支持。 Region 服务器的工作原理Region 服务器内部管理一系列的Region 对象和一个HLog 文件。 HLog 文件： 磁盘上面的记录文件， 它记录着所有的更新操作。 Region 对象： 由多个Store 组成， 每个Store 对应表中的一个列族的存储 Store： 包含一个MemStore 和若干个StoreFile， 其中， MemStore 是内存中的缓存， 保存最近更新的数据， StoreFile 是磁盘中的文件（B+树结构）， StoreFile 在底层的实现方式是HDFS 文件系统的HFile， 而HFile 的数据块通过采用压缩的方式存储， 压缩之后可以大大减少网络IO和磁盘IO。 用户读写数据的过程当用户写入数据时， 会被分配到相应的Region 服务器执行操作， 用户数据被写入MemStore 和HLog 中， 当操作写入Hlog 之后， commit()调用才会将其返回给客户端。 当用户读取数据时， Region 服务器首先会访问MemStore 缓存， 如果数据不在缓存中， 才会到磁盘上面的StoreFile 中去寻找。 缓存的刷新MemStore的缓存容量有限， 所有会周期性地将MemStore 的数据存入到磁盘的StoreFile 中， 同时在HLog中写入一个标记， 以表示缓存中的内容已经写入到磁盘中， 每次刷新操作都会在磁盘中生成一个新的StoreFile， 所有每个Store 都会含有多个StoreFile。 Region 每次启动都会去检查HLog， 以确保缓存中的数据已经写入到StoreFile 中， 若没有， 则先将更新写入MemStore， 再刷新缓存， 写入到StoreFile中， 最后删除旧的HLog。 StoreFile 的合并当StoreFile文化的数量达到一个阀值时， 会进行合并操作。 Store 的工作原理 HLog 的工作原理每个Region 服务器维护共同一个HLog, 当Region服务器出现故障时， Zoopkeeper 会同时Master， 首先处理故障服务器上遗留的HLog 文件， 根据Region对象拆分日志， 分配到各自对象的目录下， 然后再将失效的Region分配到可用的Region服务器中， 新的服务器会将分配到给自己的HLog 日志重新执行一边， 然后刷新缓存。","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wt-git-repository.github.io/tags/Hadoop/"}]},{"title":"Hadoop学习笔记②","slug":"hadoop学习笔记②","date":"2019-06-25T03:17:14.000Z","updated":"2019-09-10T12:12:41.590Z","comments":true,"path":"2019/06/25/hadoop学习笔记②/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/25/hadoop学习笔记②/","excerpt":"","text":"分布式文件系统HDFS计算机集群架构集群中的计算机结点存放在机架上， 每个机架可以存放8-64个结点， 同一个机架上的不同节点通过网络互联， 多个不同机架之间采用另一级网络或交换机互联。 分布式文件系统的结构分布式文件系统在物理结构上是由计算机集群中的多个节点构成的， 这些节点分为两类： 主节点或者是名称节点(NameNode)：负责文件和目录的创建、删除和重命名等，同时管理着数据节点和文件块的映射关系， 客户端只有访问名称结点才能请求文件块所在的位置，进而找到相应位置读取所需文件块。 从结点或者是数据节点(DataNode)：负责数据的存储和读取， 在存储时， 由名称结点分配存储位置， 然后由客户端把数据直接写入相应数据节点 在读取数据时， 客户端从名称节点获取数据节点和文件块的映射关系， 然后就可以到相应位置访问文件块。 数据节点也要根据名称节点的命令创建、删除数据块和冗余复制。 为了保证数据的完整性， 分布式文件系统通常采用多副本存储， 文件块会被复制为多个副本， 存储在不同的节点上， 而且存储同一个文件块的不同副本的各个节点会分布在不同机架上， 这样， 在单个节点出现故障时， 就可以快速调用副本重启单个节点上的计算过程， 而不用重启整个计算过程。 分布式文件系统是针对大规模数据存储而设计的（TB级别）， 规模过小会影响性能。 HDFS特性 兼容廉价硬件设备 流数据读写 大数据集 简单的文件模型 强大的跨平台特性 不适合低延迟数据访问， 主要面向大规模数据批量处理而设计的， 采用流式数据读取， 具有很高的吞吐率， 高延迟（对于低延时要求的应用程序， HBase是一个更好的选择） 无法高效存储大量小文件 不支持多用户写入以及任意修改文件 HDFS的 相关概念块在传统的文件系统中，为了提高磁盘的读写效率， 一般以数据块为单位， 而不是以字节为单位。 HDFS也采用了块的概念， 默认一个块大小是64MB， 在HDFS中的文件会被拆分成多个块， 每个块作为独立的单元进行存储。 HDFS在块的大小设计明显大于普通文件系统， 为的是最小化寻址开销（磁盘寻道开销和数据块定位开销） 通常MapReduce中的Map任务一次只处理一个块中中的数据 HDFS采用抽象块概念可以带来几个好处： 支持大规模文件存储： 一个大规模文件可以被拆分成若干个文件块， 不同的文件块被分发到不同的节点上， 因此一个文件的大小不会受到单个节点的存储容量的限制， 可以远大于任意节点的存储容量。 简化系统设计：方便计算一个节点可以存储多少个文件块，其次，方便元数据的管理，元数据不需要和文件夹一起存储，可以由其他系统负责管理元数据。 适合数据备份：每个文件块可以冗余存储到多个节点上，大大提高了系统的容错性和可用性。 名称节点和数据节点在名称节点（NameNode）中， 负责管理分布式文件系统的命名空间（Namespace）， 保存了两个核心的数据结构， 即FsImage和EditLog。 FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据 EditLog是操作日志文件，记录了所有针对文件的创建、删除、重命名等操作。 名称节点记录了每个文件各个块所在的数据节点的位置信息， 但是不会持久化保存这些数据， 而是在系统每次启动时， 扫描所有数据节点然后重构这些信息。 名称节点启动时会处于安全模式， 对外只提供读操作。 名称节点在启动时， 会将FsImage加载到内存中， 然后执行EditLog文件中的各项操作， 使得内存中的元数据保持最新， 然后创建一个新的FsImage和空的EditLog文件， 名称节点启动成功之后进入正常运行状态， HDFS中的更新操作都会写入EditLog中， 而不是直接写入FsImage中， 因为在分布式文件系统中， FsImage都十分庞大， 如果每次的更新都往FsImage里面添加， 会带来十分大的系统开销， 而Editlog十分小， 写入操作是十分高效的。 数据节点（DataNode）是分布式文件系统HDFS的工作节点， 负责数据的存储和读取， 会根据客户端或者名称节点的调度来进行数据的存储和检索， 并向名称节点定期发送自己所存储的块列表。 第二名称节点在名称节点运行期间， HDFS会不断发生更新操作， 会使EditLog文件逐渐变大， 当名称节点重启时， 需要逐条执行EditLog的记录， 这个过程就会变得十分缓慢。 为了解决这个问题， HDFS采用第二名称节点（Secondary NameNode）， 来完成合并EditLog与FsImage的合并操作， 减少EditLog的文件大小， 缩短名称节点重启时间 HDFS体系结构HDFS 采用主/从结构模型， 一个HDFS集群包括一个名称节点和若干个数据节点。 名称节点是中心服务器， 负责管理文件系统的命名空间以及客户端对文件的访问。 集群中的数据节点一般是一个节点运行一个数据节点进程， 负责处理文件系统客户端的读写请求， 在名称节点的统一调度之下进行数据块的创建、删除和复制等操作，每个数据节点的数据是保存在本地Linux文件系统中的， 每个数据节点会周期性地向名称节点发送“心跳”信息， 报告自己的状态， 对于没有按时发送心跳信息的数据结构会被标记为死机， 不会在给它分配任务。 用户在使用HDFS时， 仍然可以像普通文件系统那样， 使用文件名去存储和访问文件， 在系统内部， HDFS会将一个文件切分为若干个数据块， 这些数据块会被分布存储到若干个数据节点上。 访问流程：客户端访问文件， 将文件名发送给名称节点-&gt;名称节点根据文件名查找到对应的数据块-&gt;根据数据块信息找到数据节点的位置-&gt; 把数据节点的位置发送给客户端-&gt;客户端直接访问数据节点获取数据信息 HDFS命名空间管理HDFS通信协议RPC HDFS客户端Shell and Java API HDFS 的存储原理数据的冗余存储作为分布式文件系统， 为了保证系统的容错性和可用性， HDFS采用了多副本方式对数据进行冗余存储， 通过一个数据块的多个副本会被分布到不同的数据节点上。 好处： 加快数据传输速度： 当多个客户端需要同时访问同一个文件时， 可以让各个客户端分别从不同的数据块副本读取数据， 加快数据的传输速度 检查数据错误 保证数据的可靠性 数据的存取策略数据存放：HDFS采用机架为基础的数据存放策略， HDFS默认的冗余复制因子是3， 每一个文件块会被同时保存到3个地方， 其中， 有两份副本放在同一个机架上的不同机器上， 第三个副本放在不同机架的机器上。 数据读取 数据复制 数据错误与恢复名称节点出错 把名称节点上的元数据同步存储到其他文件系统 运行一个第二名称节点 数据节点出错 心跳检测 数据出错 由于网络传输和磁盘错误等因素造成的数据错误。 采用md5和sha1对数据块进行校验。 HDFS数据读写过程（待写）","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wt-git-repository.github.io/tags/Hadoop/"}]},{"title":"Hadoop学习笔记①","slug":"hadoop学习笔记①","date":"2019-06-25T01:57:47.000Z","updated":"2019-07-08T01:15:52.703Z","comments":true,"path":"2019/06/25/hadoop学习笔记①/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/25/hadoop学习笔记①/","excerpt":"Hadoop 简介Hadoop 是一个开源的、可运行于大规模集群上的分布式计算平台， 实现了MapReduce计算模型和分布式文件系统HDFS等功能 Hadoop的核心是分布式文件系统HDFS和MapReduce， HDFS是针对谷歌GFS的开源实现， 是面向普通硬件环境的分布式文件系统， 具有支持大规模数据的分布式存储。 MapReduce允许用户在不了解分布式系统底层细节的情况下开发并行应用程序， 整合分布式文件系统上的数据， 保证分析和处理数据的高效性。","text":"Hadoop 简介Hadoop 是一个开源的、可运行于大规模集群上的分布式计算平台， 实现了MapReduce计算模型和分布式文件系统HDFS等功能 Hadoop的核心是分布式文件系统HDFS和MapReduce， HDFS是针对谷歌GFS的开源实现， 是面向普通硬件环境的分布式文件系统， 具有支持大规模数据的分布式存储。 MapReduce允许用户在不了解分布式系统底层细节的情况下开发并行应用程序， 整合分布式文件系统上的数据， 保证分析和处理数据的高效性。 Hadoop生态系统 HDFSHadoop分布式文件系统是Hadoop项目的两大核心之一， 具有处理超大数据、流式处理、可以运行在廉价商用服务其上等优点, HDFS在访问应用层程序时，具有很高的吞吐率， 因此对于超大数据集的应用程序而言，选择HDFS作为底层数据存储是较好的选择。 HBaseHBase是一个高可靠性、高性能、可伸缩、实时读写、分布式的列式数据库， 一般采用HDFS作为底层数据存储 HBase是针对Google BigTable的开源实现， 具有强大的非结构化数据存储能力 HBase具有良好的横向扩展能力 MapReduceMapReduce是一种编程模型， 用于大规模数据的并行运算， 它将复杂、运行于大规模集群上的并行计算过程高度抽象到了两个函数–Map和Reduce上 MapReduce的核心思想是分而治之， 它把输入的数据切分为若干个数据块， 分发给一个主节点管理下各个分结点来共同并行完成， 最后， 通过整合各个结点的中间结果得到最终结果。 HiveHive是基于Hadoop的一个数据存库工具， 可以用于对Hadoop文件中的数据集进行整理、特殊查询和分析存储， 提供了类似SQL语言的查询语言Hive QL， 可以通过Hive QL快速实现简单的MapReduce统计， Hive自身可以将Hive QL语句装换成MapReduce任务进行运行， 适合数据存库的统计分析。 PigPig是一种数据流语言和运行环境， 适合使用Hadoop和MapReduce平台来查询大型半结构化数据集 Mahout数据挖掘等智能应用相关 ZookeeperZookeeper是针对谷歌Chubby的一个开源实现，是高效可靠的协同工作系统， 提供分布式锁之类的基本服务， 用于构建分布式应用， 减轻分布式应用程序所承担的协调任务。 FlumeFlume是Cloudera提供的一个高可用、高可靠、分布式的海量日志采集、聚合和传输的系统。 Sqoop(SQL-to-Hadoop)主要用于Hadoop和关系数据库之间交换数据 Ambari一种基于Web的根据， 支持Hadoop集群的安装、部署、配置和管理。","categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wt-git-repository.github.io/tags/Hadoop/"}]},{"title":"hadoop伪分布式安装（配置文件相关）","slug":"hadoop伪分布式安装（配置文件相关）","date":"2019-06-20T07:06:09.000Z","updated":"2019-07-08T01:15:52.702Z","comments":true,"path":"2019/06/20/hadoop伪分布式安装（配置文件相关）/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/20/hadoop伪分布式安装（配置文件相关）/","excerpt":"推荐使用docker去部署hadoop集群Github - Big Data Europe","text":"推荐使用docker去部署hadoop集群Github - Big Data Europe 手动安装 /etc/hadoop hadoop-env指定JDK 路径 1export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.201.b09-2.el7_6.x86_64 core-site.xml12345678910111213&lt;configuration&gt; &lt;!--hdfs主节点地址--&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://namenode:9000&lt;/value&gt; &lt;/property&gt; &lt;!--指定hadoop运行时产生文件的存储目录--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/root/hadoop/data/&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1234567&lt;configuration&gt; &lt;!--hdfs副本数--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml1234567&lt;configuration&gt; &lt;!--指明mapreduce的资源调度程序--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml12345678910111213141516&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;!--yarn主结点--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;resourcemanager&lt;/value&gt; &lt;!--yarn主结点--&gt; &lt;/property&gt; &lt;!--指定map传递数据给reduce的机制--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://wt-git-repository.github.io/tags/hadoop/"}]},{"title":"hadoop对海量数据处理的解决思路","slug":"hadoop对海量数据处理的解决思路","date":"2019-06-20T03:39:06.000Z","updated":"2019-07-08T01:15:52.719Z","comments":true,"path":"2019/06/20/hadoop对海量数据处理的解决思路/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/20/hadoop对海量数据处理的解决思路/","excerpt":"","text":"如何解决海量数据的存储问题 如何解决海量数据的计算MapReduce Map: 计算本地数据， 在各个结点下计算， 本地并发 Reduce: 全局处理， 可以分组处理， 对各个Map的中间结果进行处理","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://wt-git-repository.github.io/tags/hadoop/"}]},{"title":"《浪潮之巅》读书笔记","slug":"《浪潮之巅》读书笔记","date":"2019-06-20T01:52:59.000Z","updated":"2019-07-08T01:15:52.725Z","comments":true,"path":"2019/06/20/《浪潮之巅》读书笔记/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/20/《浪潮之巅》读书笔记/","excerpt":"","text":"近百年来，总有一些公司很幸运地、有意识或无意识地站在技术革命的浪尖之上， 一旦处于那个位置， 即使不做任何事情， 也可以随着波浪顺顺当当地先前漂泊个十年甚至更长的世纪。 在这十几年间， 它们代表着科技的浪潮， 直到下一波浪潮的来临。 AT&amp;T由电话之父压力上大·贝尔创立， 第一次实现了人类的远程实时的交互通信。 它创建的贝尔实验室， 走出了计算机的Unix系统和C语言等等 错过了2000年前后的网络革命和90年代中期延续至今的无线通信，这两个极佳的发展机遇， 并丢了性命 AT&amp;T后来被分成了三部分，从事电信业务的AT&amp;T、从事设备制造业务的朗讯和从事计算机业务的NCR。 朗讯后来为MCI和Sprint提供设备，曾一度股价暴涨， 但是如果需要保证股票的持续增长， 它的销售额和利润就必须不断超过华尔街的预期， 为了支撑这一个高股价， 朗讯走了一步败笔的险棋， 具体做法是， 有朗讯借钱给各个公司买朗讯的设备， 从而促进销售额的增长， 但是到了后来互联网泡沫破灭之后， 借钱买设备的公司统统倒闭了， 朗讯这笔钱一下子变成了收不回来的账了， 从此便不得不关闭贝尔实验室的几乎全部研究部门， 走向没落。 当一个公司没有人对它有控制时，它的长期发展就会出现问题， 在上个世纪九十年代， AT&amp;T已经不属于任何一个人，任何一个机构，没有人对它的发展着想， 而跟多的是， 从华尔街到它的高管和员工， 都希望从它身上快快地捞一笔。","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://wt-git-repository.github.io/tags/随笔/"}]},{"title":"Vue常用命令","slug":"Vue常用命令","date":"2019-06-19T07:59:57.000Z","updated":"2019-07-08T01:15:52.697Z","comments":true,"path":"2019/06/19/Vue常用命令/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/19/Vue常用命令/","excerpt":"","text":"起步安装12345# 安装vuenpm install vue# 安装vue-clinpm install -g @vue/cli 项目创建123vue create my-project# ORvue ui 相关文档 Vue.js官方文档 Vue CLI官方文档","categories":[],"tags":[{"name":"Vue","slug":"Vue","permalink":"https://wt-git-repository.github.io/tags/Vue/"}]},{"title":"《JAVA并发编程的艺术》读书笔记⑥","slug":"《JAVA并发编程的艺术》读书笔记⑥","date":"2019-06-17T06:49:49.000Z","updated":"2019-07-08T01:15:52.724Z","comments":true,"path":"2019/06/17/《JAVA并发编程的艺术》读书笔记⑥/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/17/《JAVA并发编程的艺术》读书笔记⑥/","excerpt":"Java并发编程基础线程优先级在Java线程中，通过一个整型成员变量priority来控制优先级， 优先级分为从1-10， 在线程构建的时候可以通过setPriority(int)方法来修改优先级， 默认优先级是5， 优先级高的线程分配时间片的数量要多余优先级低的线程。 针对阻塞频繁的线程需要设置较高的优先级。 偏重计算的线程设置较低的优先级。","text":"Java并发编程基础线程优先级在Java线程中，通过一个整型成员变量priority来控制优先级， 优先级分为从1-10， 在线程构建的时候可以通过setPriority(int)方法来修改优先级， 默认优先级是5， 优先级高的线程分配时间片的数量要多余优先级低的线程。 针对阻塞频繁的线程需要设置较高的优先级。 偏重计算的线程设置较低的优先级。 线程的状态 NEW： 初始状态， 线程被构建， 但是还没有调用start()方法 RUNNABLE： 运行状态， Java线程将操作系统中的就绪和运行两种状态笼统地称作“运行中” BLOCKED：阻塞状态，表示线程阻塞干锁 WAITING：等待状态， 表示线程进入等待状态，进入该状态的线程表示当前线程需要等待其他线程作出一些特定的动作 TIME_WAITING：超时等待状态， 它是可以在指定的时间自行返回 TERMINATED：终止状态，表示当前线程已经执行完毕 Daemon线程Daemon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持工作，当一个JAVA虚拟机中不存在非Daemon线程的时候，JAVA虚拟机将会退出。 可以使用Thread.setDaemon(true)来设置Daemon线程。 Deamon线程被用作支持性工作，但是在JAVA虚拟机退出时，Daemo线程中的finally块不一定会执行。 对象、监视器、同步队列和执行线程之间的关系 线程间的通信等待/通知机制 有一些细节上的东西需要注意一下： 使用wait、notify、notifyAll时需要对调用对象加锁。 调用wait方法后，线程状态由RUNNING变成WAITING，并将当前线程放置在对象的等待队列中。 notify和notifyAll调用后，等到线程依旧不会从wait返回，而是要等本线程释放锁之后，等待线程才有机会返回 notify是将等待队列中的一个等待线程从等待队列中移到同步队列中 notifyAll是将等待队列中所有的线程全部移到同步队列中，被移动的线程状态由WAITING变为BLOKCED 从wait方法返回后重新执行的前提是获得对象的锁 管道输入\\输出流管道输入输出流主要用于线程之间的数据传输，而传输的媒介是内存。 管道输入输出流主要包括：PipedOutputStream、PipedInputStream、PipedReader和PipedWriter，前两种面向字节，后两种面向字符。 123PipedWriter out = new PipedWriter();PipedReader in = new PipedReader();out.connect(in); 对于Piped类型的流，必须先要进行绑定（调用connect方法） Thread.join()的使用如果线程A执行了thread.join()语句， 意味着当前线程A等到thread线程终止之后才从thread.join()返回。 除此之外， 线程Thread还提供了join(long millis)和join(long millis, int nanos)两个具备超时特性的方法， 表示：如果线程thread在特定的超时时间里没有终止，那么将会从该超时方法中返回。 ThreadLocal的使用ThreadLocal是线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构， 这个结构被附带在线程上，也就是说一个线程可以根据一个ThreadLocal对象查询到绑定到这个线程上的一个值。 可以通过set(T)方法来设置一个值，在当前线程下再过get()方法获取到原先设置的值。","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"《JAVA并发编程的艺术》读书笔记⑤","slug":"《JAVA并发编程的艺术》读书笔记⑤","date":"2019-06-17T01:20:09.000Z","updated":"2019-07-08T01:15:52.724Z","comments":true,"path":"2019/06/17/《JAVA并发编程的艺术》读书笔记⑤/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/17/《JAVA并发编程的艺术》读书笔记⑤/","excerpt":"双重检查锁定引发的思考在学习双重版本版本的单例模式的时候， 我书写了如下代码 123456789101112131415161718public class Singleton &#123; private static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; //检查实例，如果不存在，就进入同步代码块 if (uniqueInstance == null) &#123; //只有第一次才彻底执行这里的代码 synchronized(Singleton.class) &#123; //进入同步代码块后，再检查一次，如果仍是null，才创建实例 if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 这段代码表面上很完美， 但是在高并发的环境下， 容易触发里面一个潜在的BUG， 问题的根源出自于uniqueInstance = new Singleton(); , 因为在线程读取到uniqueInstance不为null的时候， 该变量引用的对象有可能还没有完成初始化。","text":"双重检查锁定引发的思考在学习双重版本版本的单例模式的时候， 我书写了如下代码 123456789101112131415161718public class Singleton &#123; private static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; //检查实例，如果不存在，就进入同步代码块 if (uniqueInstance == null) &#123; //只有第一次才彻底执行这里的代码 synchronized(Singleton.class) &#123; //进入同步代码块后，再检查一次，如果仍是null，才创建实例 if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 这段代码表面上很完美， 但是在高并发的环境下， 容易触发里面一个潜在的BUG， 问题的根源出自于uniqueInstance = new Singleton(); , 因为在线程读取到uniqueInstance不为null的时候， 该变量引用的对象有可能还没有完成初始化。 将uniqueInstance = new Singleton()拆分为以下三行伪代码：123memory = allocate(); // 1:分配对象的内存空间ctorInstance(memory); // 2:初始化对象instance = memory; // 3：设置instance指向刚刚分配的内存空间 在Java语言规范中， 所有线程在执行Java程序是必须要遵守intra-thread semantics， 以保证重排序不会改变单线程内的程序执行结果， 重排序在没有改变单线程程序执行结果的前提下， 可以提高程序的执行性能。 单线程下： 多线程下触发Bug的原因（线程A执行到uniqueInstance = new Singleton()， 线程B执行到第一个if (uniqueInstance == null)）： 如果按上面的时序图执行， 线程B有可能会读取到还没有初始化成功的对象。 基于volatile的解决方案1234567891011121314151617181920public class Singleton &#123; //volatile保证，当uniqueInstance变量被初始化成Singleton实例时，多个线程可以正确处理uniqueInstance变量 private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; //检查实例，如果不存在，就进入同步代码块 if (uniqueInstance == null) &#123; //只有第一次才彻底执行这里的代码 synchronized(Singleton.class) &#123; //进入同步代码块后，再检查一次，如果仍是null，才创建实例 if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 当变量声明为volitle之后，创建该对象的中所触发的指令重排序， 将会在多线程环境中禁止。","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"乐融软件面经","slug":"乐融软件面经","date":"2019-06-14T15:18:56.000Z","updated":"2019-07-08T01:15:52.727Z","comments":true,"path":"2019/06/14/乐融软件面经/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/14/乐融软件面经/","excerpt":"","text":"部分面经（笔试与面试） &amp; 与 &amp;&amp; 的区别 对称算法与非对称算法有哪些， 两种算法的区别是什么 TCP三次握手四次挥手 JVM堆内存和栈内存（注意Spring的创建， 引用的传递） GC 单例模式 二分查找 递归算法 I/O流 MySQL的ACID 死锁， 如何避免 Spring 事务 微服务 SSL的算法 Http与Https的区别 项目经历 获奖经历 面试流程笔试-&gt;根据笔试的情况，让你讲解几道题（我被抽到的是：加密算法， JVM， 单例模式， &amp;与递归算法）-&gt;根据你简历上的内容提问-&gt;根据他手上的题纲去提问-&gt;聊个人情况、期盼薪资等等","categories":[],"tags":[{"name":"面经","slug":"面经","permalink":"https://wt-git-repository.github.io/tags/面经/"}]},{"title":"如何成长为一名合格的架构师","slug":"如何成长为一名合格的架构师","date":"2019-06-13T01:49:07.000Z","updated":"2019-07-08T01:15:52.731Z","comments":true,"path":"2019/06/13/如何成长为一名合格的架构师/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/13/如何成长为一名合格的架构师/","excerpt":"今日， 在Google检索高性能、高可用以及高并发相关的书籍的时候， 无意中在知乎中看到了一场讨论， 从前辈们的分享中， 我猛然发现， 这么久以来， 我把大部分时间都花在了研究各种成熟的架构上面， 而忽略了底层的技术， 如数据结构、计算机网络、数据库存储原理等等， 殊不知， 一名合格的架构师永远都不是以你会用多少多少框架为衡量标准的， 相反， 要想成为一名合格的架构师， 数据结构，操作系统，网络原理，数据库原理等这些都是决定着我以后的高度和深度的， 虽然， 道路走的有点偏， 起码我还是能把握的正确的方向的， 因为为了相关的面试， 我也一直在对这些基础知识进行一个系统的学习， 而知道今天， 我才猛然发现这些基础知识的重要性， 还是十分庆幸自己及早看到了知乎的这一场讨论， 感恩！","text":"今日， 在Google检索高性能、高可用以及高并发相关的书籍的时候， 无意中在知乎中看到了一场讨论， 从前辈们的分享中， 我猛然发现， 这么久以来， 我把大部分时间都花在了研究各种成熟的架构上面， 而忽略了底层的技术， 如数据结构、计算机网络、数据库存储原理等等， 殊不知， 一名合格的架构师永远都不是以你会用多少多少框架为衡量标准的， 相反， 要想成为一名合格的架构师， 数据结构，操作系统，网络原理，数据库原理等这些都是决定着我以后的高度和深度的， 虽然， 道路走的有点偏， 起码我还是能把握的正确的方向的， 因为为了相关的面试， 我也一直在对这些基础知识进行一个系统的学习， 而知道今天， 我才猛然发现这些基础知识的重要性， 还是十分庆幸自己及早看到了知乎的这一场讨论， 感恩！ 原文： 作者：匿名用户链接：https://www.zhihu.com/question/22988790/answer/23290615来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 1.你在未来很长一段时间内还不会碰到除你问的分布式以外的问题，我所说的问题是，无论那个项目，未来很长一段时间都不会让你来负责架构，保证高并发高可用高扩展高性能。为什么呢？你经验还远远不够，这里的任何一个词都是需要你投入巨大的精力和时间来研究和实验，并且还需要提供跟你项目匹配的测试环境，来测试你的方案是否能达到高性能高并发等等。总结就是两点，先不说架构，你需要有机会参与到这种项目中来并学习，你需要有能测试你的技术方案的环境。放眼望去，光满足第一点你都要等待机会的来临。否则，你的预期都是水中月境中花。2.你所说的这些东西没有办法直接从任何书本上获得行之有效的方案，如有有，恐怕阿里百度这种企业会多如牛毛，遍地开花。有方案保密是一回事，公开了你看不懂驾驭不了那是另外一回事。3.如果没有机会怎么办呢？可能需要把这些看起来无比牛逼高端的字眼从你脑子里踢出来。好好的学习计算机体系的课程。数据结构，操作系统，网络原理，数据库原理等。以后能站多高取决于对这些东西的掌握程度。不搞学院派，我说的掌握不是大学里形式化的考试。这好懂，这叫广积粮4.在3进行的过程中，动手，找准技术方向。看原理，教程，写demo做测试，一步步来，这步是你迈向开发必经之路，理论转向实践，而且即使参加工作之后，这项活动还会不断进行。高筑墙不多说。5有机会接触到有一定规模的项目了，这个时候可以把你提的高端字眼考虑进来了。尝试学习思考和改进现有方案。相信到这一步已经走向正轨，不会再问这种让人摸不到头脑的问题了。而且也知道怎么学习。 你的问题是很多新手都会碰到的困惑，急功近利的表现。题主题的问题，不知道有多少人工作数年后都不会去考虑，要么项目规模不够，要么无测试环境，要么自己积粮不够无力解决。 以上，水文，看看即可，也期待知友能给出一步能至千里的方案，让我也学习学习 如果题主实在对这些技术感到困惑的话，我推荐你看一本书《淘宝技术这十年》，讲淘宝的技术演变。即使淘宝如此庞大的系统，都是实际生产环境下的问题倒逼各种牛逼方案的诞生。 空看空说无用。 github，stackoverflow，你值得拥有 附上链接：构建分布式、高并发、高性能、高可用、大规模并发、高可扩展性、高可维护性Java应用系统,书籍推荐或经验之谈?","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://wt-git-repository.github.io/tags/随笔/"}]},{"title":"Spring相关知识","slug":"Spring相关知识","date":"2019-06-12T08:26:14.000Z","updated":"2019-07-08T01:15:52.695Z","comments":true,"path":"2019/06/12/Spring相关知识/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/12/Spring相关知识/","excerpt":"Spring 模块（4.x）最新的5.x版本中Web模块的Portlet已经废弃， 同时增加了异步响应式处理的WebFlux组件。 Spring Core： 基础模块， Spring其它的功能都依赖于这个类库， 主要提供IOC依赖注入功能 Spring Aspects： 该模块与AspectJ的集成提供支持 Spring AOP： 提供了面向切面的编程实现 Spring JDBC： JAVA数据库连接 Spring JMS： JAVA消息服务 Spring ORM： 用于支持Hibernate等ORM工具 Spring Web：为了创建Web应用程序提供支持 Spring Test：提供了对Junit和TestNG测试支持","text":"Spring 模块（4.x）最新的5.x版本中Web模块的Portlet已经废弃， 同时增加了异步响应式处理的WebFlux组件。 Spring Core： 基础模块， Spring其它的功能都依赖于这个类库， 主要提供IOC依赖注入功能 Spring Aspects： 该模块与AspectJ的集成提供支持 Spring AOP： 提供了面向切面的编程实现 Spring JDBC： JAVA数据库连接 Spring JMS： JAVA消息服务 Spring ORM： 用于支持Hibernate等ORM工具 Spring Web：为了创建Web应用程序提供支持 Spring Test：提供了对Junit和TestNG测试支持 Spring IoC（工厂模式）IoC 是一种设计思想， 将原本在程序中手动创建对象的控制权， 交由Spring框架来管理。 IoC容器是Spring用来实现IoC的载体， IoC容器实际上就是个Map(K, V)， 存放各种对象。 IoC容器就像是一个工厂一样，当我们需要创建一个对象的时候， 只需要配置好配置文件或注解即可， 完全不用考虑对象是怎么被创建出来的。 为了更好地去了解IoC， 此处我需要补充几个知识点 依赖倒置原则： 把原本的高层建筑依赖底层建筑倒置过来， 变成底层建筑依赖高层建筑， 高层建筑需要什么， 底层建筑便去实现这样的需求， 高层并不需要管底层是怎么实现的， 这样就不会出现牵一发而动全身的情况。 DI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去 控制反转就是依赖倒置原则的一种代码设计思路， 具体方法是依赖注入。 Spring IoC有什么好处 AOPAOP（面向切面编程）能够将那些与业务代码无关， 却为业务模块所共同调用的逻辑或责任（如事务处理、日志处理、权限控制等）封装起来， 以便于减少系统的重复代码， 降低模块间的耦合度， 并有利于未来的可拓展性和可维护性。 Spring AOP基于动态代理， 如果要代理的对象， 实现了某个接口， 那么Spring AOP就会使用JDK Proxy， 去创建代理对象， 没有接口的话，就无法使用JDK Proxy去代理， 此时便要使用Cglib Spring AOP 与 AspectJ AOP前者集成了后者， 前者比后者简单， 后者比前者强大 Spring AOP是运行时增强， AspectJ AOP是编译时增强 Spring 中的事务管理 编程式事务：在代码中硬编码（不推荐使用） 声明式事务：在配置文件中配置（推荐使用） 声明式事务又分为两种 基于XML的声明式事务 基于注解的声明式事务 Spring 事务中的隔离级别 TransactionDefinition.ISOLATION_DEFAULT: 使用后端数据库默认的隔离级别，Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE: 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 Spring 设计模式 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 面试官:“谈谈Spring中都用到了那些设计模式?”。 参考JavaGuide","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://wt-git-repository.github.io/tags/Spring/"}]},{"title":"设计模式之建造者模式","slug":"设计模式之建造者模式","date":"2019-06-12T07:46:35.000Z","updated":"2019-10-08T10:53:19.089Z","comments":true,"path":"2019/06/12/设计模式之建造者模式/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/12/设计模式之建造者模式/","excerpt":"","text":"定义建造者模式：是一种对象构建模式，它可以将复杂对象的建造过程抽象出来，一步一步组装各种零件进而创建一个复杂的对象。 建造者模式则是要求按照指定的蓝图建造产品，它的主要目的是通过组装零配件而产生一个新产品。 优点 客户端不必知道产品内部组成的细节，将产品本身与产品的创建过程解耦，使得相同的创建过程可以创建不同的产品对象。 用户使用不同的具体建造者可以得到不同的产品对象 可以更加精细地控制产品的创建过程 增加新的具体建造者无需修改原有类库的代码 与抽象工厂的差异抽象工厂模式实现对产品家族的创建，一个产品家族是由一系列产品组成的 建造者模式是按照指定的蓝图，通过组装零配件而产生的一个新的产品 建造者模式主要包含四个角色 Product（产品角色）： 一个具体的产品对象 Builder（抽象建造者）： 创建一个Product 对象的各个部件指定的抽象接口 ConcreateBuilder（具体建造者）：实现抽象接口，构建和装配各个部件 Director（指挥者）：Builder 接口的实现类，它主要用于创建一个复杂的对象，作用：负责控制产品对象的生产过程、隔离用户与对象的生产过程 参考深入理解建造者模式 ——组装复杂的实例图说设计模式","categories":[],"tags":[{"name":"设计模式， JAVA","slug":"设计模式，-JAVA","permalink":"https://wt-git-repository.github.io/tags/设计模式，-JAVA/"}]},{"title":"Java基础知识","slug":"Java基础知识","date":"2019-06-12T01:01:34.000Z","updated":"2019-07-08T01:15:52.669Z","comments":true,"path":"2019/06/12/Java基础知识/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/12/Java基础知识/","excerpt":"","text":"牛客网上，错题相关的知识点 子类A继承父类B， A a = new A()， 问代码块的执行顺序 父类B静态代码块 -&gt; 子类A的静态代码块 -&gt; 父类B非静态代码块 -&gt; 父类B的构造函数 -&gt; 子类A非静态代码块 -&gt; 子类A的构造函数 “&gt;&gt;&gt;”表示无符号右移， 高位用0填充， 如 1&gt;&gt;&gt;2 为0 sleep是线程类（Thread）的方法， wait是Object的方法， 前者不释放对象锁， 后者释放对象锁。 Float f = 0.1f, (后面必须要加f, 负责会被识别成double) 接口中的方法默认是public abstract的，且实现接口的类中对应的方法的可见性不能小于接口方法的可见性 接口中的变量默认是public static final 接口中不能定义私有方法 重载是在同一个类中，有多个方法名相同，参数列表不同， 与方法的返回值无关， 与权限修饰符无关。 Java程序的种类有：内嵌于Web文件夹中，由浏览器来观看的_Applet、可独立运行的Application和服务器端的servlets.","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"设计模式之工厂模式","slug":"设计模式之工厂模式","date":"2019-06-11T13:13:55.000Z","updated":"2019-07-08T01:15:52.735Z","comments":true,"path":"2019/06/11/设计模式之工厂模式/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/11/设计模式之工厂模式/","excerpt":"工厂模式介绍定义在基类中定义创建对象的一个接口，让子类决定实例化哪个类。工厂方法让一个类的实例化延迟到子类中进行。 工厂模式的分类 简单工厂， 又称为静态工厂方法模式 工厂方法， 又称为多态性工厂模式 抽象工厂， 又称为工具箱模式","text":"工厂模式介绍定义在基类中定义创建对象的一个接口，让子类决定实例化哪个类。工厂方法让一个类的实例化延迟到子类中进行。 工厂模式的分类 简单工厂， 又称为静态工厂方法模式 工厂方法， 又称为多态性工厂模式 抽象工厂， 又称为工具箱模式 使用工厂模式的好处 解耦：把对象的创建和使用的过程分开 降低代码重复：如果创建某个对象的过程十分复杂，需要一定的代码量，如果很多地方都要用到，那么就会有很多重复的代码。 降低维护成本：由于创建过程都有工厂统一管理，所以发生业务逻辑变化，不需要找到所以创建对象的方法逐个去修正，只需要在工厂里修改即可，降低维护成本。 简单工厂（此处使用反射机制实现）适用场景 需要创建的对象较少 客户端不关心对象的创建过程 简单工厂模式角色分配 抽象产品角色：简单工厂模式所创建的所有对象的父类，它负责描述所有公共接口 123public interface Animal &#123; void sing();&#125; 具体产品角色：简单工厂模式的创建目标 12345678910111213public class Dog implements Animal &#123; @Override public void sing() &#123; System.out.println(\"Wang Wang ~~\"); &#125;&#125;public class Cat implements Animal &#123; @Override public void sing() &#123; System.out.println(\"Miao miao ~~\"); &#125;&#125; 抽象产品角色：简单工厂模式的核心，它负责实现创建所有实例的内部逻辑。 12345678910111213public class AnimalFactory &#123; public static Object getClass(Class&lt;? extends Animal&gt; clazz) &#123; Object obj = null; try &#123; obj = Class.forName(clazz.getName()).newInstance(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return obj; &#125;&#125; 工厂方法模式（使用最多）工厂方法是简单工厂的进一步深化，在工厂方法模式中， 我们不再提供一个统一的工厂类来创建所有的对象， 而是针对不同的对象去提供不同的工厂， 也就是说 每个对象都有一个与之对应的工厂 工厂方法模式的角色分配 抽象产品角色： 工厂方法模式锁创建的对象的父类 123public interface Animal &#123; void sing();&#125; 具体产品角色：这个角色实现了抽象产品角色所定义的接口 12345678910111213public class Dog implements Animal &#123; @Override public void sing() &#123; System.out.println(\"Wang Wang ~~\"); &#125;&#125;public class Cat implements Animal &#123; @Override public void sing() &#123; System.out.println(\"Miao miao ~~\"); &#125;&#125; 抽象工厂角色：工厂方法模式的核心， 是所有工程类的接口类 123public interface Factory &#123; Animal getAnimal();&#125; 具体工厂角色：实现抽象工厂接口的具体工厂类 123456public class DogFactory implements Factory &#123; @Override public Animal getAnimal() &#123; return new Dog(); &#125;&#125; 抽象工厂抽象工厂是生产出一套产品的（至少两个），这些产品是相互有关系或者有关联的， 而工厂方法是生产单一产品的工厂。 参考深入理解工厂模式——由对象工厂生成对象","categories":[],"tags":[{"name":"设计模式， JAVA","slug":"设计模式，-JAVA","permalink":"https://wt-git-repository.github.io/tags/设计模式，-JAVA/"}]},{"title":"设计模式之单例模式","slug":"设计模式之单例模式","date":"2019-06-11T12:45:49.000Z","updated":"2019-07-08T01:15:52.734Z","comments":true,"path":"2019/06/11/设计模式之单例模式/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/11/设计模式之单例模式/","excerpt":"单例模式定义保证一个类只有一个实例， 并且提供一个访问它的全局访问点。 好处 对于频繁使用而且不需要记录某些状态的对象，可以省略创建对象所花费的时间，这对于这些重量级对象而言，这是非常客观的一笔系统开销。 由于new的次数减少，因为对系统内存的使用频率也会降低，这将减轻GC压力，缩短GC的停顿时间。 为什么不设置为静态变量因为我们要保证资源的可用性，静态变量在程序加载了类的字节码之后，不需要创建任何实例对象就会被分配相应的空间，静态变量就可以被使用了。 如果对象一直没有被使用，这么对资源也会是一种消耗，此时我们就需要在使用时才创建对象，避免不必要的资源浪费。","text":"单例模式定义保证一个类只有一个实例， 并且提供一个访问它的全局访问点。 好处 对于频繁使用而且不需要记录某些状态的对象，可以省略创建对象所花费的时间，这对于这些重量级对象而言，这是非常客观的一笔系统开销。 由于new的次数减少，因为对系统内存的使用频率也会降低，这将减轻GC压力，缩短GC的停顿时间。 为什么不设置为静态变量因为我们要保证资源的可用性，静态变量在程序加载了类的字节码之后，不需要创建任何实例对象就会被分配相应的空间，静态变量就可以被使用了。 如果对象一直没有被使用，这么对资源也会是一种消耗，此时我们就需要在使用时才创建对象，避免不必要的资源浪费。 单例模式的实现两种构建方式： 饿汉模式：指全局的单例实例在类装载时构建。 懒汉模式：指全局的单例实例在第一次被使用时构建。 共同点： 统一的private级别的构造函数 instance 成员变量和 uniqueInstance 方法必须是 static 的 饿汉模式（线程安全）12345678910public class Singleton &#123; //在静态初始化器中创建单例实例，这段代码保证了线程安全 //volatile保证，当uniqueInstance变量被初始化成Singleton实例时，多个线程可以正确处理uniqueInstance变量 private volatile static Singleton uniqueInstance = new Singleton(); //Singleton类只有一个构造方法并且是被private修饰的，所以用户无法通过new方法创建该对象实例 private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return uniqueInstance; &#125;&#125; 懒汉模式（双重检查加锁版本）1234567891011121314151617181920public class Singleton &#123; //volatile保证，当uniqueInstance变量被初始化成Singleton实例时，多个线程可以正确处理uniqueInstance变量 private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; //检查实例，如果不存在，就进入同步代码块 if (uniqueInstance == null) &#123; //只有第一次才彻底执行这里的代码 synchronized(Singleton.class) &#123; //进入同步代码块后，再检查一次，如果仍是null，才创建实例 if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 参考深入理解单例模式——只有一个实例","categories":[],"tags":[{"name":"设计模式， JAVA","slug":"设计模式，-JAVA","permalink":"https://wt-git-repository.github.io/tags/设计模式，-JAVA/"}]},{"title":"centos7常用命令(IP与防火墙相关)","slug":"Linux常用命令","date":"2019-06-11T01:35:49.000Z","updated":"2019-07-08T01:15:52.682Z","comments":true,"path":"2019/06/11/Linux常用命令/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/11/Linux常用命令/","excerpt":"查看自己所在的公网ip1curl members.3322.org/dyndns/getip 查看端口是否开放1telnet ip port","text":"查看自己所在的公网ip1curl members.3322.org/dyndns/getip 查看端口是否开放1telnet ip port 开放端口1234567891011121314151617181920- 查看已打开的端口 # netstat -anp- 查看想开的端口是否已开 # firewall-cmd --query-port=666/tcp 若此提示 FirewallD is not running 表示为不可知的防火墙 需要查看状态并开启防火墙- 查看防火墙状态 # systemctl status firewalldrunning 状态即防火墙已经开启dead 状态即防火墙未开启- 开启防火墙，# systemctl start firewalld 没有任何提示即开启成功- 开启防火墙 # service firewalld start 关闭防火墙 # systemctl stop firewalld centos7.3 上述方式可能无法开启，可以先#systemctl unmask firewalld.service 然后 # systemctl start firewalld.service- 查看想开的端口是否已开 # firewall-cmd --query-port=666/tcp 提示no表示未开- 开永久端口号 firewall-cmd --add-port=666/tcp --permanent 提示 success 表示成功- 重新载入配置 # firewall-cmd --reload 比如添加规则之后，需要执行此命令- 再次查看想开的端口是否已开 # firewall-cmd --query-port=666/tcp 提示yes表示成功- 若移除端口 # firewall-cmd --permanent --remove-port=666/tcp- 修改iptables 有些版本需要安装iptables-services # yum install iptables-services 然后修改进目录 /etc/sysconfig/iptables 修改内容 docker与centos7(firewall)123firewall-cmd --permanent --zone=trusted --change-interface=docker0 --permanentfirewall-cmd --reloadsystemctl restart docker 参考Centos7 使用firewalld代替了原来的iptables","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://wt-git-repository.github.io/tags/操作系统/"}]},{"title":"软件工程管理","slug":"软件工程管理","date":"2019-06-10T08:14:53.000Z","updated":"2019-07-08T01:15:52.737Z","comments":true,"path":"2019/06/10/软件工程管理/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/10/软件工程管理/","excerpt":"软件工程规范软件过程的分类 软件主要过程： 软件获取、供应、开发运行和维护的过程。 软件支持过程： 对软件的主要过程提供支持的过程。 软件组织过程：对软件主要过程和支持过程过程的提供组织保证的过程。 软件过程的组成管理工程、工程过程和支持过程。","text":"软件工程规范软件过程的分类 软件主要过程： 软件获取、供应、开发运行和维护的过程。 软件支持过程： 对软件的主要过程提供支持的过程。 软件组织过程：对软件主要过程和支持过程过程的提供组织保证的过程。 软件过程的组成管理工程、工程过程和支持过程。 过程规范过程规范指的是对输入、输出和活动所构成的过程进行明文规定或约定俗成的标准。 作用： 帮助团队实现共同的目标， 一个规范的软件过程必将能带来稳定、高水平的过程质量， 过程规范使软件组织的生产效率更高。 软件过程规范软件过程规范是软件开发组织行动的准则与指南， 可以依据上垒各类过程的特点而建立相应的规范。 软件过程规范的建立 软件能力成熟度模型（CMM/CMMI） 个体软件过程（PSP） 团体软件过程（TSP） IBM-Raional 统一过程（RUP） 极限编程 （eXtreme Programming，XP） 微软软件框架（MSF） 软件生命周期的过程需求 软件工程过程 软件支持过程 软件管理过程 软件组织过程 软件客户-供应商的过程 软件过程模型 瀑布模型 螺旋模型 增量模型 软件过程成熟度CMMCMM是软件过程能力成熟度模型， 描述一条从无序混乱到成熟有纪律的过程的改进途径， 描绘出软件组织如何增加对软件开发和维护的过程控制等方面的指导。 CMMICMMI是能力成熟度模型集成， 前身是CMM CMM/CMMI五个等级 初始级: 具有明显不成熟过程的特点 可重复级： 建立了管理软件项目的方针和实施这些方针的规程， 使软件项目的有效管理制度化 已定义级：包含一组协调的、集成的、适度定义的软件工程过程和管理过程，具有良好的文档化、标准化，使软件过程具有可视性、一致性、稳定性和可重复性，软件过程被集成为一个有机的整体 定量管理级： 在上述已定义级的基础上，可以建立有关软件过程和产品质量的、一致的度量体系，采集详细的数据进行分析，从而对软件产品和过程进行有效的定量控制和管理。 优化级： 断改善组织的软件过程能力和项目的过程性能，利用来自过程和来自新思想、新技术的先导性试验的定量反馈信息，使持续过程改进成为可能。为了预防缺陷出现，组织有办法识别出弱点并预先针对性地加强过程 CMMI过程域 工程管理 支持管理 项目管理 过程管理 PSP/TSP和CMM组成的软件过程框架软件过程的组织管理组织过程定义-过程裁剪 剪裁指南和准则的主要作用： 选择一个适合项目的生命周期模型。 剪裁和细化组织标准软件过程和所选择的软件生命周期，使之适合项目的具体特征。 组织过程焦点-执行约定 组织应该遵循一个文档化的关于协调软件流程的指定和改进活动的组织方针 高级管理人员发起对软件过程制定和改进的组织活动 高级管理人员进度软件过程的指定和改进的组织活动 组织过程焦点-执行能力 建立一个负责整个组织的软件过程活动的工作组 为软件过程活动提供足够的资源和资金 组织软件过程活动的组员进行培训 软件过程组和其它工程组的组员接受软件过程活动的相关培训 组织过程焦点-执行活动 定期评估软件过程并根据评估结果制订相应的更改计划 组织制定和维护有关软件过程和改进活动的计划 协调组织的标准软件过程和项目自定义的软件过程的制定和改进工作 协调组织的软件过程数据库的使用 新过程、新方法、新工具的评价、监控和推广 对有关组织和项目的软件过程培训进行统一管理 及时将有关软件过程制定和改进的活动通知与实施软件过程相关的组和人员 组织过程焦点-评估 度量和分析 实施验证 PSP过程框架和成熟度模型 PSP过程框架：PSP过程由一系列方法、表格、脚本等组成，用以指导软件开发人员计划、度量和管理他们的工作。 PSP成熟度模型：PSP是一个具有4个等级的成熟度框架， 其中四个等级分别为个体度量过程、个体计划过程、个体质量管理过程和个体循环过程。 TSP软件需求管理业务需求、用户需求、功能需求 需求开发需求开发的目的是通过调查与分析，获取用户需求并定义产品需求。 需求获取概述需求回去是通过各种途径获取用户的需求信息 需求获取的方法 需求研讨会 头脑风暴 用例模型 访谈 角色扮演 原型法 需求跟踪 需求的标识 需求的属性 需求的状态： 已建议、已批准、已实现和已删除 正向跟踪：以用户需求为切入点，检查《用户需求说明书》或《需求规格说明书》中的每个需求是否都能在后继工作产品中找到对应点。 逆向跟踪：检查设计文档、代码、测试用例等工作产品是否都能在《需求规格说明书》中找到出处。 正向跟踪和逆向跟踪合称为“双向跟踪”。 需求变更软件过程的技术管理技术路线 软件项目过程的技术解决流程 技术解决计划的建立和实施 开发设计 编程和单元测试 验证、确认与测试 软件过程的项目管理PMI项目管理知识域 整合管理 范围管理 时间管理 成本管理 质量管理 人力资源管理 沟通管理 风险管理 采购管理 干系人管理 PMI将47个管理过程归纳为5大类 启动过程组：获得授权，定义一个新项目或现有项目的一个新阶段，正式开始该项目或阶段的一组过程。 规划过程组：明确项目范围，优化目标，为实现目标而制定行动方案的一组过程。 执行过程组：完成项目管理计划中确定的工作以实现项目目标的一组过程。 监控过程组：跟踪、审查和调整项目进展与绩效，识别必要的计划变更并启动相应变更的一组过程。 收尾过程组：为完结所有过程组的所有活动以正式结束项目或阶段而实施的一组过程。 估算活动持续时间的方法三点估算 最乐观时间（tO）。基于活动的最好情况，所估算的活动持续时间 最悲观时间（tP）。基于活动的最差情况，所估算的活动持续时间 最可能时间（tM）。基于最可能获得的资源、最可能取得的资源生产率、对资源可用时间的现实预计、资源对其他参与者的可能依赖及可能发生的各种干扰等，所估算的活动持续时间。 基于持续时间在三种估算值区间内的假定分布情况（β分布），使用公式来计算期望持续时间tE tE = (t0 + 4 * tM + tP) / 6 正态曲线下，横轴区间（μ-σ,μ+σ）内的面积为68.268949%。P{|X-μ|&lt;σ}=2Φ（1）-1=0.6826 横轴区间（μ-1.96σ,μ+1.96σ）内的面积为95.449974%。P{|X-μ|&lt;2σ}=2Φ（2）-1=0.9544 横轴区间（μ-2.58σ,μ+2.58σ）内的面积为99.730020%。P{|X-μ|&lt;3σ}=2Φ（3）-1=0.9974 其余公式 PV[Planned Value]计划值：应该完成多少工作？[96版的BCWS] EV[Earned Value]挣值：完成了多少预算工作？[96版的BCWP] AC[Actual Cost]实际成本：完成工作的实际成本是多少？[96版的ACWP] BAC[Budget cost at completion]完工预算：全部工作的预算是多少？不改变成本基准，BAC就不会发生变化 EAC[Estimate at completion]完成预估：全部工作的成本是多少？是根据项目的绩效和风险量化对项目最可能的总成本所做的一种预测。 ETC完工尚需估算：剩余工作在当前的估算是多少？ CPI成本绩效指数：CPI=EV/AC CPI&gt;1代表工作价值高，好 SPI进度绩效指数：SPI=EV/PV SPI&gt;1代表实际进度快，好 PC任务完成指数：PC=EV/BA CV成本差异：CV=EV-AC CV&gt;0代表成本节约，好 成本差异比例%=CV/EV=(EV-AC)/EV=1-1/CPI SV进度差异：SV=EV-PV SV&gt;0代表进度提前，好 成本差异比例%=SV/PV=(EV-PV)/PV=SPI-1 EAC=BAC+AC-EV=BAC-CV EAC=BAC/CPI EAC=ETC+AC","categories":[],"tags":[{"name":"软件过程管理","slug":"软件过程管理","permalink":"https://wt-git-repository.github.io/tags/软件过程管理/"}]},{"title":"操作系统之进程与线程","slug":"操作系统之进程与线程","date":"2019-06-10T06:09:11.000Z","updated":"2019-07-08T01:15:52.732Z","comments":true,"path":"2019/06/10/操作系统之进程与线程/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/10/操作系统之进程与线程/","excerpt":"进程与线程进程进程是资源分配的基本单位. 进程控制块PCB: 描述了进程的基本信息和运行状态, 所谓的创建进程和撤销进程都是指对PCB的操作 线程线程是独立调度的基本单位. 一个进程中可以有多个线程, 它们共享进程资源.","text":"进程与线程进程进程是资源分配的基本单位. 进程控制块PCB: 描述了进程的基本信息和运行状态, 所谓的创建进程和撤销进程都是指对PCB的操作 线程线程是独立调度的基本单位. 一个进程中可以有多个线程, 它们共享进程资源. 线程与进程的区别 一个进程至少有一个线程 在同一个进程中,线程的切换不会引起进程的切换,从一个进程中的线程切换到另一个进程中的线程,会引起进程切换. 创建和销毁进程的开销远大于创建或销毁线程时的开销. 线程间的通信可以通过直接读写统一进程中的数据进行通信, 但是进程间的通信需要借助IPC. 进程状态的转换 就绪 运行 阻塞 进程调度算法批处理系统批处理系统没有太多的用户操作, 在该系统中, 调度算法的目标是保证吞吐量和周转时间. 先来先服务(FCFS) 短作业优先(SJF) 最短剩余时间优先(SRTN) 交互式系统交互式系统有大量的用户交互操作, 在该系统中调度算法的目的是快速地响应 时间片轮转 优先级调度 多级反馈队列 实时系统实时系统要求一个请求在一个确定的时间内得到响应 硬实时: 满足绝对的截止时间 软实时: 可以容忍一定的超时 进程同步临界区对临界资源访问的那段代码成为临界区 同步和互斥 同步: 多个进程按照一定的顺序执行 互斥: 多个进程在统一时刻只有一个进程能够进入临界区 信号量信号量(Semaphore)是一个整形变量, 是常见的PV操作 进程通信管道管道是通过调用pipe函数创建的, fd[0]用于读, fd[1]用于写. 支持半双工, 只能在父子进程中使用 FIFO命名管道常用于客户-服务器应用程序, 在二者的进程中传递数据. 消息队列相比于FIFO, 有一下优点: 消息队列可以独立与读写进程存在, 从而避免了FIFO中同步管道的打开和关闭时可能产生的困难. 避免了FIFO同步阻塞问题, 不需要进程自己提供同步方法 读进程可以根据消息类型有选择接受, 而不像FIFO那样只能默认接收 信号量计数器, 用于为多个进程提供对共享对象的访问服务. 共享存储云溪多个进程共享一个给定的存储区 需要使用信号量来同步对共享存储的访问 套接字用于不同机器间的进程通信","categories":[],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://wt-git-repository.github.io/tags/操作系统/"}]},{"title":"Redis","slug":"Redis","date":"2019-06-09T01:35:35.000Z","updated":"2019-07-18T06:49:51.956Z","comments":true,"path":"2019/06/09/Redis/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/09/Redis/","excerpt":"简介Redis使用C语言开发的一个开源的高性能键值对(key-value)数据库,它通过提供多种键值数据类型来适应不同场景下的存储需求,目前Redis支持的键值数据类型如下: 字符串类型 散列类型 列表类型 集合类型 有序集合类型","text":"简介Redis使用C语言开发的一个开源的高性能键值对(key-value)数据库,它通过提供多种键值数据类型来适应不同场景下的存储需求,目前Redis支持的键值数据类型如下: 字符串类型 散列类型 列表类型 集合类型 有序集合类型 应用场景 缓存(主要用途) 分布式集群架构中的session分离 聊天室的在线好友列表 任务队列 应用排行榜 网络访问统计 数据过期处理 数字的自增(高并发下,订单号为yyyymmddHHmmsss001,使用Redis的自增功能,避免重复) Redis HA方案HA(High Available, 高可用性集群)集群系统的简称, 是 保证业务连续性的有效解决方案, 一般有两个或两个以上的结点,分为活动结点以及备用结点, 若活动结点出现了问题,导致正在运行的业务不能正常运行,备用结点此时就能侦测得到,并立即代替活动结点来执行业务,从而实现业务的不中断或短暂中断. Redis一般以主/从方式部署,官方推荐使用sentinel(哨兵)来实现高可用. sentinel是解决HA问题,cluster是解决主从复制问题. Redis集群可以在一组Redis节点之间实现高可用性,在集群中有一个master和多个slave节点,当master节点失效时,应选举出一个slave节点作为新的master, 然后Redis本身并没有自动发现故障并且进行主从切换的能力, 需要外部的监控方案来实现自动故障恢复. Redis Sentinel是官方推荐的高可用性解决方案,它是Redis集群的监控管理工具,可以提供节点监控.通知,自动故障恢复和客户端配置发现服务. 搭建Redis集群12345678910111213141516171819version: '3.1'services: master: image: redis container_name: redis-master ports: - 6379:6379 slave1: image: redis container_name: redis-slave-1 ports: - 6380:6379 command: redis-server --slaveof redis-master 6379 slave2: image: redis container_name: redis-slave-2 ports: - 6381:6379 command: redis-server --slaveof redis-master 6379 搭建Redis Sentinel集群12345678910111213141516171819202122232425262728version: '3.1'services: sentinel1: image: redis container_name: redis-sentinel-1 ports: - 26379:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel1.conf:/usr/local/etc/redis/sentinel.conf sentinel2: image: redis container_name: redis-sentinel-2 ports: - 26380:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel2.conf:/usr/local/etc/redis/sentinel.conf sentinel3: image: redis container_name: redis-sentinel-3 ports: - 26381:26379 command: redis-sentinel /usr/local/etc/redis/sentinel.conf volumes: - ./sentinel3.conf:/usr/local/etc/redis/sentinel.conf redis sentinel.conf12345678910port 26379dir /tmp# 自定义集群名,其中127.0.0.1为Redis-master的IP,6379为redis-master的端口,2为最小投票数# 如果使用docker部署, 切记别使用127.0.0.1, 要具体的IP地址sentinel monitor mymaster 127.0.0.1 6379 2sentinel down-after-milliseconds mymaster 30000sentinel parallel-syncs mymaster 1sentinel failover-timeout mymaster 180000sentinel deny-scripts-reconfig yes 相关命令123redis-cli -p 26379 sentinel master mymastersentinel slaves mymaster lettuce依赖12345678&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; application.yml1234567891011spring: redis: lettuce: pool: max-active: 8 max-idle: 8 max-wait: -1ms min-idle: 0 sentinel: master: mymaster nodes: 192.168.75.140:26379, 192.168.75.140:26380, 192.168.75.140:26381 接口1234public interface RedisService &#123; public void set(String key, Object value, long seconds); public Object get(String key);&#125; 参考李卫民的教学视频","categories":[],"tags":[{"name":"Nosql","slug":"Nosql","permalink":"https://wt-git-repository.github.io/tags/Nosql/"}]},{"title":"SpringMVC","slug":"SpringMVC","date":"2019-06-08T07:53:17.000Z","updated":"2019-07-08T01:15:52.695Z","comments":true,"path":"2019/06/08/SpringMVC/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/08/SpringMVC/","excerpt":"MVC设计模式","text":"MVC设计模式 Spring MVC工作原理（适配器模式）1、前端控制器DispatcherServlet（不需要工程师开发）,由框架提供（重要）作用：Spring MVC 的入口函数。接收请求，响应结果，相当于转发器，中央处理器。有了 DispatcherServlet 减少了其它组件之间的耦合度。用户请求到达前端控制器，它就相当于mvc模式中的c，DispatcherServlet是整个流程控制的中心，由它调用其它组件处理用户的请求，DispatcherServlet的存在降低了组件之间的耦合性。 2、处理器映射器HandlerMapping(不需要工程师开发),由框架提供作用：根据请求的url查找Handler。HandlerMapping负责根据用户请求找到Handler即处理器（Controller），SpringMVC提供了不同的映射器实现不同的映射方式，例如：配置文件方式，实现接口方式，注解方式等。 3、处理器适配器HandlerAdapter作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler 通过HandlerAdapter对处理器进行执行，这是适配器模式的应用，通过扩展适配器可以对更多类型的处理器进行执行。 4、处理器Handler(需要工程师开发)注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler Handler 是继DispatcherServlet前端控制器的后端控制器，在DispatcherServlet的控制下Handler对具体的用户请求进行处理。 由于Handler涉及到具体的用户业务请求，所以一般情况需要工程师根据业务需求开发Handler。 5、视图解析器View resolver(不需要工程师开发),由框架提供作用：进行视图解析，根据逻辑视图名解析成真正的视图（view） View Resolver负责将处理结果生成View视图，View Resolver首先根据逻辑视图名解析成物理视图名即具体的页面地址，再生成View视图对象，最后对View进行渲染将处理结果通过页面展示给用户。 springmvc框架提供了很多的View视图类型，包括：jstlView、freemarkerView、pdfView等。 一般情况下需要通过页面标签或页面模版技术将模型数据通过页面展示给用户，需要由工程师根据业务需求开发具体的页面。 6、视图View(需要工程师开发)View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf…） 注意：处理器Handler（也就是我们平常说的Controller控制器）以及视图层view都是需要我们自己手动开发的。其他的一些组件比如：前端控制器DispatcherServlet、处理器映射器HandlerMapping、处理器适配器HandlerAdapter等等都是框架提供给我们的，不需要自己手动开发。 参考JavaGuide","categories":[],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"https://wt-git-repository.github.io/tags/SpringMVC/"}]},{"title":"SpringBean","slug":"SpringBean","date":"2019-06-08T07:06:40.000Z","updated":"2019-07-08T01:15:52.692Z","comments":true,"path":"2019/06/08/SpringBean/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/08/SpringBean/","excerpt":"前言在Spring中，那些组成应用程序的主体以及那些由Spring IoC 容器锁管理的对象，被称之为bean。 简单来讲，bean就是由IoC容器初始化、装配及管理的对象。 Spring中的bean默认是单例的，Spring的单例基于JVM，每个JVM内只有一个实例。 在大多数情况下，单例子bean都是很理想的方案，除了使用一些需要保持一些状态的bean.","text":"前言在Spring中，那些组成应用程序的主体以及那些由Spring IoC 容器锁管理的对象，被称之为bean。 简单来讲，bean就是由IoC容器初始化、装配及管理的对象。 Spring中的bean默认是单例的，Spring的单例基于JVM，每个JVM内只有一个实例。 在大多数情况下，单例子bean都是很理想的方案，除了使用一些需要保持一些状态的bean. bean的作用域 配置和注解1&lt;bean id=\"ServiceImpl\" class=\"cn.csdn.service.ServiceImpl\" scope=\"singleton\"&gt; 1234@Service@Scope(\"singleton\")public class ServiceImpl&#123;&#125; bean的生命周期initialization and destroySpring 框架提供了很多方法让我们在Spring Bean生命周期中执行initialization和pre-destroy方法. 使用@PostConstruct和@PreDestroy注解 12345678910public class GiraffeService &#123; @PostConstruct public void initPostConstruct()&#123; System.out.println(\"执行PostConstruct注解标注的方法\"); &#125; @PreDestroy public void preDestroy()&#123; System.out.println(\"执行preDestroy注解标注的方法\"); &#125;&#125; 通过bean的配置文件中指定init-method和destroy-method方法 123&lt;bean name=\"giraffeService\" class=\"com.giraffe.spring.service.GiraffeService\" init-method=\"initMethod\" destroy-method=\"destroyMethod\"&gt;&lt;/bean&gt; 12345678910public class GiraffeService &#123; //通过&lt;bean&gt;的destroy-method属性指定的销毁方法 public void destroyMethod() throws Exception &#123; System.out.println(\"执行配置的destroy-method\"); &#125; //通过&lt;bean&gt;的init-method属性指定的初始化方法 public void initMethod() throws Exception &#123; System.out.println(\"执行配置的init-method\"); &#125;&#125; Aware接口 ApplicationContextAware: 获得ApplicationContext对象,可以用来获取所有Bean definition的名字。 BeanFactoryAware:获得BeanFactory对象，可以用来检测Bean的作用域。 BeanNameAware:获得Bean在配置文件中定义的名字。 ResourceLoaderAware:获得ResourceLoader对象，可以获得classpath中某个文件。 ServletContextAware:在一个MVC应用中可以获取ServletContext对象，可以读取context中的参数。 ServletConfigAware： 在一个MVC应用中可以获取ServletConfig对象，可以读取config中的参数。 总结 Bean容器找到配置文件中 Spring Bean 的定义。 Bean容器利用Java Reflection API创建一个Bean的实例。 如果涉及到一些属性值 利用set方法设置一些属性值。 如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字。 如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 如果Bean实现了BeanFactoryAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 与上面的类似，如果实现了其他Aware接口，就调用相应的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执-行postProcessBeforeInitialization()方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执 行postProcessAfterInitialization()方法 当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。 当要销毁Bean的时候，如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。 Spring IoC（工厂模式）IoC 是一种设计思想， 将原本在程序中手动创建对象的控制权， 交由Spring框架来管理。 IoC容器是Spring用来实现IoC的载体， IoC容器实际上就是个Map(K, V)， 存放各种对象。 IoC容器就像是一个工厂一样，当我们需要创建一个对象的时候， 只需要配置好配置文件或注解即可， 完全不用考虑对象是怎么被创建出来的。 为了更好地去了解IoC， 此处我需要补充几个知识点 依赖倒置原则： 把原本的高层建筑依赖底层建筑倒置过来， 变成底层建筑依赖高层建筑， 高层建筑需要什么， 底层建筑便去实现这样的需求， 高层并不需要管底层是怎么实现的， 这样就不会出现牵一发而动全身的情况。 DI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去 控制反转就是依赖倒置原则的一种代码设计思路， 具体方法是依赖注入。 Spring IoC有什么好处 工厂设计模式Spring 使用工厂模式可以通过BeanFactory 和 ApplicationContext创建bean对象 BeanFactory： 延迟注入， 需要使用到某个Bean时才会注入， 占用较少内存， 程序启动速度更快 ApplicationContext: 启动是一次性创建所有的Bean 对比： BeanFactory仅仅提供了最基本的依赖注入支持， ApplicationContext扩展了BeanFactory， 所以一般会使用ApplicationContext会更多一点。 ApplicationContext三个实现类： ClassPathXmlApplication: 从上下文中加载资源文件 FileSystemXmlApplication: 从文件系统中加载资源文件 XmlWebApplicationContext: 从Web系统中加载资源文件 参考JavaGuide-Spring Bean","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://wt-git-repository.github.io/tags/Spring/"}]},{"title":"MySQL三范式","slug":"MySQL三范式","date":"2019-06-08T02:13:16.000Z","updated":"2019-07-08T01:15:52.686Z","comments":true,"path":"2019/06/08/MySQL三范式/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/08/MySQL三范式/","excerpt":"","text":"第一范式（1st NF - 列都是不可再分的）要求：第一范式的目标是确保每一列的原子性，每一列都是不可再分的最小数据单元。 第二范式（2st NF - 每张表只描述一件事情）前提： 满足第一范式要求： 表中的非主键列不存在对主键的部分依赖。 第三范式（3st NF - 不存在对非主键列的传递依赖）前提：满足第二范式要求：表中的列不存在对非主键列的传递依赖。 参考数据库，部分函数依赖，传递函数依赖，完全函数依赖，三种范式的区别","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wt-git-repository.github.io/tags/MySQL/"}]},{"title":"Manjaro配置中国源","slug":"Manjaro配置中国源","date":"2019-06-06T03:40:34.000Z","updated":"2019-07-08T01:15:52.684Z","comments":true,"path":"2019/06/06/Manjaro配置中国源/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/06/Manjaro配置中国源/","excerpt":"","text":"配置镜像源123sudo pacman-mirrors -gb testing -c Chinasudo pacman-mirrors -g 系统更新1sudo pacman -Syyu","categories":[],"tags":[]},{"title":"Kubernetes","slug":"Kubernetes","date":"2019-06-06T02:28:19.000Z","updated":"2019-07-08T01:15:52.679Z","comments":true,"path":"2019/06/06/Kubernetes/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/06/Kubernetes/","excerpt":"","text":"简介（容器编排工具）Kubernetes 是Google创建管理的容器集群关系系统，开源，开源实现容器集群的自动化部署、自动扩缩容、维护等功能，其目标是促进完善组件和工具的生态系统，以减轻应用程序在公有云或私有云中运行的负担。 安装kubeadmkubeadm是kubernetes的集群安装工具，能够快速安装kubernetes集群。 待定","categories":[],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://wt-git-repository.github.io/tags/Docker/"}]},{"title":"数据结构-树相关","slug":"数据结构-树相关","date":"2019-06-05T07:53:43.000Z","updated":"2019-07-08T01:15:52.733Z","comments":true,"path":"2019/06/05/数据结构-树相关/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/05/数据结构-树相关/","excerpt":"","text":"堆排序堆分为最大堆和最小堆 最大堆定义：设数组a存放了n个数据元素，数组下标从0开始，如果当数组下标2i+1&lt;n时存在a[i].key&gt;a[2i+1].key&gt;=a[i].key，当数组下标2i+2&lt;n时，有a[i].key&gt;=a[2i+2].key，这样的数据结构称为最大堆。 最大堆的根节点是堆值中最大的数据元素。 对于最大堆，从根结点到每个叶结点的路径，数组元素组成的序列都是递减有序的 通常把堆的根节点称为堆顶元素。 最小堆（与最大堆类似，这里不再累赘） 创建堆在完全二叉树中，第一个非叶子结点a[i] (i = (n - 2) / 2) 创建思路：从第一个非叶子结点开始，找出a[2i+1]和a[2i+2]的最大值，并与a[i]进行比对，若比a[i]小，则说明以a[i]为更结点的堆已经是最大堆，否则，二者交换。以此类推，直到调整到根节点。 值得注意的是：若左右子节点并非叶子结点，与a[i]的调换可能会引起子节点的一连串调整，这也是值得我们注意的地方 1234567891011121314151617181920212223242526272829303132/** * @param data 待排序的数组 * @param n 元素的总个数 * @param h 当前的根节点 */void createHeap(int[] data, int n, int h) &#123; int i = h; int j = 2 * i + 1; int temp = data[i]; while (j &lt; n) &#123; // 寻找左右子节点的最大值 if (j + 1 &lt; n &amp;&amp; data[j] &lt; data[j + 1]) j++; // 对比根节点与左右子节点的最大值 if (temp &gt; data[j]) &#123; break; &#125; else &#123; data[i] = data[j]; i = j; j = 2 * i + 1; &#125; &#125; data[i] = temp;&#125;void initHeap(int[] data, int n) &#123; for (int i = (n - 2) / 2; i &gt; -1; i--) &#123; createHeap(data, n, i); &#125;&#125; 利用最大堆来进行排序12345678910void heapSort(int[] data, int n) &#123; initHeap(data, n); for (int i = n - 1;i &gt; -1;i--) &#123; int temp = data[i]; data[i] = data[0]; data[0] = temp; createHeap(data, i, 0); &#125;&#125;","categories":[],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://wt-git-repository.github.io/tags/数据结构/"}]},{"title":"Nginx","slug":"Nginx","date":"2019-06-05T06:20:12.000Z","updated":"2019-10-08T10:53:19.067Z","comments":true,"path":"2019/06/05/Nginx/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/05/Nginx/","excerpt":"","text":"Nginx简介Nginx是一款高性能Http服务器、反向代理服务器以及电子邮件（IMAP、POP3）代理服务器，能支撑5万并发，CPU、内存等资源消耗非常低，运行稳定。 Nginx应用场景 HTTP服务器：Nginx是一个HTTP服务，可以独立提供HTTP服务，可以做网页静态服务器。 虚拟主机：可以实在一台服务器虚拟出多个网站。 反向代理、负载均衡：可以使用Nginx反向代理到多个集群。 docker-compose.yml123456789101112131415161718192021222324version: '3.1'services: tomcat9090: image: tomcat container_name: tomcat9090 restart: always ports: - 9090:8080 tomcat9091: image: tomcat container_name: tomcat9091 restart: always ports: - 9091:8080 nginx: image: nginx container_name: nginx restart: always ports: - 80:80 - 9000:9000 volumes: - ./conf/nginx.conf:/etc/nginx/nginx.conf - ./web:/usr/share/nginx/wwwroot 虚拟主机虚拟主机是一种特殊的软硬件技术，它可以将网络上的每一台计算机分成多个虚拟主机，每个虚拟主机可以独立对外提供www服务，这样就实现一台主机对外提供web服务。 通过Nginx可以实现虚拟主机的配置，Nginx支持三种类型的虚拟主机的配置 基于IP的虚拟主机 基于域名的虚拟主机 基于端口的虚拟主机 配置文件123456789101112131415161718192021222324252627282930313233343536373839404142# CPU多少核就填多少核，充分利用CPU资源worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application:octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; # 基于 IP server_name 172.28.7.36; # 基于域名 # server_name endalin.com; location / &#123; root /usr/share/nginx/wwwroot/welcome; index index.html; &#125; &#125; server &#123; # 基于端口 # listen 81; listen 80; server_name 172.28.7.36; # 基于域名 # server_name oj.endalin.com; location / &#123; root /usr/share/nginx/wwwroot/welcome81; index index.html; &#125; &#125;&#125; 正向代理在客户端（浏览器）配置代理服务器，通过代理服务器访问指定网址 反向代理、负载均衡反向代理服务器架设在服务器端，通过缓存经常被请求的页面来缓解服务器的工作量，将客户机请求转发给内部网络上的目标服务器，并将从服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器和目标主机对外表现为一个服务器。 docker-compose.yml123456789101112131415161718192021222324version: '3.1'services: tomcat9090: image: tomcat container_name: tomcat9090 restart: always ports: - 9090:8080 tomcat9091: image: tomcat container_name: tomcat9091 restart: always ports: - 9091:8080 nginx: image: nginx container_name: nginx restart: always ports: - 80:80 - 9000:9000 volumes: - ./conf/nginx.conf:/etc/nginx/nginx.conf - ./web:/usr/share/nginx/wwwroot 配置文件12345678910111213141516171819202122232425262728worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application:octet-stream; sendfile on; keepalive_timeout 65; upstream tomcatServer&#123; server 10.42.29.120:9000 weight=10; server 10.42.29.120:9091 weight=10; &#125; server &#123; listen 80; server_name 10.42.29.120; location / &#123; proxy_pass http://tomcatServer; index index.html index.jsp; &#125; &#125;&#125; 动静分离通过location 指定不同的后缀名实现不同的请求转发，通过expires 参数设置，可以使浏览器缓存过期时间，减少与服务器之间的请求和流量。 具体Expires 定义：给一个资源设定一个过期时间，无需服务器去验证，直接通过浏览器自身确定是否过期，不会产生额外的流量。这种方式适合不经常变动的资源。假如我设置了3d,即三天，表示在三天之内访问这个URL，发送一个请求给服务器，对比服务器该文件最后的更新时间，如果更新时间没有发生变化，则不会从服务器中抓取，返回状态码304，如果有修改，则直接从服务器中直接下载，返回状态200 1234567891011121314151617181920212223242526272829303132333435worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application:octet-stream; sendfile on; keepalive_timeout 65; upstream tomcatServer&#123; server 10.60.2.128:9000 weight=10; server 10.60.2.128:9091 weight=10; &#125; server &#123; listen 80; server_name 10.60.2.128; location /welcome81 &#123; root /usr/share/nginx/wwwroot/; index index.html; &#125; location /image &#123; root /usr/share/nginx/wwwroot/; # 列出文件目录 autoindex on; &#125; &#125;&#125;","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"https://wt-git-repository.github.io/tags/运维/"}]},{"title":"RabbitMQ","slug":"RabbitMQ","date":"2019-06-05T01:27:13.000Z","updated":"2019-07-08T01:15:52.690Z","comments":true,"path":"2019/06/05/RabbitMQ/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/05/RabbitMQ/","excerpt":"","text":"RabbitMQ相关概念生产者和消费者 producer：消息的生产者 consumer：消息的消费者 Queue 消息队列：提供了FIFO的处理机制，具有缓存数据的能力 在RabbitMQ中，队列消息可以设置为持久化、临时或自动删除 持久化队列：队列中的消息会在Server本地磁盘存储一份，防止系统挂掉，导致数据丢失。 临时队列：队列中的数据在系统重启后就会丢失。 自动删除的队列：当不存在用户连接到Server，队列中的数据就会被自动删除。 ExChange类似于交换机，提供消息路由策略。在RabbitMQ中，Producer不是通过信道直接将消息发送给Queue的，而是先发给ExChange，一个ExChange与多个Queue绑定，Producer在传递消息的时候，会传递一个ROUTING_KEY,ExChange会根据这个值按照特定的路由算法，将消息分配给指定的Queue，与Queue一样，ExChange也有持久、临时和自动删除的。 Binding所谓绑定就是一个特定的Exchange与一个特定的Queue绑定起来。ExChange和Queue的绑定可以是多对多的关系。 VirtualRabbitMQ的使用过程 客户端连接到消息队列服务器，打开一个Channel 客户端声明一个ExChange，并设置相关属性 客户端声明一个Queue，并设置相关属性 客户端使用Routing Key，在ExChange与Queue之间建立好绑定关系 客户端投递消息到ExChange ExChange接收到消息之后，就根据消息的key和已经绑定好的binging，进行消息路由，将消息投递到一个或多个队列里 docker-compose.yml1234567891011121314version: '3.1'services: rabbitmq: restart: always image: rabbitmq:management container_name: rabbitmq ports: - 5672:5672 - 15672:15672 environment: RABBITMQ_DEFAULT_USER: rabbit RABBITMQ_DEFAULT_PASS: 123456 volumes: - ./data:/var/lib/rabbitmq 消息提供者123456spring: rabbitmq: host: 127.0.0.1 port: 5672 username: rabbit password: 123456 123456789101112131415161718192021@Configurationpublic class RabbitConfiguration &#123; @Bean public Queue queue() &#123; return new Queue(\"helloRabbit\"); &#125;&#125;@Componentpublic class RabbitProvider &#123; @Autowired private AmqpTemplate amqpTemplate; public void send() &#123; String content = \"Hello World \" + new Date(); System.out.println(\"Provider: \" + content); amqpTemplate.convertAndSend(\"helloRabbit\", content); &#125;&#125; 消息消费者123456789@Component@RabbitListener(queues = \"helloRabbit\")public class RabbitConsumer &#123; @RabbitHandler public void process(String content) &#123; System.out.println(\"Consumer:\" + content); &#125;&#125; 问题 如何保证消息队列中的数据百分之一百被消费掉","categories":[],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"https://wt-git-repository.github.io/tags/消息队列/"}]},{"title":"计算机网络基础知识之运输层","slug":"计算机网络基础知识","date":"2019-06-03T10:39:19.000Z","updated":"2019-11-14T07:38:19.273Z","comments":true,"path":"2019/06/03/计算机网络基础知识/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/03/计算机网络基础知识/","excerpt":"OSI 七层模型 TCP/IP 五层模型 应用层状态码 1XX Informational（信息性状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 运输层基本术语 进程：指计算机正在运行的实体 应用进程的相互通信： 一台主机的进程和另一台主机的一个进程交换数据的过程 传输层的复用与分用：复用是指发送方不同的进程可以通过一个传输层协议传送数据， 分用指的是接收方的传输层在剥去报文的首部后能把这些数据正确地交到目的应用进程中。 TCP：传输控制协议 UDP：用户数据报协议 端口：为的是确认对方机器是哪一个进程在和自己交互 停止等待协议：指发送方每发送完一个分组就停止发送，等待对方确认，在收到确认之后再发送下一个分组。 流量控制：控制对方的发送速率，既要让接收方来得及接收，也不要使网络发送拥塞 拥塞控制：防止过多数据注入网络中，防止路由器或者链路过载。","text":"OSI 七层模型 TCP/IP 五层模型 应用层状态码 1XX Informational（信息性状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 运输层基本术语 进程：指计算机正在运行的实体 应用进程的相互通信： 一台主机的进程和另一台主机的一个进程交换数据的过程 传输层的复用与分用：复用是指发送方不同的进程可以通过一个传输层协议传送数据， 分用指的是接收方的传输层在剥去报文的首部后能把这些数据正确地交到目的应用进程中。 TCP：传输控制协议 UDP：用户数据报协议 端口：为的是确认对方机器是哪一个进程在和自己交互 停止等待协议：指发送方每发送完一个分组就停止发送，等待对方确认，在收到确认之后再发送下一个分组。 流量控制：控制对方的发送速率，既要让接收方来得及接收，也不要使网络发送拥塞 拥塞控制：防止过多数据注入网络中，防止路由器或者链路过载。 重要知识 网络层为主机提供逻辑通信，运输层为进程提供逻辑通信。 运输层两个重要协议：TCP和UDP。 硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层各种进程交互的一种地址。 运输层的端口号分为服务器端使用的端口号和客户端暂时使用的端口号。 UDP的主要特点：无连接、尽最大努力交互、面向报文、无拥塞控制、支持一对一，一对多，多对一和多对多的交互通信，首部开销小 TCP：面向连接、只能是一对一，提供可靠交互，提供全双工通信，面向字节流 TCP用主机的IP地址加上主机上的端口号作为TCP连接的端点，这个端点就称为套接字，每一条TCP连接都被两个可确定的端口所确定。 停止等待协议：为了实现可靠传输，每传输完一个分组就停止发送，等对方确认之后，再继续发送下一个分组。 停止等待协议的超时重传（自动重传ARQ）：只要超过一段时间仍然没有收到确定，就重传前面发送过的分组。若在该协议中收到重复分组，则还要继续发送确认（因为确认分组也有可能会丢失） 发送维持一个发送窗口，凡是位于窗口之内的分组可以连续发送出去，而不需要等待对方确认，接收方按最后一个分组发送确认，表面到这个分组的位置的全部分组已经全部接收。（滑动窗口机制） 滑动窗口机制：用于流量控制 发送窗口：拥塞控制窗口的大小取决于网络的拥挤程度，并且动态变化，TCP发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口较小的一个。 流量控制：点对点通信量的控制，是个端对端的问题，流量控制所要做到的就是抑制发送端发送数据的速率，以适应接收方的接受能力。 拥塞：在某段时间，软对某一资源的需求超过了该资源所能提供的可用部分，网络性能就会变差，如信道运输数据的能力 拥塞控制：复制过多的数据注入到网络中，防止路由器或者链路不至于过载，拥塞控制是一个全局性的过程，设计到所有主机、路由器以及降低网络传输性能有关的所有因素 TCP拥塞控制的四种算法：慢开始、拥塞避免、快重传和快恢复。 运输层连接的三个阶段：连接建立、数据传送和连接释放。 TCP三次握手四次挥手一开始，服务器首先创建传输块TCB（存储每一个连接中的重要信息），准备接受客户进程的连接请求，此处，服务器进程便进入的LISTEN（监听）状态。 三次握手 客户端首先创建一个TCB，在打算建立TCP连接之时，向服务器发出连接请求报文，SYM=1（SYM=1的报文段表示为一个连接请求报文），seq=x ，TCP规定，SYM=1的报文段需要消耗掉一个序列号。此处客户端进入SYN-SENT（同步已发送）状态 服务器收到请求之后，如果同意建立连接，则向客户端发出确认。ACK=1,SYM=1,ack=x+1,seq=y.TCP 规定，在连接建立后，所有传送的报文端都需要把ACK置为1，此时服务器端进入SYN-RCVD(同步收到)状态 客户端收到服务器端的确认后，还要给服务器确认。ACK=1,ack=y+1,seq=x+1, 此时不是SYN请求连接报文了，ACK报文段是可以携带数据的，但是此处不用，所以seq=x+1.此时客户端进入ESTABLISHED(已经建立)的状态 四次挥手数据传输结束后，通信双方都可释放连接，现在客户端和服务器端都处于ESTABLISHED状态。 客户端发送连接释放报文，FIN=1，seq=u，u是前面已经传送过的数据的最后一个字节加1.TCP规定，FIN报文段即使不携带数据，它也要消耗一个序号。此时，客户端进入FIN-WAIT-1（终止等待1）状态，等待B确认。 服务器收到连接释放报文段后发出确认，ACK=1(ACK=1时，ack才有效),ack=u+1,seq=v，V是服务器之前传送过的数据的最后一个字节的序号+1，此时服务器进入CLOSE-WAIT(关闭等待)状态此时客户端进入FIN-WAIT-2(终止等待2)状态,等待B发出连接释放报文 若服务器已经没有要向客户端发出的数据，应用进程就通知TCP释放连接。FIN=1，ACK=1，ack=u+1,seq=w，seq之所以为w，是因为B可能还有一些数据发送到了给A，此时服务器进入LAST-ACK(最后确认状态)。 客户端收到服务器段的连接释放报文后，必须要对服务器进行确认。ACK=1，ack=w+1,seq=u+1客户端进入TIME—WAIT（时间等待）状态，然后在进入CLOSE状态。 Time-Wait 的作用 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文，A 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。 问题 端口和套接字的意义 无连接UDP的特点 面向连接TCP的特点 在不可靠的网络上实现可靠传输的工作原理，停止等待协议和APQ协议 TCP滑动窗口、流量控制、拥塞控制和连接管理。 参考TCP三次握手四次挥手","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://wt-git-repository.github.io/tags/计算机网络/"}]},{"title":"SpringCloud微服务架构实战读书笔记②","slug":"SpringCloud微服务架构实战读书笔记②","date":"2019-06-03T07:45:27.000Z","updated":"2019-07-08T01:15:52.694Z","comments":true,"path":"2019/06/03/SpringCloud微服务架构实战读书笔记②/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/03/SpringCloud微服务架构实战读书笔记②/","excerpt":"","text":"使用Feign实现声明式REST调用为服务消费者整合Feign 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt;&lt;/dependency&gt; 创建一个Feign接口，并添加@FeignClient注解 12345@FeignClient(name=\"applicationName\")public interface UserFeignClient &#123; @RequestMapping(value = \"/&#123;id&#125;\") public User findById(@PathVatiable(\"id\")Long id);&#125; @FeignClient注解是任意一个服务提供方的名称，用以创建Ribbon负载均衡器。 自定义Feign配置修改接口1@FeignClient(name=\"applicationName\"， configuration = FeignConfiguration.class) 有一点值得注意的是，本例中的FeignConfiguration类不能包括在主程序上下文的@ComponentScan中，否则该类中的配置信息就会被所有的@FeignClient共享， 如果只想定义某个FeignClient客户端的配置，该点需要特别注意。此处可以配置@ComponentScan不扫描配置类的所在包 手动创建FeignFeign对继承的支持Feign对压缩的支持Feign的日志使用Feign构造多参数请求Spring Cloud为Feign添加了Spring MVC的注解支持。 例子一 12345@FeignClient(name=\"applicationName\")public interface UserFeignClient &#123; @GetMapping(value = \"/&#123;id&#125;\") public User findById(@RequestParam Map&lt;String, Object&gt; map);&#125; 例子二 12345@FeignClient(name=\"applicationName\")public interface UserFeignClient &#123; @PostMapping(value = \"/&#123;id&#125;\") public User findById(User user);&#125;","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"MySQL索引与锁","slug":"MySQL索引与锁","date":"2019-06-03T02:58:02.000Z","updated":"2019-07-08T01:15:52.687Z","comments":true,"path":"2019/06/03/MySQL索引与锁/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/03/MySQL索引与锁/","excerpt":"MySQL存储引擎的基础知识 MySQL的基本存储结构是页，所有记录都存储在页里面。 每个页之间组成的是一个双向链表。 每个数据页里面的记录可以组成一个单向链表。 每个数据页都会为存储在本页里面的数据生成一个页目录，在通过主键查找某条记录时可以使用二分法快速定位 而根据其它列进行检索时，就只能通过遍历的手段 在没有任何索引的的表中，select语句的执行会进行如下两次遍历 遍历双向链表，找到所在页 遍历页内的单链表，找到所在的记录","text":"MySQL存储引擎的基础知识 MySQL的基本存储结构是页，所有记录都存储在页里面。 每个页之间组成的是一个双向链表。 每个数据页里面的记录可以组成一个单向链表。 每个数据页都会为存储在本页里面的数据生成一个页目录，在通过主键查找某条记录时可以使用二分法快速定位 而根据其它列进行检索时，就只能通过遍历的手段 在没有任何索引的的表中，select语句的执行会进行如下两次遍历 遍历双向链表，找到所在页 遍历页内的单链表，找到所在的记录 索引提高检索速度索引的主要作用就是将无序变成有序。 record_type=1 代表存放的是普通目录项的记录record_type=0 代表存放的是普通用户的记录 底层结构一般都是B+树，B+树是平衡树的一种，它是一个空树或者是左右子树的高度差的绝对值不会超过1，左右子树都是一颗平衡二叉树。深度为lgn 索引在提高检索速度的同时，同时会降低增删改的速度，因为要对B+树做增删改的话，会破坏它原来的结构，而且要维护平衡树，就必要做额外的工作。 聚集索引和非聚集索引概括： 聚集索引是以主键创建的索引。 非聚集索引是以非主键创建的索引。InnoDB要求表必须有主键（MyISAM可以没有），Innodb会按照如下规则进行处理： 如果一个主键被定义了，那么这个主键就是作为聚集索引 如果没有主键被定义，那么该表的第一个唯一非空索引被作为聚集索引 如果没有主键也没有合适的唯一索引，那么innodb内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键是一个6个字节的列，改列的值会随着数据的插入自增。 区别： 聚集索引在叶子节点存储的是表中的数据。 非聚集索引在叶子节点存储的是主键和索引列。 使用非聚集索引查询出数据时，拿到叶子上的主键再去查想要查找的数据。（拿到主键再去查找的这个过程叫做回表） 聚集索引是物理上的连续，而非聚集索引是逻辑上的连续，物理存储并不连续。 非聚集索引和聚集索引的区别在于:通过聚集索引可以查到需要查找的数据， 而通过非聚集索引可以查到记录对应的主键值 ， 再使用主键的值通过聚集索引查找到需要的数据 覆盖索引： 如果不是聚集索引，叶子节点存储的是主键+索引列，如果需要查询的列，叶子节点都存在，那么就不用回表，提高效率。 索引最左匹配原则最左匹配原则： 索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。 如果是联合索引，那么key也由多个列组成，同时，索引只能用于查找key是否存在（相等），遇到范围查询(&gt;、&lt;、between、like左匹配)等就不能进一步匹配了，后续退化为线性查找。 因此，列的排列顺序决定了可命中索引的列数。 例子： 如有索引(a, b, c, d)，查询条件a = 1 and b = 2 and c &gt; 3 and d = 4，则会在每个节点依次命中a、b、c，无法命中d。(很简单：索引命中只能是相等的情况，不能是范围匹配) 索引总结 最左匹配原则 尽量选择区分度高的列作为索引 索引列不能参与计算，尽量保持列干净 尽可能扩展索引，不要新建立索引 锁 对于UPDATE、DELETE、INSERT语句，InnoDB会自动给涉及数据集加排他锁 表锁（InnoDB行锁表锁都支持，MyISAM只支持表锁） 表锁：开销小，加锁快，不会出现死锁，锁的粒度大，并发度低。 行锁：开销大，加锁慢，会出现死锁，锁的粒度小，并发度大 行锁 InnoDB支持行锁 InnoDB支持表锁 行锁类型： 共享锁（S锁）：允许多个获得共享锁的事物同时读取同一个资源，但不允许其他客户端修改 排他锁（X锁）：允许获得排他锁的事物更新数据，阻止其他事物修改或读取同一数据集。 乐观锁与悲观锁 乐观锁（认为一个用户读数据时，别人不会去写自己所读的数据）：在表中添加一个版本字段，第一次读的时候，获取到这个字段，处理完业务逻辑准备更新的时候，需要再次查看这个字段是否和第一次获取到的字段是否一样，若一样则更新，否则则拒绝。 悲观锁（在读取数据是，不允许别人去修改）：直接在把数据库层面上加锁。 事物的隔离级别什么是事务事物是逻辑上的一组操作，要么全部执行，要么全部不执行。 事务的特性（ACID） 原子性：事务是最小的执行单位，要么全部执行，要么全部不执行 一致性：在事务执行的前后，所有事务对同一数据源的读取结果的一致的 隔离性：并发访问数据库时，各个事务互不干扰 持久性：在事务提交之后，它对数据库的改变是持久的，即使数据库发生故障，也不会有任何影响。 并发事务带来的问题 脏读：当一个事务对数据进行了修改之后就马上释放了排它锁，导致其它事务对未修改的数据进行了“脏数据”。 丢失修改：指两个事务同时对同一个数据进行了修改操作，导致一个事务的修改丢失 不可重复读：指在同一个事务中，对同一个数据进行多次读取，但是读出来的结果不一致，这里侧重于数据修改 幻读：在同一个事务中，同一次查询会多出或者少了一些数据，这是因为另一个并发的任务作出了增删操作，这里侧重与数据的增删 SQL标准定义的四个隔离级别(InnoDB 默认支持可重复读) READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 参考资料 JavaGuide MySQL索引与锁 深入浅出数据库索引原理","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wt-git-repository.github.io/tags/MySQL/"}]},{"title":"《JAVA并发编程的艺术》读书笔记④","slug":"《JAVA并发编程的艺术》读书笔记④","date":"2019-06-03T01:24:05.000Z","updated":"2019-07-08T01:15:52.722Z","comments":true,"path":"2019/06/03/《JAVA并发编程的艺术》读书笔记④/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/03/《JAVA并发编程的艺术》读书笔记④/","excerpt":"","text":"数据依赖性 如果两个操作之间访问同一个变量，且这两个操作有一个为写操作，此时这两个操作就存在数据依赖性。 编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 这里所说的数据依赖性针对单个处理器中的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial语义as-if-serial意思就是说：不管怎么重排序（编译器和处理器为了提高并行度），单线程程序的执行结果不能改变。 同步程序的顺序一致性效果 JMM在具体实现的基本方针是：在不改变正确同步的程序执行结果的前提下，尽可能地为编译器和处理器的优化打开方便之门。 JMM不保证对64位的long型和double型变量的写操作具有原子性。因为当JVM在一些32位的处理器上运行的时候，如果对64为数据的写操作要求原子性，会有比较大的开销，所以JVM可能会把一个64位的long/double写操作拆分成两个32位的写操作的来执行，这两个32位的写操作可能会被分配到不同的总线事务中，此时对这个64位的变量的写操作不具有原子性。 未同步程序的执行特性对于未同步或未正确同步的多线程程序,JMM只提供最小安全性: 线程执行时读取到的值, 要么是之前某个线程写入的值, 要么是默认值(0, NULL, false), JMM保证线程读操作读到的值不会无中生有. 为了实现最小安全性, JVM在堆上分配对象时,首先会对内存空间进行清零, 然后才会在上面分配对象(JVM会同步这两个操作), 因此, 在已清零的内存空间分配对象是,域的默认初始化已经完成了. 锁的内存语义锁的释放和获取的内存语义当线程获取锁的时候,JMM会把该线程对应的本地内存置为无效,从而是的被监视器保护的临界区代码必须从主内存中读取共享变量. 对锁释放和锁获取的内存含义做个总结: 线程A释放一个锁, 实质上是线程A向接下来将要获取这个锁的某个线程发出了线程A对共享变量所做出了修改的消息 线程B获取一个锁,实质上是线程B接收了之前某个线程(即A线程)对这个变量所作出的修改的消息 线程A释放锁, 然后线程B获取锁, 这个过程实质上是线程A通过主内存向线程B发送消息.","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"《JAVA并发编程的艺术》读书笔记③","slug":"《JAVA并发编程的艺术》读书笔记③","date":"2019-06-02T11:20:48.000Z","updated":"2019-09-12T07:48:01.428Z","comments":true,"path":"2019/06/02/《JAVA并发编程的艺术》读书笔记③/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/02/《JAVA并发编程的艺术》读书笔记③/","excerpt":"","text":"JAVA内存模型的基础在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 在共享内存的并发模型中，线程之间共享程序的公共状态，通过读写内存中的公共状态来进行隐式通信。 在消息传递的并发模型中，线程之间没有公共状态，线程之间必须通过发送消息来显式进行通信。 JAVA的并发采用的是共享内存模型。 JAVA内存模型的抽象结构 在JAVA中，所有实例域、静态域和数组元素都在堆内存中，堆内存在线程之间共享。 局部变量、方法定义参数和异常处理函数，在栈内存里面，不会在线程之间共享。 线程之间的共享变量存储在主内存中，每个线程都有一个私有的本地内存，本地内存存储了该内存以读写共享变量的副本。A与B之间的通信，需要通过以下2个步骤 线程A把本地内存A中更新过的共享变量刷新的主内存中。 线程B到主内存中去读取线程A之前已经更新过的共享变量。 从源代码到指令序列的重排序在执行程序时，为了提高性能，编译器和处理器通常会对指令做重排序，重排序分为三种： 编译器优化的重排序：编译器在不改变单线程语义的前提下，可以重新安排语句的执行顺序。 指令级并行的重排序：现代处理器采用了指令级并行技术来将多条指令重叠执行，如果不存在数据依赖性，处理器可以改变语句对应的机器指令的执行顺序。 内存系统的重排序：由于处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是乱序执行。 对于指令重排，JAVA内存模型的处理器重排序规则可以要求JAVA编译器在生成指令序列时，插入特定类型的内存屏障指令，通过内存屏障指令来禁止特定类型的处理器重排序。 内存屏障：指的是重排序时不能把后面的指令重排序到内存屏障之前的位置。 展示一个由于内存操作重排带来问题的例子（经典）在展示之前，首先补充一点知识：现代处理器使用写缓存区临时保存向内存下入的数据，以保证指令流水线持续运行，避免处理器停顿下来等待向内存写入数据而产生延迟，同时，以批处理的方式刷新缓存区，以及合并写缓存区对统一内存地址的多次写，减少堆内存总线的占用。 这个特性会对内存操作的顺序产生影响：处理器堆内存的读写操作的执行顺序，不一定与内存实际发生的读写操作顺序一致（因为内存操作重排） 如果AB同时执行，可能会得到读取到脏数据，原因如下： 此处由于指令重排，导致A1-&gt;A2的顺序变成了A2-&gt;A1，所以读取了脏数据。 一般情况下，处理器都不允许对存在数据依赖的操作做重排序。 happens-before在JMM中，如果一个操作的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关系。 A happens-before B, JMM 并不要求A一定要在B之前执行, JMM仅仅要求前一个操作的结果对后一个操作课件,且前一个操作按顺序排在第二个操作之前.","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"一条SQL语句在MySQL中的执行流程","slug":"一条SQL语句在MySQL中的执行流程","date":"2019-06-02T08:24:55.000Z","updated":"2019-07-08T01:15:52.726Z","comments":true,"path":"2019/06/02/一条SQL语句在MySQL中的执行流程/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/02/一条SQL语句在MySQL中的执行流程/","excerpt":"","text":"MySQL 架构分析首先，我们需要了解MySQL的一个简要的架构 连接器： 身份验证与权限相关 查询缓存：执行查询语句时，会先查询缓存（失效率过高，不推荐用） 分析器：分析SQL语句 优化器：优化SQL语句 执行器：执行SQL语句，并从存储引擎用返回数据 值得注意的是，Mysql分为了Server层和存储引擎层： Server层： 主要包括连接器、查询缓存、分析器、优化器和执行器等组件，很多功能都是在这一层实现的，如存储过程、触发器、视图、函数等等，还有一个日志模块binglog 存储引擎模块：支持InnoDB、MyISAM等等，InnoDB为默认的存储模块。 MySQL的日志模块这里，我们需要特别主义redo log 和 binlog这两个日志模块，前者是InnoDB自带的日志模块，后者是MySQL自带的日志模式。 SQL 语句的类型SQL语句的类型主要分为两种 查询： select 更新：update、delete 执行更新语句时，该如何操作这两个日志模块1update tb_student set name = 'wt' where name = 'name' 首先，要根据where查询到对应那一条数据 根据查询到的数据，先进行修改，然后调用引擎接口，写入这一行数据，InnoDB会把数据保存在缓存中，同时记录redo log,此时redo log 进入prepare状态，然后告诉执行器，执行结束 执行器收到通知后记录binlog，然后调用引擎接口，提交redo log为提交状态 更新完成 小结 MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用,redolog 只有 InnoDB 有。 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。 查询语句的执行流程如下：权限校验（如果命中缓存）—》查询缓存—》分析器—》优化器—》权限校验—》执行器—》引擎 更新语句执行流程如下：分析器—-》权限校验—-》执行器—》引擎—redo log(prepare 状态—》binlog—》redo log(commit状态)","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://wt-git-repository.github.io/tags/MySQL/"}]},{"title":"《JAVA并发编程的艺术》读书笔记②","slug":"《JAVA并发编程的艺术》读书笔记②","date":"2019-06-01T10:44:37.000Z","updated":"2019-07-08T01:15:52.721Z","comments":true,"path":"2019/06/01/《JAVA并发编程的艺术》读书笔记②/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/01/《JAVA并发编程的艺术》读书笔记②/","excerpt":"","text":"前言JAVA代码在编译之后会变成JAVA字节码，字节码被类加载器加载到JVM里面，JVM执行字节码，将字节码装换成汇编指令在CPU上执行。 JAVA中所执行的并发机制依赖JVM的实现和CPU的指令。 volatile在多线程并发编程中，volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的可见性，如果volatile变量修饰符使用恰当，它比synchronized的使用或执行成本更低，因为它不会引起线程上下文的切换和调度。 作用1（保证共享变量的可见性） 将当前处理器缓存行的数据写会到系统内存中。 这个写会内存的操作会使其他CPU里缓存了该内存地址的数据无效。 对作用1的解释：为了提高处理速度，处理器不直接与内存进行通信，而是将内存中的数据读到内部的缓存里面再进行操作，而为了保持缓存的一致性，在多处理器模式下，就实现缓存一致性的协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是否已经过期，当处理器发现自己缓存行对应的内存地址被修改，将会将当前处理器的缓存行设置为无效状态，当处理器对这个数据进行修改操作的时候，就会重新从系统内存中把数据督导处理器缓存中。 volatile实现原则 Lock前缀指令会引起处理器缓存写回到内存中。 一个处理器的缓存回写到内存中会导致其他处理器的缓存无效。 特性 可见性：对一个Volatile变量的读，总是能够看到任意线程对这个volatile的最后写入。 原子性：对任意单个volatile的变量的读写具有原子性，但是类似于volatile++这种复合操作不具有原子性。 volatile写-读建立的happens-before关系 从内存语义的角度来说，volatile的写-读与锁的释放-获取有相同的内存效果。 synchronized 对于普通同步方法，锁是当前实例对象。 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是synchronized括号里配置的对象。 原子操作原子操作意思就是不可被中断的一个或一系列的操作。 处理器保证原子操作的措施 总线锁：举个例子，当CPU获取到共享变量时，会发出LOCK#信号去组织其它处理器对这个变量的请求 缓存锁：","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"《JAVA并发编程的艺术》读书笔记①","slug":"《JAVA并发编程的艺术》读书笔记①","date":"2019-06-01T09:50:59.000Z","updated":"2019-07-08T01:15:52.720Z","comments":true,"path":"2019/06/01/《JAVA并发编程的艺术》读书笔记①/","link":"","permalink":"https://wt-git-repository.github.io/2019/06/01/《JAVA并发编程的艺术》读书笔记①/","excerpt":"","text":"上下文切换在单核处理器下，支持多线程执行代码，CPU给每一个线程分配CPU时间片来实现这个机制，由于分配的时间片比较短，一般为几十毫秒，所以CPU需要不停地切换线程，在切换之前会通过程序计数器去保存上一个线程的任务状态，以便在下一次执行这个任务时，可以再次加载这个任务的状态。 任务从保存到再加载的过程就是一次上下文切换。 问题：多线程一定快吗答：不一定，因为多线程有上下文切换的开销。 如何减少上下文切换答：减少上下文切换的方法有无锁并发编程、CAS算法、使用最小线程和使用协程。 无锁并发编程：多线程竞争锁时，会引起上下文切换，所以多线程处理数据时，可以使用一些方法来避免使用锁 CAS算法 使用最小线程：避免创建不需要的线程，避免造成大量线程处于等待状态 协程：在当线程里面实现多任务的调度，并在单线程里维持多个任务间的切换 如何避免死锁 避免在一个线程同时获取多个锁 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只用一个资源 尝试使用定时锁，使用lock,tryLock(timeout)来替代使用内部锁机制 对于数据库锁，加锁和解锁都必须在同一个数据库连接里，否则会出现解锁失败的情况 资源限制的问题所谓资源限制，通常所指的是硬件的条件不能满足程序的需求，在这种情况下，多线程可能会延长执行时间，因为存在上下文切换的开销，这个时候，通常可以考虑集群并行执行程序。 在资源受限的情况下，需要根据具体的情况去决定程序的并发度，以达到最优状态。","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"进程、程序与线程","slug":"进程、程序与线程","date":"2019-05-31T02:42:14.000Z","updated":"2019-07-08T01:15:52.737Z","comments":true,"path":"2019/05/31/进程、程序与线程/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/31/进程、程序与线程/","excerpt":"","text":"线程线程是一个比进程更小的执行单位，一个进程在其执行的过程中可以产生多个线程，在同一个进程内，各个线程共享着同一块内存空间和同一组系统资源，线程的上下文切换也要比进程要小 程序程序是含有指令和数据的文件，被存储在磁盘或者其它数据存储设备中，程序是静态的代码。 进程进程是系统运行程序的基本单位，是动态的，是程序的一次执行过程。系统运行一个程序是一个进程从创建、运行到消亡的过程。简单来说， 一个进程就相当于一个执行中的程序，它在计算机中一个指令接着另一个指令地执行着， 同时每个进程还会占有某些系统资源如CPU时间、内存空间、文件、输入输出设备等等， 换句话说，当程序在执行时，程序会被操作系统载入内存中，。 JAVA线程状态 Daemon线程Daemon线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持工作，当一个JAVA虚拟机中不存在非Daemon线程的时候，JAVA虚拟机将会退出。 可以使用Thread.setDaemon(true)来设置Daemon线程。 Deamon线程被用作支持性工作，但是在JAVA虚拟机退出时，Daemo线程中的finally块不一定会执行。 对象、监视器、同步队列和执行线程之间的关系 线程间的通信等待/通知机制 有一些细节上的东西需要注意一下： 使用wait、notify、notifyAll时需要对调用对象加锁。 调用wait方法后，线程状态由RUNNING变成WAITING，并将当前线程放置在对象的等待队列中。 notify和notifyAll调用后，等到线程依旧不会从wait返回，而是要等本线程释放锁之后，等待线程才有机会返回 notify是将等待队列中的一个等待线程从等待队列中移到同步队列中 notifyAll是将等待队列中所有的线程全部移到同步队列中，被移动的线程状态由WAITING变为BLOKCED 从wait方法返回后重新执行的前提是获得对象的锁","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"hashCode和equals","slug":"hashCode和equals","date":"2019-05-31T02:13:46.000Z","updated":"2019-07-08T01:15:52.719Z","comments":true,"path":"2019/05/31/hashCode和equals/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/31/hashCode和equals/","excerpt":"","text":"问题缘由：重写equals时必须要重写hashCode方法 简介hashCode()的作用是获取哈希码， 也称为散列码，它实际上就是一个int整数， 这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode定义在JDK的Object中， 这就意味者所有的类都含有hashCode方法 散列表存储的是键值对， 它的特点是， 能够根据键快速检索出对应的值， 其中就利用到了散列码。 哈希码的作用是获取对象在哈希表中的索引位置。 HashSet的插入机制HashSet不允许重复的插入， 在每一次插入时， 会根据hashCode来判断对象的插入位置， 同时会与已经存在的hashcode作比较， 如果没有相符的hashCode， 则没有重复的对象， 如果有重复的hashCode, 则会使用equals来判断值是否相等， 若相等则不插入， 若不相等则重新散列到其它位置， 这样可以大大减少equals的次数， 大大提高执行速度。 重写equals时必须要重写hashCode方法因为在使用equals之前，首先是使用hashcode去判断对象的索引位置， hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.util.*;import java.lang.Comparable;public class ConflictHashCodeTest2&#123; public static void main(String[] args) &#123; // 新建Person对象， Person p1 = new Person(\"eee\", 100); Person p2 = new Person(\"eee\", 100); Person p3 = new Person(\"aaa\", 200); Person p4 = new Person(\"EEE\", 100); // 新建HashSet对象 HashSet set = new HashSet(); set.add(p1); set.add(p2); set.add(p3); // 比较p1 和 p2， 并打印它们的hashCode() System.out.printf(\"p1.equals(p2) : %s; p1(%d) p2(%d)\\n\", p1.equals(p2), p1.hashCode(), p2.hashCode()); // 比较p1 和 p4， 并打印它们的hashCode() System.out.printf(\"p1.equals(p4) : %s; p1(%d) p4(%d)\\n\", p1.equals(p4), p1.hashCode(), p4.hashCode()); // 打印set System.out.printf(\"set:%s\\n\", set); &#125; /** * @desc Person类。 */ private static class Person &#123; int age; String name; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String toString() &#123; return name + \" - \" +age; &#125; /** * @desc重写hashCode */ @Override public int hashCode()&#123; int nameHash = name.toUpperCase().hashCode(); return nameHash ^ age; &#125; /** * @desc 覆盖equals方法 */ @Override public boolean equals(Object obj)&#123; if(obj == null)&#123; return false; &#125; //如果是同一个对象返回true，反之返回false if(this == obj)&#123; return true; &#125; //判断是否类型相同 if(this.getClass() != obj.getClass())&#123; return false; &#125; Person person = (Person)obj; return name.equals(person.name) &amp;&amp; age==person.age; &#125; &#125;&#125;","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"LinkedList源码学习","slug":"LinkedList源码学习","date":"2019-05-31T00:34:52.000Z","updated":"2019-07-08T01:15:52.681Z","comments":true,"path":"2019/05/31/LinkedList源码学习/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/31/LinkedList源码学习/","excerpt":"简介LinkedList 是一个实现了List和Deque接口的双向链表， List接口都是一个常规的增删改查操作， 而Deque是的LinkedList具有队列的特性， LinkList不是线程安全的，如果先是的LinkedList变成线程安全的，可以使用1List list=Collections.synchronizedList(new LinkedList(...));","text":"简介LinkedList 是一个实现了List和Deque接口的双向链表， List接口都是一个常规的增删改查操作， 而Deque是的LinkedList具有队列的特性， LinkList不是线程安全的，如果先是的LinkedList变成线程安全的，可以使用1List list=Collections.synchronizedList(new LinkedList(...)); 源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, Serializable &#123; //一旦变量被transient修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。 //transient关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被transient关键字修饰的。变量如果是用户自定义类变量，则该类需要实现Serializable接口。 //被transient关键字修饰的变量不再能被序列化，一个静态变量不管是否被transient修饰，均不能被序列化 transient int size; transient LinkedList.Node&lt;E&gt; first; transient LinkedList.Node&lt;E&gt; last; private static final long serialVersionUID = 876323262645176354L; public LinkedList() &#123; this.size = 0; &#125; // 根据指定集合添加元素 public LinkedList(Collection&lt;? extends E&gt; var1) &#123; this(); this.addAll(var1); &#125; // 将元素插入到队列首部 private void linkFirst(E var1) &#123; LinkedList.Node var2 = this.first; LinkedList.Node var3 = new LinkedList.Node((LinkedList.Node)null, var1, var2); this.first = var3; if (var2 == null) &#123; this.last = var3; &#125; else &#123; var2.prev = var3; &#125; ++this.size; ++this.modCount; &#125; // 将元素插入到队列尾部 void linkLast(E var1) &#123; LinkedList.Node var2 = this.last; LinkedList.Node var3 = new LinkedList.Node(var2, var1, (LinkedList.Node)null); this.last = var3; if (var2 == null) &#123; this.first = var3; &#125; else &#123; var2.next = var3; &#125; ++this.size; ++this.modCount; &#125; // 将元素插入到指定位置之前 void linkBefore(E var1, LinkedList.Node&lt;E&gt; var2) &#123; LinkedList.Node var3 = var2.prev; LinkedList.Node var4 = new LinkedList.Node(var3, var1, var2); var2.prev = var4; if (var3 == null) &#123; this.first = var4; &#125; else &#123; var3.next = var4; &#125; ++this.size; ++this.modCount; &#125; private E unlinkFirst(LinkedList.Node&lt;E&gt; var1) &#123; Object var2 = var1.item; LinkedList.Node var3 = var1.next; var1.item = null; var1.next = null; this.first = var3; if (var3 == null) &#123; this.last = null; &#125; else &#123; var3.prev = null; &#125; --this.size; ++this.modCount; return var2; &#125; private E unlinkLast(LinkedList.Node&lt;E&gt; var1) &#123; Object var2 = var1.item; LinkedList.Node var3 = var1.prev; var1.item = null; var1.prev = null; this.last = var3; if (var3 == null) &#123; this.first = null; &#125; else &#123; var3.next = null; &#125; --this.size; ++this.modCount; return var2; &#125; E unlink(LinkedList.Node&lt;E&gt; var1) &#123; Object var2 = var1.item; LinkedList.Node var3 = var1.next; LinkedList.Node var4 = var1.prev; if (var4 == null) &#123; this.first = var3; &#125; else &#123; var4.next = var3; var1.prev = null; &#125; if (var3 == null) &#123; this.last = var4; &#125; else &#123; var3.prev = var4; var1.next = null; &#125; var1.item = null; --this.size; ++this.modCount; return var2; &#125; public E getFirst() &#123; LinkedList.Node var1 = this.first; if (var1 == null) &#123; throw new NoSuchElementException(); &#125; else &#123; return var1.item; &#125; &#125; public E getLast() &#123; LinkedList.Node var1 = this.last; if (var1 == null) &#123; throw new NoSuchElementException(); &#125; else &#123; return var1.item; &#125; &#125; public E removeFirst() &#123; LinkedList.Node var1 = this.first; if (var1 == null) &#123; throw new NoSuchElementException(); &#125; else &#123; return this.unlinkFirst(var1); &#125; &#125; public E removeLast() &#123; LinkedList.Node var1 = this.last; if (var1 == null) &#123; throw new NoSuchElementException(); &#125; else &#123; return this.unlinkLast(var1); &#125; &#125; public void addFirst(E var1) &#123; this.linkFirst(var1); &#125; public void addLast(E var1) &#123; this.linkLast(var1); &#125; public boolean contains(Object var1) &#123; return this.indexOf(var1) != -1; &#125; public int size() &#123; return this.size; &#125; public boolean add(E var1) &#123; this.linkLast(var1); return true; &#125; public boolean remove(Object var1) &#123; LinkedList.Node var2; if (var1 == null) &#123; for(var2 = this.first; var2 != null; var2 = var2.next) &#123; if (var2.item == null) &#123; this.unlink(var2); return true; &#125; &#125; &#125; else &#123; for(var2 = this.first; var2 != null; var2 = var2.next) &#123; if (var1.equals(var2.item)) &#123; this.unlink(var2); return true; &#125; &#125; &#125; return false; &#125; public boolean addAll(Collection&lt;? extends E&gt; var1) &#123; return this.addAll(this.size, var1); &#125; public boolean addAll(int var1, Collection&lt;? extends E&gt; var2) &#123; this.checkPositionIndex(var1); Object[] var3 = var2.toArray(); int var4 = var3.length; if (var4 == 0) &#123; return false; &#125; else &#123; LinkedList.Node var5; LinkedList.Node var6; if (var1 == this.size) &#123; var6 = null; var5 = this.last; &#125; else &#123; var6 = this.node(var1); var5 = var6.prev; &#125; Object[] var7 = var3; int var8 = var3.length; for(int var9 = 0; var9 &lt; var8; ++var9) &#123; Object var10 = var7[var9]; LinkedList.Node var12 = new LinkedList.Node(var5, var10, (LinkedList.Node)null); if (var5 == null) &#123; this.first = var12; &#125; else &#123; var5.next = var12; &#125; var5 = var12; &#125; if (var6 == null) &#123; this.last = var5; &#125; else &#123; var5.next = var6; var6.prev = var5; &#125; this.size += var4; ++this.modCount; return true; &#125; &#125; public void clear() &#123; LinkedList.Node var2; for(LinkedList.Node var1 = this.first; var1 != null; var1 = var2) &#123; var2 = var1.next; var1.item = null; var1.next = null; var1.prev = null; &#125; this.first = this.last = null; this.size = 0; ++this.modCount; &#125; public E get(int var1) &#123; this.checkElementIndex(var1); return this.node(var1).item; &#125; public E set(int var1, E var2) &#123; this.checkElementIndex(var1); LinkedList.Node var3 = this.node(var1); Object var4 = var3.item; var3.item = var2; return var4; &#125; public void add(int var1, E var2) &#123; this.checkPositionIndex(var1); if (var1 == this.size) &#123; this.linkLast(var2); &#125; else &#123; this.linkBefore(var2, this.node(var1)); &#125; &#125; public E remove(int var1) &#123; this.checkElementIndex(var1); return this.unlink(this.node(var1)); &#125; private boolean isElementIndex(int var1) &#123; return var1 &gt;= 0 &amp;&amp; var1 &lt; this.size; &#125; private boolean isPositionIndex(int var1) &#123; return var1 &gt;= 0 &amp;&amp; var1 &lt;= this.size; &#125; private String outOfBoundsMsg(int var1) &#123; return \"Index: \" + var1 + \", Size: \" + this.size; &#125; private void checkElementIndex(int var1) &#123; if (!this.isElementIndex(var1)) &#123; throw new IndexOutOfBoundsException(this.outOfBoundsMsg(var1)); &#125; &#125; private void checkPositionIndex(int var1) &#123; if (!this.isPositionIndex(var1)) &#123; throw new IndexOutOfBoundsException(this.outOfBoundsMsg(var1)); &#125; &#125; LinkedList.Node&lt;E&gt; node(int var1) &#123; LinkedList.Node var2; int var3; if (var1 &lt; this.size &gt;&gt; 1) &#123; var2 = this.first; for(var3 = 0; var3 &lt; var1; ++var3) &#123; var2 = var2.next; &#125; return var2; &#125; else &#123; var2 = this.last; for(var3 = this.size - 1; var3 &gt; var1; --var3) &#123; var2 = var2.prev; &#125; return var2; &#125; &#125; // 从头开始找 public int indexOf(Object var1) &#123; int var2 = 0; LinkedList.Node var3; if (var1 == null) &#123; for(var3 = this.first; var3 != null; var3 = var3.next) &#123; if (var3.item == null) &#123; return var2; &#125; ++var2; &#125; &#125; else &#123; for(var3 = this.first; var3 != null; var3 = var3.next) &#123; if (var1.equals(var3.item)) &#123; return var2; &#125; ++var2; &#125; &#125; return -1; &#125; // 从后面开始找 public int lastIndexOf(Object var1) &#123; int var2 = this.size; LinkedList.Node var3; if (var1 == null) &#123; for(var3 = this.last; var3 != null; var3 = var3.prev) &#123; --var2; if (var3.item == null) &#123; return var2; &#125; &#125; &#125; else &#123; for(var3 = this.last; var3 != null; var3 = var3.prev) &#123; --var2; if (var1.equals(var3.item)) &#123; return var2; &#125; &#125; &#125; return -1; &#125; // 返回头部，为空则返回null public E peek() &#123; LinkedList.Node var1 = this.first; return var1 == null ? null : var1.item; &#125; public E element() &#123; return this.getFirst(); &#125; public E poll() &#123; LinkedList.Node var1 = this.first; return var1 == null ? null : this.unlinkFirst(var1); &#125; public E remove() &#123; return this.removeFirst(); &#125; public boolean offer(E var1) &#123; return this.add(var1); &#125; public boolean offerFirst(E var1) &#123; this.addFirst(var1); return true; &#125; public boolean offerLast(E var1) &#123; this.addLast(var1); return true; &#125; public E peekFirst() &#123; LinkedList.Node var1 = this.first; return var1 == null ? null : var1.item; &#125; public E peekLast() &#123; LinkedList.Node var1 = this.last; return var1 == null ? null : var1.item; &#125; public E pollFirst() &#123; LinkedList.Node var1 = this.first; return var1 == null ? null : this.unlinkFirst(var1); &#125; public E pollLast() &#123; LinkedList.Node var1 = this.last; return var1 == null ? null : this.unlinkLast(var1); &#125; public void push(E var1) &#123; this.addFirst(var1); &#125; public E pop() &#123; return this.removeFirst(); &#125; public boolean removeFirstOccurrence(Object var1) &#123; return this.remove(var1); &#125; public boolean removeLastOccurrence(Object var1) &#123; LinkedList.Node var2; if (var1 == null) &#123; for(var2 = this.last; var2 != null; var2 = var2.prev) &#123; if (var2.item == null) &#123; this.unlink(var2); return true; &#125; &#125; &#125; else &#123; for(var2 = this.last; var2 != null; var2 = var2.prev) &#123; if (var1.equals(var2.item)) &#123; this.unlink(var2); return true; &#125; &#125; &#125; return false; &#125; public ListIterator&lt;E&gt; listIterator(int var1) &#123; this.checkPositionIndex(var1); return new LinkedList.ListItr(var1); &#125; public Iterator&lt;E&gt; descendingIterator() &#123; return new LinkedList.DescendingIterator(); &#125; private LinkedList&lt;E&gt; superClone() &#123; try &#123; return (LinkedList)super.clone(); &#125; catch (CloneNotSupportedException var2) &#123; throw new InternalError(var2); &#125; &#125; public Object clone() &#123; LinkedList var1 = this.superClone(); var1.first = var1.last = null; var1.size = 0; var1.modCount = 0; for(LinkedList.Node var2 = this.first; var2 != null; var2 = var2.next) &#123; var1.add(var2.item); &#125; return var1; &#125; public Object[] toArray() &#123; Object[] var1 = new Object[this.size]; int var2 = 0; for(LinkedList.Node var3 = this.first; var3 != null; var3 = var3.next) &#123; var1[var2++] = var3.item; &#125; return var1; &#125; public &lt;T&gt; T[] toArray(T[] var1) &#123; if (var1.length &lt; this.size) &#123; var1 = (Object[])((Object[])Array.newInstance(var1.getClass().getComponentType(), this.size)); &#125; int var2 = 0; Object[] var3 = var1; for(LinkedList.Node var4 = this.first; var4 != null; var4 = var4.next) &#123; var3[var2++] = var4.item; &#125; if (var1.length &gt; this.size) &#123; var1[this.size] = null; &#125; return var1; &#125; private void writeObject(ObjectOutputStream var1) throws IOException &#123; var1.defaultWriteObject(); var1.writeInt(this.size); for(LinkedList.Node var2 = this.first; var2 != null; var2 = var2.next) &#123; var1.writeObject(var2.item); &#125; &#125; private void readObject(ObjectInputStream var1) throws IOException, ClassNotFoundException &#123; var1.defaultReadObject(); int var2 = var1.readInt(); for(int var3 = 0; var3 &lt; var2; ++var3) &#123; this.linkLast(var1.readObject()); &#125; &#125; public Spliterator&lt;E&gt; spliterator() &#123; return new LinkedList.LLSpliterator(this, -1, 0); &#125; static final class LLSpliterator&lt;E&gt; implements Spliterator&lt;E&gt; &#123; static final int BATCH_UNIT = 1024; static final int MAX_BATCH = 33554432; final LinkedList&lt;E&gt; list; LinkedList.Node&lt;E&gt; current; int est; int expectedModCount; int batch; LLSpliterator(LinkedList&lt;E&gt; var1, int var2, int var3) &#123; this.list = var1; this.est = var2; this.expectedModCount = var3; &#125; final int getEst() &#123; int var1; if ((var1 = this.est) &lt; 0) &#123; LinkedList var2; if ((var2 = this.list) == null) &#123; var1 = this.est = 0; &#125; else &#123; this.expectedModCount = var2.modCount; this.current = var2.first; var1 = this.est = var2.size; &#125; &#125; return var1; &#125; public long estimateSize() &#123; return (long)this.getEst(); &#125; public Spliterator&lt;E&gt; trySplit() &#123; int var2 = this.getEst(); LinkedList.Node var1; if (var2 &gt; 1 &amp;&amp; (var1 = this.current) != null) &#123; int var3 = this.batch + 1024; if (var3 &gt; var2) &#123; var3 = var2; &#125; if (var3 &gt; 33554432) &#123; var3 = 33554432; &#125; Object[] var4 = new Object[var3]; int var5 = 0; do &#123; var4[var5++] = var1.item; &#125; while((var1 = var1.next) != null &amp;&amp; var5 &lt; var3); this.current = var1; this.batch = var5; this.est = var2 - var5; return Spliterators.spliterator(var4, 0, var5, 16); &#125; else &#123; return null; &#125; &#125; public void forEachRemaining(Consumer&lt;? super E&gt; var1) &#123; if (var1 == null) &#123; throw new NullPointerException(); &#125; else &#123; LinkedList.Node var2; int var3; if ((var3 = this.getEst()) &gt; 0 &amp;&amp; (var2 = this.current) != null) &#123; this.current = null; this.est = 0; do &#123; Object var4 = var2.item; var2 = var2.next; var1.accept(var4); if (var2 == null) &#123; break; &#125; --var3; &#125; while(var3 &gt; 0); &#125; if (this.list.modCount != this.expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125; public boolean tryAdvance(Consumer&lt;? super E&gt; var1) &#123; if (var1 == null) &#123; throw new NullPointerException(); &#125; else &#123; LinkedList.Node var2; if (this.getEst() &gt; 0 &amp;&amp; (var2 = this.current) != null) &#123; --this.est; Object var3 = var2.item; this.current = var2.next; var1.accept(var3); if (this.list.modCount != this.expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; else &#123; return true; &#125; &#125; else &#123; return false; &#125; &#125; &#125; public int characteristics() &#123; return 16464; &#125; &#125; private class DescendingIterator implements Iterator&lt;E&gt; &#123; private final LinkedList&lt;E&gt;.ListItr itr; private DescendingIterator() &#123; this.itr = LinkedList.this.new ListItr(LinkedList.this.size()); &#125; public boolean hasNext() &#123; return this.itr.hasPrevious(); &#125; public E next() &#123; return this.itr.previous(); &#125; public void remove() &#123; this.itr.remove(); &#125; &#125; private static class Node&lt;E&gt; &#123; E item; LinkedList.Node&lt;E&gt; next; LinkedList.Node&lt;E&gt; prev; Node(LinkedList.Node&lt;E&gt; var1, E var2, LinkedList.Node&lt;E&gt; var3) &#123; this.item = var2; this.next = var3; this.prev = var1; &#125; &#125; private class ListItr implements ListIterator&lt;E&gt; &#123; private LinkedList.Node&lt;E&gt; lastReturned; private LinkedList.Node&lt;E&gt; next; private int nextIndex; private int expectedModCount; ListItr(int var2) &#123; this.expectedModCount = LinkedList.this.modCount; this.next = var2 == LinkedList.this.size ? null : LinkedList.this.node(var2); this.nextIndex = var2; &#125; public boolean hasNext() &#123; return this.nextIndex &lt; LinkedList.this.size; &#125; public E next() &#123; this.checkForComodification(); if (!this.hasNext()) &#123; throw new NoSuchElementException(); &#125; else &#123; this.lastReturned = this.next; this.next = this.next.next; ++this.nextIndex; return this.lastReturned.item; &#125; &#125; public boolean hasPrevious() &#123; return this.nextIndex &gt; 0; &#125; public E previous() &#123; this.checkForComodification(); if (!this.hasPrevious()) &#123; throw new NoSuchElementException(); &#125; else &#123; this.lastReturned = this.next = this.next == null ? LinkedList.this.last : this.next.prev; --this.nextIndex; return this.lastReturned.item; &#125; &#125; public int nextIndex() &#123; return this.nextIndex; &#125; public int previousIndex() &#123; return this.nextIndex - 1; &#125; public void remove() &#123; this.checkForComodification(); if (this.lastReturned == null) &#123; throw new IllegalStateException(); &#125; else &#123; LinkedList.Node var1 = this.lastReturned.next; LinkedList.this.unlink(this.lastReturned); if (this.next == this.lastReturned) &#123; this.next = var1; &#125; else &#123; --this.nextIndex; &#125; this.lastReturned = null; ++this.expectedModCount; &#125; &#125; public void set(E var1) &#123; if (this.lastReturned == null) &#123; throw new IllegalStateException(); &#125; else &#123; this.checkForComodification(); this.lastReturned.item = var1; &#125; &#125; public void add(E var1) &#123; this.checkForComodification(); this.lastReturned = null; if (this.next == null) &#123; LinkedList.this.linkLast(var1); &#125; else &#123; LinkedList.this.linkBefore(var1, this.next); &#125; ++this.nextIndex; ++this.expectedModCount; &#125; public void forEachRemaining(Consumer&lt;? super E&gt; var1) &#123; Objects.requireNonNull(var1); while(LinkedList.this.modCount == this.expectedModCount &amp;&amp; this.nextIndex &lt; LinkedList.this.size) &#123; var1.accept(this.next.item); this.lastReturned = this.next; this.next = this.next.next; ++this.nextIndex; &#125; this.checkForComodification(); &#125; final void checkForComodification() &#123; if (LinkedList.this.modCount != this.expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125;&#125; JavaGuide的demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121package list;import java.util.Iterator;import java.util.LinkedList;public class LinkedListDemo &#123; public static void main(String[] srgs) &#123; //创建存放int类型的linkedList LinkedList&lt;Integer&gt; linkedList = new LinkedList&lt;&gt;(); /************************** linkedList的基本操作 ************************/ linkedList.addFirst(0); // 添加元素到列表开头 linkedList.add(1); // 在列表结尾添加元素 linkedList.add(2, 2); // 在指定位置添加元素 linkedList.addLast(3); // 添加元素到列表结尾 System.out.println(\"LinkedList（直接输出的）: \" + linkedList); System.out.println(\"getFirst()获得第一个元素: \" + linkedList.getFirst()); // 返回此列表的第一个元素 System.out.println(\"getLast()获得第最后一个元素: \" + linkedList.getLast()); // 返回此列表的最后一个元素 System.out.println(\"removeFirst()删除第一个元素并返回: \" + linkedList.removeFirst()); // 移除并返回此列表的第一个元素 System.out.println(\"removeLast()删除最后一个元素并返回: \" + linkedList.removeLast()); // 移除并返回此列表的最后一个元素 System.out.println(\"After remove:\" + linkedList); System.out.println(\"contains()方法判断列表是否包含1这个元素:\" + linkedList.contains(1)); // 判断此列表包含指定元素，如果是，则返回true System.out.println(\"该linkedList的大小 : \" + linkedList.size()); // 返回此列表的元素个数 /************************** 位置访问操作 ************************/ System.out.println(\"-----------------------------------------\"); linkedList.set(1, 3); // 将此列表中指定位置的元素替换为指定的元素 System.out.println(\"After set(1, 3):\" + linkedList); System.out.println(\"get(1)获得指定位置（这里为1）的元素: \" + linkedList.get(1)); // 返回此列表中指定位置处的元素 /************************** Search操作 ************************/ System.out.println(\"-----------------------------------------\"); linkedList.add(3); System.out.println(\"indexOf(3): \" + linkedList.indexOf(3)); // 返回此列表中首次出现的指定元素的索引 System.out.println(\"lastIndexOf(3): \" + linkedList.lastIndexOf(3));// 返回此列表中最后出现的指定元素的索引 /************************** Queue操作 ************************/ System.out.println(\"-----------------------------------------\"); System.out.println(\"peek(): \" + linkedList.peek()); // 获取但不移除此列表的头 System.out.println(\"element(): \" + linkedList.element()); // 获取但不移除此列表的头 linkedList.poll(); // 获取并移除此列表的头 System.out.println(\"After poll():\" + linkedList); linkedList.remove(); System.out.println(\"After remove():\" + linkedList); // 获取并移除此列表的头 linkedList.offer(4); System.out.println(\"After offer(4):\" + linkedList); // 将指定元素添加到此列表的末尾 /************************** Deque操作 ************************/ System.out.println(\"-----------------------------------------\"); linkedList.offerFirst(2); // 在此列表的开头插入指定的元素 System.out.println(\"After offerFirst(2):\" + linkedList); linkedList.offerLast(5); // 在此列表末尾插入指定的元素 System.out.println(\"After offerLast(5):\" + linkedList); System.out.println(\"peekFirst(): \" + linkedList.peekFirst()); // 获取但不移除此列表的第一个元素 System.out.println(\"peekLast(): \" + linkedList.peekLast()); // 获取但不移除此列表的第一个元素 linkedList.pollFirst(); // 获取并移除此列表的第一个元素 System.out.println(\"After pollFirst():\" + linkedList); linkedList.pollLast(); // 获取并移除此列表的最后一个元素 System.out.println(\"After pollLast():\" + linkedList); linkedList.push(2); // 将元素推入此列表所表示的堆栈（插入到列表的头） System.out.println(\"After push(2):\" + linkedList); linkedList.pop(); // 从此列表所表示的堆栈处弹出一个元素（获取并移除列表第一个元素） System.out.println(\"After pop():\" + linkedList); linkedList.add(3); linkedList.removeFirstOccurrence(3); // 从此列表中移除第一次出现的指定元素（从头部到尾部遍历列表） System.out.println(\"After removeFirstOccurrence(3):\" + linkedList); linkedList.removeLastOccurrence(3); // 从此列表中移除最后一次出现的指定元素（从尾部到头部遍历列表） System.out.println(\"After removeFirstOccurrence(3):\" + linkedList); /************************** 遍历操作 ************************/ System.out.println(\"-----------------------------------------\"); linkedList.clear(); for (int i = 0; i &lt; 100000; i++) &#123; linkedList.add(i); &#125; // 迭代器遍历 long start = System.currentTimeMillis(); Iterator&lt;Integer&gt; iterator = linkedList.iterator(); while (iterator.hasNext()) &#123; iterator.next(); &#125; long end = System.currentTimeMillis(); System.out.println(\"Iterator：\" + (end - start) + \" ms\"); // 顺序遍历(随机遍历) start = System.currentTimeMillis(); for (int i = 0; i &lt; linkedList.size(); i++) &#123; linkedList.get(i); &#125; end = System.currentTimeMillis(); System.out.println(\"for：\" + (end - start) + \" ms\"); // 另一种for循环遍历 start = System.currentTimeMillis(); for (Integer i : linkedList) ; end = System.currentTimeMillis(); System.out.println(\"for2：\" + (end - start) + \" ms\"); // 通过pollFirst()或pollLast()来遍历LinkedList LinkedList&lt;Integer&gt; temp1 = new LinkedList&lt;&gt;(); temp1.addAll(linkedList); start = System.currentTimeMillis(); while (temp1.size() != 0) &#123; temp1.pollFirst(); &#125; end = System.currentTimeMillis(); System.out.println(\"pollFirst()或pollLast()：\" + (end - start) + \" ms\"); // 通过removeFirst()或removeLast()来遍历LinkedList LinkedList&lt;Integer&gt; temp2 = new LinkedList&lt;&gt;(); temp2.addAll(linkedList); start = System.currentTimeMillis(); while (temp2.size() != 0) &#123; temp2.removeFirst(); &#125; end = System.currentTimeMillis(); System.out.println(\"removeFirst()或removeLast()：\" + (end - start) + \" ms\"); &#125;&#125;","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"String、StringBuilder和StringBuffer","slug":"String、StringBuilder和StringBuffer","date":"2019-05-30T11:13:33.000Z","updated":"2019-07-08T01:15:52.696Z","comments":true,"path":"2019/05/30/String、StringBuilder和StringBuffer/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/30/String、StringBuilder和StringBuffer/","excerpt":"","text":"String12345678910111213141516171819public final class String implements Serializable, Comparable&lt;String&gt;, CharSequence &#123; private final char[] value; private int hash; private static final long serialVersionUID = -6849794470754667710L; private static final ObjectStreamField[] serialPersistentFields = new ObjectStreamField[0]; public static final Comparator&lt;String&gt; CASE_INSENSITIVE_ORDER = new String.CaseInsensitiveComparator(); public String() &#123; this.value = \"\".value; &#125; public String(String var1) &#123; this.value = var1.value; this.hash = var1.hash; &#125; public String(char[] var1) &#123; this.value = Arrays.copyOf(var1, var1.length); &#125; 从String 的源码中，我们不难发现， String存储的字符串， 对象是final类型的， 不可变， 线程安全。 StringBuffer and StringBuilderStringBuffer和StringBuilder都继承了AbstractStringBuilder并且使用的基本都是父类的方法，区别在于前程是线程安全的，后者是线程不安全的，前者比后者快。1234567891011abstract class AbstractStringBuilder implements Appendable, CharSequence &#123; char[] value; int count; private static final int MAX_ARRAY_SIZE = 2147483639; AbstractStringBuilder() &#123; &#125; AbstractStringBuilder(int var1) &#123; this.value = new char[var1]; &#125; 性能每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 总结 操作少量的数据: 适用String 单线程操作字符串缓冲区下操作大量数据: 适用StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用StringBuffer","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"利用Python进行数据分析－Pandas入门","slug":"利用Python进行数据分析－Pandas入门","date":"2019-05-30T04:59:56.000Z","updated":"2019-07-08T01:15:52.729Z","comments":true,"path":"2019/05/30/利用Python进行数据分析－Pandas入门/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/30/利用Python进行数据分析－Pandas入门/","excerpt":"本文出自&lt;利用Python进行数据分析 第二版&gt; 侵删 PandasPandas是一个非常巨大的库，采用了大量Numpy的编程风格，基于Numpy构建，含有使数据清洗和数据分析工作变得更加简单的工具． Pandas的数据结构Pandas两个主要的数据结构：Series和DataFrame","text":"本文出自&lt;利用Python进行数据分析 第二版&gt; 侵删 PandasPandas是一个非常巨大的库，采用了大量Numpy的编程风格，基于Numpy构建，含有使数据清洗和数据分析工作变得更加简单的工具． Pandas的数据结构Pandas两个主要的数据结构：Series和DataFrame SeriesSeries是由一组数据以及一组索引组成，其数据类型包括各种NumPy数据类型．可用Series()函数来定义12345678In [62]: obj = pd.Series([1,2,3])In [63]: objOut[63]:0 11 22 3dtype: int64 在没有指定索引形式的情况下，Pandas自动为数据指定了(０－Ｎ-1)的索引结构，我们可用通过index和values属性来查看相关的信息12345In [64]: obj.valuesOut[64]: array([1, 2, 3])In [65]: obj.indexOut[65]: RangeIndex(start=0, stop=3, step=1) 除此之外，我们在创建Series的时候，还可以指定索引的形式，这一次和有序字典相似，所以，其实我们可以直接将一个字典使用Series函数转换成Series，同时我们还可以通过索引来访问其中的value,此处和NumPy的数据匹配方面是十分相似1234567891011121314In [67]: obj2 = pd.Series([90,80,60],index = [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;])In [68]: obj2Out[68]:a 90b 80c 60dtype: int64In [70]: obj2.index #此时访问index属性便会显示具体的索引值Out[70]: Index([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], dtype=&apos;object&apos;)In [69]: obj2[&apos;a&apos;]Out[69]: 90 与NumPy相比，Series有一个杰出的优点，Series可以通过索引来取一组数值，还可以借此来检索是否存在对应的数据，若有，则返回实际的value，若无，则返回NAN备注：Pandas的isnull和notnull函数可用于检测缺失数据，返回boolean值1234567891011In [74]: obj2[[&apos;c&apos;,&apos;a&apos;]]Out[74]:c 60a 90dtype: int64In [77]: obj2[[&apos;c&apos;,&apos;g&apos;]]Out[77]:c 60.0g NaNdtype: float64 同时，Series还可以使用NumPy的函数或者NumPy所拥有的一些运算123456789101112131415161718In [73]: obj2[obj2&gt;80]Out[73]:a 90dtype: int64In [75]: obj2*2Out[75]:a 180b 160c 120dtype: int64In [76]: np.exp(obj2)Out[76]:a 1.220403e+39b 5.540622e+34c 1.142007e+26dtype: float64 对于绝大多数的功能而言,Series最重要的一个跟你是,根据运算的索引标签来自动对齐数据 123456789101112131415161718192021222324In [35]: obj3Out[35]:Ohio 35000Oregon 16000Texas 71000Utah 5000dtype: int64In [36]: obj4Out[36]:California NaNOhio 35000.0Oregon 16000.0Texas 71000.0dtype: float64In [37]: obj3 + obj4Out[37]:California NaNOhio 70000.0Oregon 32000.0Texas 142000.0Utah NaNdtype: float64 Series对象本身及其索引都有一个name属性,该属性跟pandas其它的关键功能关系非常密切123456789101112In [38]: obj4.name = &apos;population&apos;In [39]: obj4.index.name = &apos;state&apos;In [40]: obj4Out[40]:stateCalifornia NaNOhio 35000.0Oregon 16000.0Texas 71000.0Name: population, dtype: float64 Series的索引可以通过赋值的方式就地修改：1234567891011121314151617In [41]: objOut[41]:0 41 72 -53 3dtype: int64In [42]: obj.index = [&apos;Bob&apos;, &apos;Steve&apos;, &apos;Jeff&apos;, &apos;Ryan&apos;]In [43]: objOut[43]:Bob 4Steve 7Jeff -5Ryan 3dtype: int64 DataFrameDataFrame是一个表格型的数据结构,以二维结构保存数组.构建DataFrame最常用的方法是闯入一个等长列表或者NumPy数组构成的字典,构建出来的DataFrame会被自动加上索引备注:head函数可以使得DataFrame只显示前五行,同时我们还可以像在Series一样,给DataFrame指定索引值12345678910In [84]: data = &#123;&apos;a&apos;:[1,2,3],&apos;b&apos;:[4,5,6],&apos;c&apos;:[7,8,9]&#125;In [85]: frame = pd.DataFrame(data)In [86]: frameOut[86]: a b c0 1 4 71 2 5 82 3 6 9 下面我们再来看一个典型的例子我们在构建的时候可以指定列序列,可以和数据源对不上,顺序以传入数据为准,但是,传入的index的长度必须要和数据源的列长度相等.1234567891011In [90]: frame2 = pd.DataFrame(data,columns=[&apos;a&apos;,&apos;c&apos;,&apos;d&apos;],index = [&apos;one&apos;,&apos;two&apos;,&apos;three&apos;])In [91]: frame2Out[91]: a c done 1 7 NaNtwo 2 8 NaNthree 3 9 NaNIn [92]: frame2.columnsOut[92]: Index([&apos;a&apos;, &apos;c&apos;, &apos;d&apos;], dtype=&apos;object&apos;) 可以通过列名来得到一个Series,有两种方法12345678910111213In [93]: frame2[&apos;a&apos;] #第一种Out[93]:one 1two 2three 3Name: a, dtype: int64In [95]: frame2.a #第二种Out[95]:one 1two 2three 3Name: a, dtype: int64 同理,可以通过行名来得到一个Series123456In [99]: frame2.loc[&apos;one&apos;]Out[99]:a 1c 7d NaNName: one, dtype: object","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://wt-git-repository.github.io/tags/Python/"}]},{"title":"利用Python进行数据分析－NumPy基础","slug":"利用Python进行数据分析－NumPy基础","date":"2019-05-30T04:59:21.000Z","updated":"2019-07-08T01:15:52.728Z","comments":true,"path":"2019/05/30/利用Python进行数据分析－NumPy基础/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/30/利用Python进行数据分析－NumPy基础/","excerpt":"本文出自＜利用Python进行数据分析 第２版＞ 侵删 NumPyNumPy 是Python数值计算最重要的基础包，可以高效处理大数组的数据． NumPy的ndarray：一种多维的数组对象ndarray是一个快速而又灵活的同构数据多维容器，是一个Ｎ维数组对象，其中所有的元素对象必须要是相同的数据类型，每一个对象包含一个元组和一个属性，分别是shape（一个表示各维度大小的元组）和dtype（一个说明数组数据类型的对象）","text":"本文出自＜利用Python进行数据分析 第２版＞ 侵删 NumPyNumPy 是Python数值计算最重要的基础包，可以高效处理大数组的数据． NumPy的ndarray：一种多维的数组对象ndarray是一个快速而又灵活的同构数据多维容器，是一个Ｎ维数组对象，其中所有的元素对象必须要是相同的数据类型，每一个对象包含一个元组和一个属性，分别是shape（一个表示各维度大小的元组）和dtype（一个说明数组数据类型的对象） 创建ndarray创建ndarray最简单的方法是使用array函数，它接受一切序列型的对象（包括其它数组），然后产生一个新的含有传入数据的NumPy数组123456In [1]: list1 = (1,2,3)In [3]: import numpy as npIn [5]: data = np.array(list1)In [6]: dataOut[6]: array([1, 2, 3]) 嵌套序列会被转换成一个多维的数组，array函数会为新建的数组推断出一个较为合适的数据类型，数据类型保存在一个特殊的dtype对象中1234567891011121314In [7]: list2 = [[1,2,3],[4,5,6]]In [8]: data2 = np.array(list2)In [9]: data2Out[9]:array([[1, 2, 3], [4, 5, 6]])In [10]: data2.shape #查看维度大小Out[10]: (2, 3)In [11]: data2.dtype #查看元素类型Out[11]: dtype(&apos;int64&apos;) 常用函数zeros：创建指定长度或者形状全为０的数组ones：创建指定长度或者形状全为１的数组empty：可以创建一个没有任何具体值的数组使用这些方法创建数组，只需要传入一个可以表示形状的元组便可1234567891011121314151617181920212223In [12]: np.zeros((10))Out[12]: array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])In [13]: np.ones((2,3))Out[13]:array([[ 1., 1., 1.], [ 1., 1., 1.]])In [14]: np.empty((2,3,4))Out[14]:array([[[ 6.94667955e-310, 4.65986064e-310, 6.94667972e-310, 6.94667971e-310], [ 6.94667972e-310, 6.94667971e-310, 6.94667852e-310, 6.94667972e-310], [ 6.94667852e-310, 6.94667852e-310, 3.55727265e-321, 5.53353523e-322]], [[ 0.00000000e+000, 6.94667867e-310, 6.94666603e-310, 6.94666605e-310], [ 6.94666603e-310, 6.94667969e-310, 6.94666603e-310, 6.94667725e-310], [ 6.94667974e-310, 6.94666603e-310, 6.94667974e-310, 6.94667971e-310]]]) arange是Python内置函数range的数组版12In [15]: np.arange(10)Out[15]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 备注：数据类型基本都是float64(浮点数) 创建时可以指定类型1234567In [21]: data2 = np.array([1,2,3],dtype=np.float64)In [22]: data2Out[22]: array([ 1., 2., 3.])In [23]: data2.dtypeOut[23]: dtype(&apos;float64&apos;) 创建后可以修改类型，astype函数12345In [24]: data2.astype(int)Out[24]: array([1, 2, 3])In [25]: data2.dtype #注意，astype只会返回转换后的类型，但并不会实际地去转换元素类型 Out[25]: dtype(&apos;float64&apos;) #NumPy 数组的运算数组不需要通过循环便可执行批量运算12345678910111213141516171819202122232425In [51]: arr = np.array([[1., 2., 3.], [4., 5., 6.]])In [52]: arrOut[52]:array([[ 1., 2., 3.], [ 4., 5., 6.]])In [53]: arr * arr #乘Out[53]:array([[ 1., 4., 9.], [ 16., 25., 36.]])In [54]: arr - arr #减Out[54]:array([[ 0., 0., 0.], [ 0., 0., 0.]])In [55]: 1 / arr #除Out[55]:array([[ 1. , 0.5 , 0.3333], [ 0.25 , 0.2 , 0.1667]])In [56]: arr ** 0.5 #指数Out[56]:array([[ 1. , 1.4142, 1.7321], [ 2. , 2.2361, 2.4495]]) 大小相同的数组可以产生布尔值数组大小不同的数组之间的运算叫做广播1234567891011In [57]: arr2 = np.array([[0., 4., 1.], [7., 2., 12.]])In [58]: arr2Out[58]:array([[ 0., 4., 1.], [ 7., 2., 12.]])In [59]: arr2 &gt; arrOut[59]:array([[False, True, False], [ True, False, True]], dtype=bool) 切片和索引NumPy产生的切片是视图，而并非是新的对象，当将一个标量值传给一个切片时，如arr[5:8]=12时，该值会自动传播到整个选区，这意味这在视图上，任意数据的修改都会影响到源数组1234567891011121314In [32]: data2 = np.array([1,2,3])In [33]: data3 = data2[1:2]In [34]: data3Out[34]: array([2])In [35]: data3[0] = 12 #通过索引去赋值，也可以通过索引去访问In [36]: data3Out[36]: array([12])In [37]: data2Out[37]: array([ 1, 12, 3]) 切片［：］会给所有值赋值，例如arr[:] = 13NumPy对数据的处理不包括复制粘贴的优势将体现在处理大规模的数据中 ，如果我们需要的是一份副本而不是一个视图的话，我们可以使用arr[5:8].copy()索引的返回值可为元素也可为数组123456789101112131415161718In [39]: arrOut[39]:array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])In [40]: arr[1]Out[40]:array([[ 7, 8, 9], [10, 11, 12]])In [41]: arr[1,1] #访问索引以（１，１）开头的那些值Out[41]: array([10, 11, 12])In [45]: arr[:1,1:,2:3] #切片的用法基本一致Out[45]: array([[[6]]]) 布尔型索引常用于数据的匹配123456789In [46]: names = np.array([&apos;wt&apos;,&apos;Bob&apos;])In [47]: scores = np.array([90,80])In [48]: names == &apos;wt&apos;Out[48]: array([ True, False], dtype=bool)In [49]: scores[names == &apos;wt&apos;]Out[49]: array([90]) 注意：布尔型数组的长度需要匹配对应数组的最高维度，在匹配的同事，我们还可以索引列，例如scores[name == ‘wt’,2:]，还可以通过这样来赋值，例如：scores[names == ‘wt’] = 0备注：～操作符可以用来反转条件，例如cond = names == ‘wt’，~cond就等同于names != ‘wt’还有一点值得注意的是：Python关键字and和or在布尔型数组中无效，要使用＆和｜． 花式索引数组装置转置是重塑的一种特殊形式，它返回的是源数据的视图，不会进行任何的复制操作，转置使用数组的Ｔ属性高维数组的装置需要使用transpose123456789101112131415array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]])In [11]: arr.TOut[11]:array([[ 0., 0., 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0., 0., 0.]]) 计算矩阵内积使用np.dot(arr1, arr2)来计算两个矩阵的内积 通用函数：快速的元素级数组函数通用函数是一种对ndarray中的数据执行元素级运算的函数1234In [15]: arr1 = np.array([2,2])In [16]: np.sqrt(arr1)Out[16]: array([ 1.41421356, 1.41421356]) 将条件逻辑表述为数组运算假设我们想根据cond中的值来选取xarr和yarr的值12345678910In [29]: xarr = np.array([1.1,1.2,1.3,1.4,1.5])In [30]: yarr = np.array([2.1,2.2,2.3,2.4,2.5])In [31]: cond = np.array([True,False,True,True,False])In [32]: result = [(x if c else y) for x,y,c in zip(xarr,yarr,cond)]In [33]: resultOut[33]: [1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5] 等同于1234In [34]: result = np.where(cond,xarr,yarr)In [35]: resultOut[35]: array([ 1.1, 2.2, 1.3, 1.4, 2.5]) np.where的第二个和第三个不必是数组，它们都可以是标量值．在数据分析的工作中，where的工作通常是根据一个数组产生另一个数组．假设一个由随机数组组成的矩阵，大于零的数都变成３，小于０的数都变成－３，若此时用利用where，则会变得非常方便12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364In [36]: arr = np.random.randn(6,6)In [37]: arrOut[37]:array([[-1.45536948, -0.01610929, -0.02849423, -0.82497092, 1.05006367, -0.20924655], [-0.4434815 , -0.33147041, -0.61486327, -0.5423556 , 1.512384 , -1.35921009], [-0.53875138, -0.25256538, 0.32190533, -0.20779243, 0.48525456, 0.97019284], [ 0.12193935, -0.26348046, 0.86740783, -0.32927907, 0.35186663, 2.24697225], [-0.49439342, 0.38880278, 0.52902035, 0.86600846, 1.31413569, 0.58566283], [ 0.34011322, 0.96141724, -1.00353822, -0.30896308, -1.03500063, -0.43719574]])In [38]: arr&gt;0Out[38]:array([[False, False, False, False, True, False], [False, False, False, False, True, False], [False, False, True, False, True, True], [ True, False, True, False, True, True], [False, True, True, True, True, True], [ True, True, False, False, False, False]], dtype=bool)In [39]: np.where(arr&gt;0,3,-3)Out[39]:array([[-3, -3, -3, -3, 3, -3], [-3, -3, -3, -3, 3, -3], [-3, -3, 3, -3, 3, 3], [ 3, -3, 3, -3, 3, 3], [-3, 3, 3, 3, 3, 3], [ 3, 3, -3, -3, -3, -3]])In [40]: np.where(arr&gt;0,3,arr) #把大于０的数赋值为３，其余赋值arrOut[40]:array([[-1.45536948, -0.01610929, -0.02849423, -0.82497092, 3. , -0.20924655], [-0.4434815 , -0.33147041, -0.61486327, -0.5423556 , 3. , -1.35921009], [-0.53875138, -0.25256538, 3. , -0.20779243, 3. , 3. ], [ 3. , -0.26348046, 3. , -0.32927907, 3. , 3. ], [-0.49439342, 3. , 3. , 3. , 3. , 3. ], [ 3. , 3. , -1.00353822, -0.30896308, -1.03500063, -0.43719574]])In [41]: np.where(arr&gt;0,arr,3) #把大于０的赋值为arr,其余赋值为３Out[41]:array([[ 3. , 3. , 3. , 3. , 1.05006367, 3. ], [ 3. , 3. , 3. , 3. , 1.512384 , 3. ], [ 3. , 3. , 0.32190533, 3. , 0.48525456, 0.97019284], [ 0.12193935, 3. , 0.86740783, 3. , 0.35186663, 2.24697225], [ 3. , 0.38880278, 0.52902035, 0.86600846, 1.31413569, 0.58566283], [ 0.34011322, 0.96141724, 3. , 3. , 3. , 3. ]]) 数学和统计方法 用于布尔型数组的方法1234In [47]: arr = np.array([-1,2,-3])In [48]: (arr &gt; 0).sum() #计算元素大于０的个数Out[48]: 1 另外还有两个方法any和all，它们对布尔型数组非常有用。any用于测试数组中是否存在一个或多个True，而all则检查数组中所有值是否都是True：1234567In [192]: bools = np.array([False, False, True, False])In [193]: bools.any()Out[193]: TrueIn [194]: bools.all()Out[194]: False 这两个方法也能用于非布尔型数组，所有非0元素将会被当做True。 排序123456789In [195]: arr = np.random.randn(6)In [196]: arrOut[196]: array([ 0.6095, -0.4938, 1.24 , -0.1357, 1.43 , -0.8469])In [197]: arr.sort()In [198]: arrOut[198]: array([-0.8469, -0.4938, -0.1357, 0.6095, 1.24 , 1.43 ]) 唯一化以及其它集合逻辑NumPy提供了一些针对ndarray的基本集合运算，最常用的是np.unique123456In [56]: arr = np.array([&apos;wt&apos;,&apos;jm&apos;,&apos;wt&apos;]) In [57]: np.unique(arr) #找到唯一值，并返回Out[57]:array([&apos;jm&apos;, &apos;wt&apos;], dtype=&apos;&lt;U2&apos;) 线性代数NumPy提供了一个用于矩阵乘法的dot函数（既是一个数组方法也是numpy命名空间中的一个函数） 伪随机数的生成numpy.random模块对Python内置的random进行了补充，增添一些可以高效生成多种概率发布的样本值的方法","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://wt-git-repository.github.io/tags/Python/"}]},{"title":"利用Python进行数据分析－数据结构准备（元组、列表、字典、集合、函数、推导式、柯里化、生成器、itertools模块以及文件读写）","slug":"利用Python进行数据分析－数据结构准备（元组、列表、字典、集合、函数、推导式、柯里化、生成器、itertools模块以及文件读写）","date":"2019-05-30T04:58:45.000Z","updated":"2019-07-08T01:15:52.730Z","comments":true,"path":"2019/05/30/利用Python进行数据分析－数据结构准备（元组、列表、字典、集合、函数、推导式、柯里化、生成器、itertools模块以及文件读写）/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/30/利用Python进行数据分析－数据结构准备（元组、列表、字典、集合、函数、推导式、柯里化、生成器、itertools模块以及文件读写）/","excerpt":"本文引用《利用Python进行数据分析·第2版》 元组tuple元组是一个固定长度而且不可以改变的序列对象 定义元组的方法:(1) 最简单的方法:12345In [23]: top = 1,2,3In [24]: topOut[24]: (1, 2, 3)","text":"本文引用《利用Python进行数据分析·第2版》 元组tuple元组是一个固定长度而且不可以改变的序列对象 定义元组的方法:(1) 最简单的方法:12345In [23]: top = 1,2,3In [24]: topOut[24]: (1, 2, 3) (2) 复杂元组的定义:123456In [25]: top = (1,2,3),(4,5)In [26]: topOut[26]: ((1, 2, 3), (4, 5)) 常用方法:tuple : 将任意序列或者迭代器转换成元组12345678In [27]: tuple([1,2,3])Out[27]: (1, 2, 3)In [28]: tuple(&quot;wt&quot;)Out[28]: (&apos;w&apos;, &apos;t&apos;) count：统计频率12345In [50]: a = 1,2,2,3In [51]: a.count(2)Out[51]: 2 访问 : 可以直接通过下标来访问123In [29]: top[0]Out[29]: (1, 2, 3) 修改:元组的对象一旦定义便不可以修改，除非其中存储的对象是可变对象，例如：1234567In [30]: top = 1, [1,2,3],&apos;a&apos;In [31]: top[1].append(&apos;wt&apos;)In [32]: topOut[32]: (1, [1, 2, 3, &apos;wt&apos;], &apos;a&apos;) 备注: [1,2,3] 为列表，有序的集合，可以随时添加和删除其中的元素 串联： 可以使用加法运算符把元组串联起来1234567891011In [32]: topOut[32]: (1, [1, 2, 3, &apos;wt&apos;], &apos;a&apos;)In [33]: top + (4,5,6) ＃第一种情况Out[33]: (1, [1, 2, 3, &apos;wt&apos;], &apos;a&apos;, 4, 5, 6)In [45]: top * 2 ＃第二种情况Out[45]: (1, [1, 2, 3, &apos;wt&apos;], &apos;a&apos;, 1, [1, 2, 3, &apos;wt&apos;], &apos;a&apos;) 拆分：拆分有两种情况，一种是等量拆分，另一种是不等量拆分等量拆分：1234567In [34]: top = 1,2,3In [35]: a,b,c = topIn [36]: aOut[36]: 1 不等量拆分，ｐ可以分配不等长度的列表12345In [42]: a,*p = topIn [43]: pOut[43]: [2, 3] 变量拆分常用来迭代元组或者列表序列12345678910111213In [46]: seq = (1,2,3),(4,5,6)In [49]: for a,b,c in seq: ...: print(&quot;a = &#123;0&#125;, b = &#123;1&#125;, c = &#123;2&#125;&quot;.format(a,b,c)) ...: a = 1, b = 2, c = 3a = 4, b = 5, c = 6* * * 列表list与元组相比，列表的长度和内容都是可变的 定义的方法：１．直接定义12345In [52]: a_list = [1,2,3,&apos;wt&apos;]In [53]: a_listOut[53]: [1, 2, 3, &apos;wt&apos;] ２．使用list函数，list函数在数据处理中常用来实体化迭代器和生成器12345In [58]: b_list = range(2,10)In [59]: list(b_list)Out[59]: [2, 3, 4, 5, 6, 7, 8, 9] 添加和删除元素：１．使用append在列表末尾添加元素12345In [60]: a_list.append(&apos;wt&apos;)In [61]: a_listOut[61]: [1, 2, 3, &apos;wt&apos;, &apos;wt&apos;] ２．使用insert在指定位置上添加元素1234567In [63]: b_list = list(b_list)In [64]: b_list.insert(1,&apos;wt&apos;)In [65]: b_listOut[65]: [2, &apos;wt&apos;, 3, 4, 5, 6, 7, 8, 9] ３．使用pop去除指定的元素1234567In [66]: b_list.pop(2)Out[66]: 3In [67]: b_listOut[67]: [2, &apos;wt&apos;, 4, 5, 6, 7, 8, 9] ４．使用remove去除某个匹配值，优先去除找到的第一个1234567In [69]: b_list = [&apos;wt&apos;,1,2,&apos;wt&apos;]In [70]: b_list.remove(&apos;wt&apos;)In [71]: b_listOut[71]: [1, 2, &apos;wt&apos;] 常用方法：１．使用in去检查列表里面是否含有某个值（not ni）123In [72]: &apos;wt&apos; in b_listOut[72]: True 2.串联和组合列表(1) 使用 + 号将两个列表串联起来123In [73]: [1,2,3] + [4,5,6]Out[73]: [1, 2, 3, 4, 5, 6] (2) 使用extend方法追加多个元素（添加的新对象为列表类型，与append不同的是，append添加的是单个元素）12345In [76]: b_list.extend([&apos;wt&apos;,&apos;wt&apos;])In [77]: b_listOut[77]: [1, 2, &apos;wt&apos;, &apos;wt&apos;, &apos;wt&apos;] 区别：使用 + 号串联对象的开销比较大，因为要新建一个列表，如果对象是一个大列表的时候，使用extend追加元素会比较可取 ３．使用sort()函数进行原地排序（不创建新的对象）1234567In [80]: a = [3,2,1]In [81]: a.sort()In [82]: aOut[82]: [1, 2, 3] ４．切片 切片的赋值：（注意要与Numpy的特点区分开来，Numpy切片下产生的是视图，而不是新的对象）123456789101112131415161718192021222324252627In [83]: a = [1,2,3,4,5,6,7,8,9]In [85]: a[2:3] = [&apos;wt&apos;,&apos;wt&apos;]In [86]: aOut[86]: [1, 2, &apos;wt&apos;, &apos;wt&apos;, 4, 5, 6, 7, 8, 9]In [87]: b = a[2:3] ＃产生一个新的对象In [88]: b = [1,1] ＃赋值In [89]: bOut[89]: [1, 1]In [90]: aOut[90]: [1, 2, &apos;wt&apos;, &apos;wt&apos;, 4, 5, 6, 7, 8, 9] ＃a的切片区域，数值没有被改变，说明产生的并非视图In [91]: a[::2] ＃隔两个数取一个值，顺序Out[91]: [1, &apos;wt&apos;, 4, 6, 8]In [92]: a[::-1] ＃隔一个数取一个值，倒叙Out[92]: [9, 8, 7, 6, 5, 4, &apos;wt&apos;, &apos;wt&apos;, 2, 1] 序列函数：enumerate函数，可以返回（i,value）元组序列1234567891011In [93]: for i,value in enumerate(a): ＃测试用例 ...: print(i,value) ...: 0 11 22 wt sorted函数从任意序列中返回一个已经拍好序的列表（列表的元素需要为同一类型）：123In [96]: sorted([&quot;dscdcsd fsdf&quot;]) #空格为分割符Out[96]: [&apos;dscdcsd fsdf&apos;] zip函数可以将多个列别，元组或者其它序列组合成一个新的列表12345678910111213141516171819202122232425262728293031323334353637In [98]: seq1 = [1,2,3]In [99]: seq2 = [&apos;wt&apos;,&apos;wt&apos;]In [100]: seq3 = zip(seq1,seq2)In [101]: seq3 #注意，如需展示列表，需要收到转换list()Out[101]:报错信息In [102]: list(seq3)Out[102]: [(1, &apos;wt&apos;), (2, &apos;wt&apos;)]In [104]: list(zip(seq1,seq2,seq4)) #zip可以合并任意长度的列别，元素的个数取决于最短的序列Out[104]: [(1, &apos;wt&apos;, 4), (2, &apos;wt&apos;, 5)]In [107]: for i,j in zip(*(zip(seq1,seq2,seq4))): ＃zip还可以用来解压，注意需要在被解压对象之前添加一个＊号 ...: print(i,j) ...: 1 2wt wt4 5#### reversed函数可以对一个列表进行倒叙处理In [109]: list(reversed([1,2,3]))Out[109]: [3, 2, 1]* * * 字典dict字典是Python重要的数据结构，被我们称之为哈希映射或者关联数组，键和值都是python对象, 其中，值可变但键不可变 创建方法：常用的创建方法是使用｛｝尖括号：12345In [110]: dict = &#123;&quot;wt&quot;:10,&quot;wt2&quot;:20&#125;In [111]: dictOut[111]: &#123;&apos;wt&apos;: 10, &apos;wt2&apos;: 20&#125; 访问、插入和设定：访问、插入和设定的方式可以像数组一样123456789In [2]: dict[&apos;wt3&apos;] = 30 #通过访问键的方式去插入In [3]: dictOut[3]: &#123;&apos;wt&apos;: 10, &apos;wt2&apos;: 20, &apos;wt3&apos;: 30&#125;In [5]: dict[&apos;wt2&apos;] #通过键去访问值valueOut[5]: 20 删除值（同时也会删除键），使用del或者pop方法1234567891011121314151617181920212223In [7]: dictOut[7]: &#123;&apos;wt&apos;: 10, &apos;wt2&apos;: 20, &apos;wt3&apos;: 30&#125;In [8]: del dict[&apos;wt3&apos;] #使用del方法去删除键为‘wt3’的值In [9]: dictOut[9]: &#123;&apos;wt&apos;: 10, &apos;wt2&apos;: 20&#125;In [9]: dictOut[9]: &#123;&apos;wt&apos;: 10, &apos;wt2&apos;: 20&#125;In [10]: value = dict.pop(&apos;wt2&apos;) #使用pop方法去删除键为‘wt2’的值，并把删除的值赋值给valueIn [11]: valueOut[11]: 20In [12]: dictOut[12]: &#123;&apos;wt&apos;: 10&#125; 常用方法：通过 in 去检查字典中是否含有指定的键123In [6]: &apos;wt2&apos; in dictOut[6]: True 访问字典中所有的keys123In [13]: dict.keys()Out[13]: dict_keys([&apos;wt&apos;]) 访问字典中所有的values123In [14]: dict.values()Out[14]: dict_values([10]) 使用update融合字典12345In [16]: dict.update(&#123;&quot;wt2&quot;:20,&quot;wt3&quot;:30&#125;) #使用update合并字典In [17]: dictOut[17]: &#123;&apos;wt&apos;: 10, &apos;wt2&apos;: 20, &apos;wt3&apos;: 30&#125; 将两个序列组合成字典123456789101112131415161718192021In [18]: key_list = [1,2,3]In [19]: value_list = [4,5,6]In [20]: mapping = &#123;&#125;#组合的第一种方法In [21]: for key,value in zip(key_list,value_list): ...: mapping[key] = value ...: In [22]: mappingOut[22]: &#123;1: 4, 2: 5, 3: 6&#125;#组合的第二种方法In [23]: list = list(zip(key_list,value_list))In [24]: listOut[24]: [(1, 4), (2, 5), (3, 6)] #此为二元列表，可以使用dict方法来转换成字典In [31]: mapping2 = dict(list）In [32]: mapping2Out[32]: &#123;1: 4, 2: 5, 3: 6&#125; 默认值dict的方法get和pop可以取默认值，并返回123456#此种代码逻辑十分常见In [33]: if key in some_dict: ...: value = some_dict[key] ...: else: ...: value = default_value ...: 常用的方法还有collections, defaultdict, setdefault 集合set集合是无序而又没有重复元素的集合，可以看作只有键而没有值的字典 定义方法:直接使用{}尖括号1234In [34]: mySet = &#123;1,2,3&#125;In [35]: mySetOut[35]: &#123;1, 2, 3&#125; 使用方法set1234In [36]: mySet = set([4,5,6])In [37]: mySetOut[37]: &#123;4, 5, 6&#125; 常用方法：合并，取两个集合不重复的元素，然后组合成一个新的集合，可以用union方法或者 | 运算符123456789In [38]: set1 = &#123;1,2,3&#125;In [39]: set2 = &#123;2,3,4&#125;In [40]: set1.union(set2) #第一种方法，使用unionOut[40]: &#123;1, 2, 3, 4&#125;In [41]: set1 | set2 #第二种方法，使用 | 运算符Out[41]: &#123;1, 2, 3, 4&#125; 交集，取两个集合中重复的集合，然后组合成一个新的集合，可以用intersection 或者 &amp; 运算符12345In [42]: set1.intersection(set2) #第一种方法，使用intersectionOut[42]: &#123;2, 3&#125;In [43]: set1 &amp; set2 # 第二种方法，使用 &amp; 运算符Out[43]: &#123;2, 3&#125; 所有逻辑集合操作都有另外的原地实现方法，可以直接用结果替代集合的内容。对于大的集合，这么做效率更高。 列表，集合，字典推导式列表推导式：1[expr for val in collection if condition] 集合推导式1set_comp = &#123;expr for value in collection if condition&#125; 字典推导式1dict_comp = &#123;key-expr : value-expr for value in collection if condition&#125; 函数函数的应用十分广泛，其中，值得注意的是，函数的返回值的形式可以多种多样，例如，返回字典12345def f(): a = 5 b = 6 c = 7 return &#123;&apos;a&apos;:a,&apos;b&apos;:b,&apos;c&apos;:c&#125; 有时候，我们需要对同一个字符串做一系列的函数操作，此时，最简单的方法是，我们把所有的方法封装成一个列表，然后再遍历执行（多函数模式）123456789101112def remove_punctuation(value): return re.sub(&apos;[!#?]&apos;, &apos;&apos;, value)clean_ops = [str.strip, remove_punctuation, str.title] #一系列的函数名def clean_strings(strings, ops): result = [] for value in strings: for function in ops: #遍历调用列表中的函数 value = function(value) result.append(value) return result 等同于12345678def clean_strings(strings): result = [] for value in strings: value = value.strip() value = re.sub(&apos;[!#?]&apos;, &apos;&apos;, value) value = value.title() result.append(value) return result 我们还可以将函数作为另一个函数的参数，例如内置的map函数，它的作用是在一组数据上应用一个函数：123456789101112In [10]: def change(value): return str(str(value) + &quot;wt&quot;) ....:In [7]: list = [1,2,3]In [11]: for i in map(change,list): #将list的元素经过change()处理之后，再赋值给 i print(i) ....: 1wt2wt3wt 同时，我们还可以使用lambda匿名函数，这种用法要比完整的函数声明和定义要简洁得多123456In [3]: def double(list,f): return [f(x) for x in list] ...:In [4]: print(double(my_list,lambda x:x*2))[2, 4, 6] 柯里化，部分参数的应用柯里化是一个计算机科学术语，它的意思就是部分参数应用，由现有的函数组合成一个新的函数12345678In [5]: def add(x,y): ...: return x+y ...:In [6]: add_one = lambda x: add(x,1) #派生出一个新函数In [7]: print(add_one(2))3 等同于12345In [9]: from functools import partialIn [12]: add_one = partial(add,y=1)In [13]: print(add_one(2))3 生成器表达式1In [189]: gen = (x ** 2 for x in range(100)) itertools模块 读写文件读写文件一般使用with语句，不仅方便读取数据，而且在退出代码块之后会自动清理文件12345In [212]: with open(path , &quot;r&quot;) as f: #读文件 .....: lines = [x.rstrip() for x in f]In [225]: with open(&apos;path&apos;.txt&apos;, &apos;w&apos;) as handle: #写文件 .....: handle.writelines(x for x in open(path) if len(x) &gt; 1)","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://wt-git-repository.github.io/tags/Python/"}]},{"title":"SpringCloud微服务架构实战读书笔记①","slug":"SpringCloud微服务架构实战读书笔记①","date":"2019-05-29T07:27:56.000Z","updated":"2019-07-08T01:15:52.693Z","comments":true,"path":"2019/05/29/SpringCloud微服务架构实战读书笔记①/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/29/SpringCloud微服务架构实战读书笔记①/","excerpt":"微服务架构概述微服务设计原则 单一设计原则： 单一职责原则指的是一个单元（类、方法或者是服务）关注的应该只是整个系统中单独且有界限的一部分。 服务自治原则： 服务自治指的是每个微服务都应具备独立的业务能力、依赖与运行环境， 服务与服务之间需要高度解耦， 每个服务从开发、测试、构建到部署都应该可以独立的运行， 而不需要依赖其它服务。 轻量级通信机制： 微服务之间应该通过轻量级的通信机制进行交互。 微服务粒度： 根据合理的粒度划分微服务， 而并非越小越好。","text":"微服务架构概述微服务设计原则 单一设计原则： 单一职责原则指的是一个单元（类、方法或者是服务）关注的应该只是整个系统中单独且有界限的一部分。 服务自治原则： 服务自治指的是每个微服务都应具备独立的业务能力、依赖与运行环境， 服务与服务之间需要高度解耦， 每个服务从开发、测试、构建到部署都应该可以独立的运行， 而不需要依赖其它服务。 轻量级通信机制： 微服务之间应该通过轻量级的通信机制进行交互。 微服务粒度： 根据合理的粒度划分微服务， 而并非越小越好。 Spring Cloud 实战服务提供者与服务消费者服务提供者定义： 服务的被调用方， 即为其它服务提供服务的服务 服务消费者定义： 服务的消费方， 即依赖其它服务的服务 Spring Boot Actuator提供了一些监控的端点 硬编码的问题硬编码指的是把网络地址嵌套在代码中， 如1@GetMapping(\"/user/&#123;id&#125;\") 在不使用注册中心的前提下， 最好将路径提取出来， 放在配置文件中，使用分布式配置中心统一去管理。 服务注册与发现 各个微服务在启动时，将自己的网络地址等信息注册到服务发现组件中，服务发现组件会存储这些信息。 服务消费者可以从服务发现组件查询服务提供者的网络地址，并使用该地址调用服务提供者的接口。 各个微服务与服务发现组件使用一定机制（如心跳）进行通信。服务发现组件若长时间服务与某服务实例通信，就会注销该实例。 微服务网络地址发生变更， 会重新注册到服务发现组件。 综上所述， 服务发现组件应该具备如下功能： 服务注册表： 服务发现组件的核心，它用来记录各个微服务的信息，例如微服务的名称、IP、端口等等，该表提供查询API和管理API， 查询API用于查询可用的微服务实例， 管理API用于管理服务的注册与注销。 服务注册与服务发现： 服务注册是指在微服务启动时，将自己的信息注册到服务发现组件上的过程。服务发现是指查询可用微服务列表及网络地址的机制。 服务检查： 服务发现组件使用一定机制定时检测已注册的服务。 EurekaRegion and Avaliability Zone Eureka 工作原理 MakeRemoteCall 是RESTFul API的调用行为 us-east-1c、us-east-1d等都是Avaliability Zone, 他们都属于us-east-1这个region Eureka 包含两个组件：一个是Eureka server， 另一个是Eureka client， 在Eureka集群的情况下， Eureka Server也是Eureka Client， 多个Eureka Server之间通过相互复制的方式来实现服务注册表中的数据同步。 几个比较重要的配置： eureka.client.registerWithEureka：表示是否将自己注册到EurekaServer eureka.client.fetchRegistry：表示是否从Eureka Server获取注册信息，默认为true。 eureka.client.serviceUrl.defaultZone：设置与EurekaServer交互的地址，查询服务和注册服务都需要依赖这个地址。默认是http://localhost:8761/eureka；多个地址间可使用,分隔。 将微服务注册到Eureka Server中几个比较重要的配置 Spring.application.name: 用于注册到Eureka Server上的应用名称 eureka.instance.prefer-ip-address=true 表示将自己的IP注册到Eureka Server中， 若为false， 一般会将操作系统的hostname注册到Eureka Server中。 为Eureka Server添加安全认证的支持， 可以使用Spring Security 将eureka.client.serviceUrl.defaultZone配置为http://user:password@EUREKA_HOST:EUREKA_PORT/eureka/的形式, 即可成功注册。 Eureka 元数据Eureka 的元数据包括两种，分别是标准元数据和自定义元数据。 标准元数据指的是主机名、IP地址、端口号、状态页和健康检查等信息，这些信息都会被发布在服务注册表中，用于服务之间的调用。 自定义元数据， 可以使用eureka.instance.metadata-map配置，这些元数据可以被远程客户端访问， 通过提前的约定使得远程客户端知道该如何使用 Eureka Server的REST端点 Eureka 的自我保护模式 默认情况下， Eureka Server心跳检测的默认限度是90s, 超过之后没有收到回复将会注销该服务 但在极端情况下， 如网络故障， 则会出现短时间内大规模注销的情况， 这时Eureka server将会启动自我保护模式， 在该模式下， 不会注销任何服务， 当网络恢复时， Eureka server会自动退出自我保护模式。 eureka.server.enable-self-preservation=false禁用自我保护模式 多网卡环境下的IP选择建议直接使用eureka.instance.ipAddress手动指定IP地址， 特别是在Docker容器中 Eureka 健康检查我们上面所说的心跳之类的，都只是Eureka与服务之间的通信是否正常，而通信正常不代表服务正常， 如服务不能与数据库进行连接， 此时便需要更加细化的检查了。 使用Ribbon实现客户端负载均衡在Spring Cloud中，当Ribbon与Eureka配合使用时，Ribbon可自动从Eureka Server获取服务提供者的地址列表，并基于负载均衡算法，请求其中一个服务提供者的实例。为RestTemplate添加@LoadBalanced注解 12345@Bean@LoadBalancedpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125; 相关注意事项： 不能将restTemplate.getForObject与loadBalancedClient.choose写在同一个方法中，因为前者已经有选择的意思，前者包括后者，restTemplate实际上已经是Ribbon客户端本身，已经包含Choose行为 虚拟主机名不能包括 _ 字符 Ribbon配置自定义(翻下官方文档自己配)脱离Eureka使用Ribbon有一些遗留的微服务，它们没有注册到Eureka Server上， 这个时候， 仍然可以使用Ribbon实现负载均衡 配置：microservice-provider-user.ribbon.listOfServers 饥饿加载Spring cloud 为每个名称的Ribbon Client维护一个子应用上下文，这个上下文默认的懒加载的，指定名称的Ribbon Client第一次请求的时候，对应的上下文才会被加载，因此第一次加载时，会比较慢，此时，我们可以配置懒加载这样，对于名为client1、client2的Ribbon Client，将在启动时加载对应的子应用程序上下文。","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"ArrayList源码学习","slug":"ArrayList源码学习","date":"2019-05-29T01:40:06.000Z","updated":"2019-07-08T01:15:52.669Z","comments":true,"path":"2019/05/29/ArrayList源码学习/","link":"","permalink":"https://wt-git-repository.github.io/2019/05/29/ArrayList源码学习/","excerpt":"ArrayList简介1public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, Serializable ArrayList 底层是数组队列， 属于动态数组， 可以根据实际的需要去动态地调整数组的大小 ArrayList 继承了AbstractList类，说明它提供了增删改查等功能 ArrayList 实现了RandomAccess接口， 说明实现这个接口的List集合是支持随机访问的 ArrayList 实现了Cloneable接口， 说明它是可克隆的 ArrayList 实现了Serialiable接口， 说明它是可以被序列化的","text":"ArrayList简介1public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, Serializable ArrayList 底层是数组队列， 属于动态数组， 可以根据实际的需要去动态地调整数组的大小 ArrayList 继承了AbstractList类，说明它提供了增删改查等功能 ArrayList 实现了RandomAccess接口， 说明实现这个接口的List集合是支持随机访问的 ArrayList 实现了Cloneable接口， 说明它是可克隆的 ArrayList 实现了Serialiable接口， 说明它是可以被序列化的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, Serializable &#123; private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始化大小， 为10 */ private static final int DEFAULT_CAPACITY = 10; /** * empty_element_data 默认空数组，对应下面var1==0时的构造方法 */ private static final Object[] EMPTY_ELEMENTDATA = new Object[0]; /** * */ private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = new Object[0]; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; /** * 数组所包含的数据的个数，值得注意的是，此处并不是指数组的大小 */ private int size; /** * 数组容量的最大值 */ private static final int MAX_ARRAY_SIZE = 2147483639; /** * 带参构造函数，自定义容量 */ public ArrayList(int var1) &#123; if (var1 &gt; 0) &#123; this.elementData = new Object[var1]; &#125; else &#123; if (var1 != 0) &#123; throw new IllegalArgumentException(\"Illegal Capacity: \" + var1); &#125; this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; /** * 默认空数组，， */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; /** * 构造一个包含指定集合的数组 */ public ArrayList(Collection&lt;? extends E&gt; var1) &#123; this.elementData = var1.toArray(); if ((this.size = this.elementData.length) != 0) &#123; // 判断是否为Objectp[]类型 if (this.elementData.getClass() != Object[].class) &#123; this.elementData = Arrays.copyOf(this.elementData, this.size, Object[].class); &#125; &#125; else &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; /** * 修改当前数组的大小，调整为与当前的数据大小保持一致 */ public void trimToSize() &#123; ++this.modCount; if (this.size &lt; this.elementData.length) &#123; this.elementData = this.size == 0 ? EMPTY_ELEMENTDATA : Arrays.copyOf(this.elementData, this.size); &#125; &#125; /** * ArrayList的扩容机制， 算法为int var3 = var2 + (var2 &gt;&gt; 1);每次扩容约为原来的1.5倍 */ public void ensureCapacity(int var1) &#123; int var2 = this.elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA ? 0 : 10; if (var1 &gt; var2) &#123; this.ensureExplicitCapacity(var1); &#125; &#125; /** * 计算最小容量 */ private static int calculateCapacity(Object[] var0, int var1) &#123; return var0 == DEFAULTCAPACITY_EMPTY_ELEMENTDATA ? Math.max(10, var1) : var1; &#125; /** * 判断是否需要扩容 */ private void ensureCapacityInternal(int var1) &#123; this.ensureExplicitCapacity(calculateCapacity(this.elementData, var1)); &#125; private void ensureExplicitCapacity(int var1) &#123; ++this.modCount; if (var1 - this.elementData.length &gt; 0) &#123; this.grow(var1); &#125; &#125; /** * 扩容的核心算法 */ private void grow(int var1) &#123; int var2 = this.elementData.length; int var3 = var2 + (var2 &gt;&gt; 1); if (var3 - var1 &lt; 0) &#123; var3 = var1; &#125; if (var3 - 2147483639 &gt; 0) &#123; var3 = hugeCapacity(var1); &#125; this.elementData = Arrays.copyOf(this.elementData, var3); &#125; private static int hugeCapacity(int var0) &#123; if (var0 &lt; 0) &#123; throw new OutOfMemoryError(); &#125; else &#123; return var0 &gt; 2147483639 ? 2147483647 : 2147483639; &#125; &#125; public int size() &#123; return this.size; &#125; public boolean isEmpty() &#123; return this.size == 0; &#125; public boolean contains(Object var1) &#123; return this.indexOf(var1) &gt;= 0; &#125; /** * 这里分了两张情况，空或非空 */ public int indexOf(Object var1) &#123; int var2; if (var1 == null) &#123; for(var2 = 0; var2 &lt; this.size; ++var2) &#123; if (this.elementData[var2] == null) &#123; return var2; &#125; &#125; &#125; else &#123; for(var2 = 0; var2 &lt; this.size; ++var2) &#123; if (var1.equals(this.elementData[var2])) &#123; return var2; &#125; &#125; &#125; return -1; &#125; public int lastIndexOf(Object var1) &#123; int var2; if (var1 == null) &#123; for(var2 = this.size - 1; var2 &gt;= 0; --var2) &#123; if (this.elementData[var2] == null) &#123; return var2; &#125; &#125; &#125; else &#123; for(var2 = this.size - 1; var2 &gt;= 0; --var2) &#123; if (var1.equals(this.elementData[var2])) &#123; return var2; &#125; &#125; &#125; return -1; &#125; /** * 浅拷贝？？？ */ public Object clone() &#123; try &#123; ArrayList var1 = (ArrayList)super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度 var1.elementData = Arrays.copyOf(this.elementData, this.size); var1.modCount = 0; return var1; &#125; catch (CloneNotSupportedException var2) &#123; throw new InternalError(var2); &#125; &#125; /** * 此处返回的数组是安全的，因为ArrayList不会保留对它的引用 */ public Object[] toArray() &#123; return Arrays.copyOf(this.elementData, this.size); &#125; public &lt;T&gt; T[] toArray(T[] var1) &#123; if (var1.length &lt; this.size) &#123; return (Object[])Arrays.copyOf(this.elementData, this.size, var1.getClass()); &#125; else &#123; //调用System提供的arraycopy()方法实现数组之间的复制 // var1是目的数组 System.arraycopy(this.elementData, 0, var1, 0, this.size); if (var1.length &gt; this.size) &#123; var1[this.size] = null; &#125; return var1; &#125; &#125; E elementData(int var1) &#123; return this.elementData[var1]; &#125; public E get(int var1) &#123; this.rangeCheck(var1); return this.elementData(var1); &#125; public E set(int var1, E var2) &#123; this.rangeCheck(var1); Object var3 = this.elementData(var1); this.elementData[var1] = var2; return var3; &#125; public boolean add(E var1) &#123; this.ensureCapacityInternal(this.size + 1); this.elementData[this.size++] = var1; return true; &#125; public void add(int var1, E var2) &#123; this.rangeCheckForAdd(var1); this.ensureCapacityInternal(this.size + 1); System.arraycopy(this.elementData, var1, this.elementData, var1 + 1, this.size - var1); this.elementData[var1] = var2; ++this.size; &#125; /** * 通过数组间的复制， 使用System.arraycopy去删除指定索引的元素 */ public E remove(int var1) &#123; this.rangeCheck(var1); ++this.modCount; Object var2 = this.elementData(var1); int var3 = this.size - var1 - 1; if (var3 &gt; 0) &#123; System.arraycopy(this.elementData, var1 + 1, this.elementData, var1, var3); &#125; this.elementData[--this.size] = null; return var2; &#125; public boolean remove(Object var1) &#123; int var2; if (var1 == null) &#123; for(var2 = 0; var2 &lt; this.size; ++var2) &#123; if (this.elementData[var2] == null) &#123; this.fastRemove(var2); return true; &#125; &#125; &#125; else &#123; for(var2 = 0; var2 &lt; this.size; ++var2) &#123; if (var1.equals(this.elementData[var2])) &#123; this.fastRemove(var2); return true; &#125; &#125; &#125; return false; &#125; /* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int var1) &#123; ++this.modCount; int var2 = this.size - var1 - 1; if (var2 &gt; 0) &#123; System.arraycopy(this.elementData, var1 + 1, this.elementData, var1, var2); &#125; this.elementData[--this.size] = null; &#125; /* * 删除所有元素 */ public void clear() &#123; ++this.modCount; for(int var1 = 0; var1 &lt; this.size; ++var1) &#123; this.elementData[var1] = null; &#125; this.size = 0; &#125; /** * 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾。 */ public boolean addAll(Collection&lt;? extends E&gt; var1) &#123; Object[] var2 = var1.toArray(); int var3 = var2.length; this.ensureCapacityInternal(this.size + var3); System.arraycopy(var2, 0, this.elementData, this.size, var3); this.size += var3; return var3 != 0; &#125; /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int var1, Collection&lt;? extends E&gt; var2) &#123; this.rangeCheckForAdd(var1); Object[] var3 = var2.toArray(); int var4 = var3.length; this.ensureCapacityInternal(this.size + var4); int var5 = this.size - var1; if (var5 &gt; 0) &#123; System.arraycopy(this.elementData, var1, this.elementData, var1 + var4, var5); &#125; System.arraycopy(var3, 0, this.elementData, var1, var4); this.size += var4; return var4 != 0; &#125; protected void removeRange(int var1, int var2) &#123; ++this.modCount; int var3 = this.size - var2; System.arraycopy(this.elementData, var2, this.elementData, var1, var3); int var4 = this.size - (var2 - var1); for(int var5 = var4; var5 &lt; this.size; ++var5) &#123; this.elementData[var5] = null; &#125; this.size = var4; &#125; /* * 检查索引是否越界 */ private void rangeCheck(int var1) &#123; if (var1 &gt;= this.size) &#123; throw new IndexOutOfBoundsException(this.outOfBoundsMsg(var1)); &#125; &#125; private void rangeCheckForAdd(int var1) &#123; if (var1 &gt; this.size || var1 &lt; 0) &#123; throw new IndexOutOfBoundsException(this.outOfBoundsMsg(var1)); &#125; &#125; /** * 返回越界的具体信息 */ private String outOfBoundsMsg(int var1) &#123; return \"Index: \" + var1 + \", Size: \" + this.size; &#125; /** * 删除集合内指定元素 */ public boolean removeAll(Collection&lt;?&gt; var1) &#123; Objects.requireNonNull(var1); return this.batchRemove(var1, false); &#125; public boolean retainAll(Collection&lt;?&gt; var1) &#123; Objects.requireNonNull(var1); return this.batchRemove(var1, true); &#125; private boolean batchRemove(Collection&lt;?&gt; var1, boolean var2) &#123; Object[] var3 = this.elementData; int var4 = 0; int var5 = 0; boolean var6 = false; while(true) &#123; boolean var11 = false; try &#123; var11 = true; if (var4 &gt;= this.size) &#123; var11 = false; break; &#125; if (var1.contains(var3[var4]) == var2) &#123; var3[var5++] = var3[var4]; &#125; ++var4; &#125; finally &#123; if (var11) &#123; if (var4 != this.size) &#123; System.arraycopy(var3, var4, var3, var5, this.size - var4); var5 += this.size - var4; &#125; if (var5 != this.size) &#123; for(int var9 = var5; var9 &lt; this.size; ++var9) &#123; var3[var9] = null; &#125; this.modCount += this.size - var5; this.size = var5; var6 = true; &#125; &#125; &#125; &#125; if (var4 != this.size) &#123; System.arraycopy(var3, var4, var3, var5, this.size - var4); var5 += this.size - var4; &#125; if (var5 != this.size) &#123; for(int var7 = var5; var7 &lt; this.size; ++var7) &#123; var3[var7] = null; &#125; this.modCount += this.size - var5; this.size = var5; var6 = true; &#125; return var6; &#125; private void writeObject(ObjectOutputStream var1) throws IOException &#123; int var2 = this.modCount; var1.defaultWriteObject(); var1.writeInt(this.size); for(int var3 = 0; var3 &lt; this.size; ++var3) &#123; var1.writeObject(this.elementData[var3]); &#125; if (this.modCount != var2) &#123; throw new ConcurrentModificationException(); &#125; &#125; private void readObject(ObjectInputStream var1) throws IOException, ClassNotFoundException &#123; this.elementData = EMPTY_ELEMENTDATA; var1.defaultReadObject(); var1.readInt(); if (this.size &gt; 0) &#123; int var2 = calculateCapacity(this.elementData, this.size); SharedSecrets.getJavaOISAccess().checkArray(var1, Object[].class, var2); this.ensureCapacityInternal(this.size); Object[] var3 = this.elementData; for(int var4 = 0; var4 &lt; this.size; ++var4) &#123; var3[var4] = var1.readObject(); &#125; &#125; &#125; /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator(int var1) &#123; if (var1 &gt;= 0 &amp;&amp; var1 &lt;= this.size) &#123; return new ArrayList.ListItr(var1); &#125; else &#123; throw new IndexOutOfBoundsException(\"Index: \" + var1); &#125; &#125; /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator() &#123; return new ArrayList.ListItr(0); &#125; /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator&lt;E&gt; iterator() &#123; return new ArrayList.Itr(); &#125; public List&lt;E&gt; subList(int var1, int var2) &#123; subListRangeCheck(var1, var2, this.size); return new ArrayList.SubList(this, 0, var1, var2); &#125; static void subListRangeCheck(int var0, int var1, int var2) &#123; if (var0 &lt; 0) &#123; throw new IndexOutOfBoundsException(\"fromIndex = \" + var0); &#125; else if (var1 &gt; var2) &#123; throw new IndexOutOfBoundsException(\"toIndex = \" + var1); &#125; else if (var0 &gt; var1) &#123; throw new IllegalArgumentException(\"fromIndex(\" + var0 + \") &gt; toIndex(\" + var1 + \")\"); &#125; &#125; public void forEach(Consumer&lt;? super E&gt; var1) &#123; Objects.requireNonNull(var1); int var2 = this.modCount; Object[] var3 = (Object[])this.elementData; int var4 = this.size; for(int var5 = 0; this.modCount == var2 &amp;&amp; var5 &lt; var4; ++var5) &#123; var1.accept(var3[var5]); &#125; if (this.modCount != var2) &#123; throw new ConcurrentModificationException(); &#125; &#125; public Spliterator&lt;E&gt; spliterator() &#123; return new ArrayList.ArrayListSpliterator(this, 0, -1, 0); &#125; public boolean removeIf(Predicate&lt;? super E&gt; var1) &#123; Objects.requireNonNull(var1); int var2 = 0; BitSet var3 = new BitSet(this.size); int var4 = this.modCount; int var5 = this.size; for(int var6 = 0; this.modCount == var4 &amp;&amp; var6 &lt; var5; ++var6) &#123; Object var7 = this.elementData[var6]; if (var1.test(var7)) &#123; var3.set(var6); ++var2; &#125; &#125; if (this.modCount != var4) &#123; throw new ConcurrentModificationException(); &#125; else &#123; boolean var10 = var2 &gt; 0; if (var10) &#123; int var11 = var5 - var2; int var8 = 0; for(int var9 = 0; var8 &lt; var5 &amp;&amp; var9 &lt; var11; ++var9) &#123; var8 = var3.nextClearBit(var8); this.elementData[var9] = this.elementData[var8]; ++var8; &#125; for(var8 = var11; var8 &lt; var5; ++var8) &#123; this.elementData[var8] = null; &#125; this.size = var11; if (this.modCount != var4) &#123; throw new ConcurrentModificationException(); &#125; ++this.modCount; &#125; return var10; &#125; &#125; public void replaceAll(UnaryOperator&lt;E&gt; var1) &#123; Objects.requireNonNull(var1); int var2 = this.modCount; int var3 = this.size; for(int var4 = 0; this.modCount == var2 &amp;&amp; var4 &lt; var3; ++var4) &#123; this.elementData[var4] = var1.apply(this.elementData[var4]); &#125; if (this.modCount != var2) &#123; throw new ConcurrentModificationException(); &#125; else &#123; ++this.modCount; &#125; &#125; public void sort(Comparator&lt;? super E&gt; var1) &#123; int var2 = this.modCount; Arrays.sort((Object[])this.elementData, 0, this.size, var1); if (this.modCount != var2) &#123; throw new ConcurrentModificationException(); &#125; else &#123; ++this.modCount; &#125; &#125; static final class ArrayListSpliterator&lt;E&gt; implements Spliterator&lt;E&gt; &#123; private final ArrayList&lt;E&gt; list; private int index; private int fence; private int expectedModCount; ArrayListSpliterator(ArrayList&lt;E&gt; var1, int var2, int var3, int var4) &#123; this.list = var1; this.index = var2; this.fence = var3; this.expectedModCount = var4; &#125; private int getFence() &#123; int var1; if ((var1 = this.fence) &lt; 0) &#123; ArrayList var2; if ((var2 = this.list) == null) &#123; var1 = this.fence = 0; &#125; else &#123; this.expectedModCount = var2.modCount; var1 = this.fence = var2.size; &#125; &#125; return var1; &#125; public ArrayList.ArrayListSpliterator&lt;E&gt; trySplit() &#123; int var1 = this.getFence(); int var2 = this.index; int var3 = var2 + var1 &gt;&gt;&gt; 1; return var2 &gt;= var3 ? null : new ArrayList.ArrayListSpliterator(this.list, var2, this.index = var3, this.expectedModCount); &#125; public boolean tryAdvance(Consumer&lt;? super E&gt; var1) &#123; if (var1 == null) &#123; throw new NullPointerException(); &#125; else &#123; int var2 = this.getFence(); int var3 = this.index; if (var3 &lt; var2) &#123; this.index = var3 + 1; Object var4 = this.list.elementData[var3]; var1.accept(var4); if (this.list.modCount != this.expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; else &#123; return true; &#125; &#125; else &#123; return false; &#125; &#125; &#125; public void forEachRemaining(Consumer&lt;? super E&gt; var1) &#123; if (var1 == null) &#123; throw new NullPointerException(); &#125; else &#123; ArrayList var5; Object[] var6; if ((var5 = this.list) != null &amp;&amp; (var6 = var5.elementData) != null) &#123; int var3; int var4; if ((var3 = this.fence) &lt; 0) &#123; var4 = var5.modCount; var3 = var5.size; &#125; else &#123; var4 = this.expectedModCount; &#125; int var2; if ((var2 = this.index) &gt;= 0 &amp;&amp; (this.index = var3) &lt;= var6.length) &#123; while(var2 &lt; var3) &#123; Object var7 = var6[var2]; var1.accept(var7); ++var2; &#125; if (var5.modCount == var4) &#123; return; &#125; &#125; &#125; throw new ConcurrentModificationException(); &#125; &#125; public long estimateSize() &#123; return (long)(this.getFence() - this.index); &#125; public int characteristics() &#123; return 16464; &#125; &#125; private class SubList extends AbstractList&lt;E&gt; implements RandomAccess &#123; private final AbstractList&lt;E&gt; parent; private final int parentOffset; private final int offset; int size; SubList(AbstractList&lt;E&gt; var2, int var3, int var4, int var5) &#123; this.parent = var2; this.parentOffset = var4; this.offset = var3 + var4; this.size = var5 - var4; this.modCount = ArrayList.this.modCount; &#125; public E set(int var1, E var2) &#123; this.rangeCheck(var1); this.checkForComodification(); Object var3 = ArrayList.this.elementData(this.offset + var1); ArrayList.this.elementData[this.offset + var1] = var2; return var3; &#125; public E get(int var1) &#123; this.rangeCheck(var1); this.checkForComodification(); return ArrayList.this.elementData(this.offset + var1); &#125; public int size() &#123; this.checkForComodification(); return this.size; &#125; public void add(int var1, E var2) &#123; this.rangeCheckForAdd(var1); this.checkForComodification(); this.parent.add(this.parentOffset + var1, var2); this.modCount = this.parent.modCount; ++this.size; &#125; public E remove(int var1) &#123; this.rangeCheck(var1); this.checkForComodification(); Object var2 = this.parent.remove(this.parentOffset + var1); this.modCount = this.parent.modCount; --this.size; return var2; &#125; protected void removeRange(int var1, int var2) &#123; this.checkForComodification(); this.parent.removeRange(this.parentOffset + var1, this.parentOffset + var2); this.modCount = this.parent.modCount; this.size -= var2 - var1; &#125; public boolean addAll(Collection&lt;? extends E&gt; var1) &#123; return this.addAll(this.size, var1); &#125; public boolean addAll(int var1, Collection&lt;? extends E&gt; var2) &#123; this.rangeCheckForAdd(var1); int var3 = var2.size(); if (var3 == 0) &#123; return false; &#125; else &#123; this.checkForComodification(); this.parent.addAll(this.parentOffset + var1, var2); this.modCount = this.parent.modCount; this.size += var3; return true; &#125; &#125; public Iterator&lt;E&gt; iterator() &#123; return this.listIterator(); &#125; public ListIterator&lt;E&gt; listIterator(final int var1) &#123; this.checkForComodification(); this.rangeCheckForAdd(var1); final int var2 = this.offset; return new ListIterator&lt;E&gt;() &#123; int cursor = var1; int lastRet = -1; int expectedModCount; &#123; this.expectedModCount = ArrayList.this.modCount; &#125; public boolean hasNext() &#123; return this.cursor != SubList.this.size; &#125; public E next() &#123; this.checkForComodification(); int var1x = this.cursor; if (var1x &gt;= SubList.this.size) &#123; throw new NoSuchElementException(); &#125; else &#123; Object[] var2x = ArrayList.this.elementData; if (var2 + var1x &gt;= var2x.length) &#123; throw new ConcurrentModificationException(); &#125; else &#123; this.cursor = var1x + 1; return var2x[var2 + (this.lastRet = var1x)]; &#125; &#125; &#125; public boolean hasPrevious() &#123; return this.cursor != 0; &#125; public E previous() &#123; this.checkForComodification(); int var1x = this.cursor - 1; if (var1x &lt; 0) &#123; throw new NoSuchElementException(); &#125; else &#123; Object[] var2x = ArrayList.this.elementData; if (var2 + var1x &gt;= var2x.length) &#123; throw new ConcurrentModificationException(); &#125; else &#123; this.cursor = var1x; return var2x[var2 + (this.lastRet = var1x)]; &#125; &#125; &#125; public void forEachRemaining(Consumer&lt;? super E&gt; var1x) &#123; Objects.requireNonNull(var1x); int var2x = SubList.this.size; int var3 = this.cursor; if (var3 &lt; var2x) &#123; Object[] var4 = ArrayList.this.elementData; if (var2 + var3 &gt;= var4.length) &#123; throw new ConcurrentModificationException(); &#125; else &#123; while(var3 != var2x &amp;&amp; SubList.this.modCount == this.expectedModCount) &#123; var1x.accept(var4[var2 + var3++]); &#125; this.lastRet = this.cursor = var3; this.checkForComodification(); &#125; &#125; &#125; public int nextIndex() &#123; return this.cursor; &#125; public int previousIndex() &#123; return this.cursor - 1; &#125; public void remove() &#123; if (this.lastRet &lt; 0) &#123; throw new IllegalStateException(); &#125; else &#123; this.checkForComodification(); try &#123; SubList.this.remove(this.lastRet); this.cursor = this.lastRet; this.lastRet = -1; this.expectedModCount = ArrayList.this.modCount; &#125; catch (IndexOutOfBoundsException var2x) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125; public void set(E var1x) &#123; if (this.lastRet &lt; 0) &#123; throw new IllegalStateException(); &#125; else &#123; this.checkForComodification(); try &#123; ArrayList.this.set(var2 + this.lastRet, var1x); &#125; catch (IndexOutOfBoundsException var3) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125; public void add(E var1x) &#123; this.checkForComodification(); try &#123; int var2x = this.cursor; SubList.this.add(var2x, var1x); this.cursor = var2x + 1; this.lastRet = -1; this.expectedModCount = ArrayList.this.modCount; &#125; catch (IndexOutOfBoundsException var3) &#123; throw new ConcurrentModificationException(); &#125; &#125; final void checkForComodification() &#123; if (this.expectedModCount != ArrayList.this.modCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125;; &#125; public List&lt;E&gt; subList(int var1, int var2) &#123; ArrayList.subListRangeCheck(var1, var2, this.size); return ArrayList.this.new SubList(this, this.offset, var1, var2); &#125; private void rangeCheck(int var1) &#123; if (var1 &lt; 0 || var1 &gt;= this.size) &#123; throw new IndexOutOfBoundsException(this.outOfBoundsMsg(var1)); &#125; &#125; private void rangeCheckForAdd(int var1) &#123; if (var1 &lt; 0 || var1 &gt; this.size) &#123; throw new IndexOutOfBoundsException(this.outOfBoundsMsg(var1)); &#125; &#125; private String outOfBoundsMsg(int var1) &#123; return \"Index: \" + var1 + \", Size: \" + this.size; &#125; private void checkForComodification() &#123; if (ArrayList.this.modCount != this.modCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; public Spliterator&lt;E&gt; spliterator() &#123; this.checkForComodification(); return new ArrayList.ArrayListSpliterator(ArrayList.this, this.offset, this.offset + this.size, this.modCount); &#125; &#125; private class ListItr extends ArrayList&lt;E&gt;.Itr implements ListIterator&lt;E&gt; &#123; ListItr(int var2) &#123; super(); this.cursor = var2; &#125; public boolean hasPrevious() &#123; return this.cursor != 0; &#125; public int nextIndex() &#123; return this.cursor; &#125; public int previousIndex() &#123; return this.cursor - 1; &#125; public E previous() &#123; this.checkForComodification(); int var1 = this.cursor - 1; if (var1 &lt; 0) &#123; throw new NoSuchElementException(); &#125; else &#123; Object[] var2 = ArrayList.this.elementData; if (var1 &gt;= var2.length) &#123; throw new ConcurrentModificationException(); &#125; else &#123; this.cursor = var1; return var2[this.lastRet = var1]; &#125; &#125; &#125; public void set(E var1) &#123; if (this.lastRet &lt; 0) &#123; throw new IllegalStateException(); &#125; else &#123; this.checkForComodification(); try &#123; ArrayList.this.set(this.lastRet, var1); &#125; catch (IndexOutOfBoundsException var3) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125; public void add(E var1) &#123; this.checkForComodification(); try &#123; int var2 = this.cursor; ArrayList.this.add(var2, var1); this.cursor = var2 + 1; this.lastRet = -1; this.expectedModCount = ArrayList.this.modCount; &#125; catch (IndexOutOfBoundsException var3) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125; private class Itr implements Iterator&lt;E&gt; &#123; int cursor; int lastRet = -1; int expectedModCount; Itr() &#123; this.expectedModCount = ArrayList.this.modCount; &#125; public boolean hasNext() &#123; return this.cursor != ArrayList.this.size; &#125; public E next() &#123; this.checkForComodification(); int var1 = this.cursor; if (var1 &gt;= ArrayList.this.size) &#123; throw new NoSuchElementException(); &#125; else &#123; Object[] var2 = ArrayList.this.elementData; if (var1 &gt;= var2.length) &#123; throw new ConcurrentModificationException(); &#125; else &#123; this.cursor = var1 + 1; return var2[this.lastRet = var1]; &#125; &#125; &#125; public void remove() &#123; if (this.lastRet &lt; 0) &#123; throw new IllegalStateException(); &#125; else &#123; this.checkForComodification(); try &#123; ArrayList.this.remove(this.lastRet); this.cursor = this.lastRet; this.lastRet = -1; this.expectedModCount = ArrayList.this.modCount; &#125; catch (IndexOutOfBoundsException var2) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125; public void forEachRemaining(Consumer&lt;? super E&gt; var1) &#123; Objects.requireNonNull(var1); int var2 = ArrayList.this.size; int var3 = this.cursor; if (var3 &lt; var2) &#123; Object[] var4 = ArrayList.this.elementData; if (var3 &gt;= var4.length) &#123; throw new ConcurrentModificationException(); &#125; else &#123; while(var3 != var2 &amp;&amp; ArrayList.this.modCount == this.expectedModCount) &#123; var1.accept(var4[var3++]); &#125; this.cursor = var3; this.lastRet = var3 - 1; this.checkForComodification(); &#125; &#125; &#125; final void checkForComodification() &#123; if (ArrayList.this.modCount != this.expectedModCount) &#123; throw new ConcurrentModificationException(); &#125; &#125; &#125;&#125; 小结在阅读源码的过程中， 我发现， 源码中使用System.arraycopy这个方法居多， 其次便是Arrays.copyOfSystem.arraycopy与Arrays.copyOf的区别是： arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置 copyOf()是系统自动在内部新建一个数组，并返回该数组。 ArrayList 核心扩容12345678910111213141516171819202122232425262728293031323334353637383940/*** 判断是否需要扩容*/private void ensureCapacityInternal(int var1) &#123; this.ensureExplicitCapacity(calculateCapacity(this.elementData, var1));&#125;private void ensureExplicitCapacity(int var1) &#123; ++this.modCount; if (var1 - this.elementData.length &gt; 0) &#123; this.grow(var1); &#125;&#125;/*** 扩容的核心算法*/private void grow(int var1) &#123; int var2 = this.elementData.length; // 对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 // 右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity /2。 int var3 = var2 + (var2 &gt;&gt; 1); if (var3 - var1 &lt; 0) &#123; var3 = var1; &#125; if (var3 - 2147483639 &gt; 0) &#123; var3 = hugeCapacity(var1); &#125; this.elementData = Arrays.copyOf(this.elementData, var3);&#125;private static int hugeCapacity(int var0) &#123; if (var0 &lt; 0) &#123; throw new OutOfMemoryError(); &#125; else &#123; return var0 &gt; 2147483639 ? 2147483647 : 2147483639; &#125;&#125; 来自JAVA Guid的demo1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class ArrayListDemo &#123; public static void main(String[] srgs)&#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); System.out.printf(\"Before add:arrayList.size() = %d\\n\",arrayList.size()); arrayList.add(1); arrayList.add(3); arrayList.add(5); arrayList.add(7); arrayList.add(9); System.out.printf(\"After add:arrayList.size() = %d\\n\",arrayList.size()); System.out.println(\"Printing elements of arrayList\"); // 三种遍历方式打印元素 // 第一种：通过迭代器遍历 System.out.print(\"通过迭代器遍历:\"); Iterator&lt;Integer&gt; it = arrayList.iterator(); while(it.hasNext())&#123; System.out.print(it.next() + \" \"); &#125; System.out.println(); // 第二种：通过索引值遍历 System.out.print(\"通过索引值遍历:\"); for(int i = 0; i &lt; arrayList.size(); i++)&#123; System.out.print(arrayList.get(i) + \" \"); &#125; System.out.println(); // 第三种：for循环遍历 System.out.print(\"for循环遍历:\"); for(Integer number : arrayList)&#123; System.out.print(number + \" \"); &#125; // toArray用法 // 第一种方式(最常用) Integer[] integer = arrayList.toArray(new Integer[0]); // 第二种方式(容易理解) Integer[] integer1 = new Integer[arrayList.size()]; arrayList.toArray(integer1); // 抛出异常，java不支持向下转型 //Integer[] integer2 = new Integer[arrayList.size()]; //integer2 = arrayList.toArray(); System.out.println(); // 在指定位置添加元素 arrayList.add(2,2); // 删除指定位置上的元素 arrayList.remove(2); // 删除指定元素 arrayList.remove((Object)3); // 判断arrayList是否包含5 System.out.println(\"ArrayList contains 5 is: \" + arrayList.contains(5)); // 清空ArrayList arrayList.clear(); // 判断ArrayList是否为空 System.out.println(\"ArrayList is empty: \" + arrayList.isEmpty()); &#125;&#125;","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"探究Spring Boot 核心技术","slug":"探究Spring-Boot-核心技术","date":"2019-03-06T12:59:45.000Z","updated":"2019-07-08T01:15:52.731Z","comments":true,"path":"2019/03/06/探究Spring-Boot-核心技术/","link":"","permalink":"https://wt-git-repository.github.io/2019/03/06/探究Spring-Boot-核心技术/","excerpt":"探究Spring BootPOM 文件1.父项目12345678910111213141516&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt;他的父项目是&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;&lt;/parent&gt;这个父项目是真正管理Spring Boot应用里面所有依赖版本,因为里面有一个dependency version, 为各种jar包定义了默认的版本号 Spring Boot的版本仲裁中心;以后我们导入依赖默认是不需要写版本的, (在没有dependencies里面管理的依赖自然要声明版本号)","text":"探究Spring BootPOM 文件1.父项目12345678910111213141516&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt;他的父项目是&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;&lt;/parent&gt;这个父项目是真正管理Spring Boot应用里面所有依赖版本,因为里面有一个dependency version, 为各种jar包定义了默认的版本号 Spring Boot的版本仲裁中心;以后我们导入依赖默认是不需要写版本的, (在没有dependencies里面管理的依赖自然要声明版本号) 2.起步依赖 Spring-boot-starter 场景启动器1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 举个例子,spring-boot-starter-web,帮我们导入web应用开发所需要的所有依赖 Spring boot 把所有的功能场景都抽取出来,做成一个个starters(启动器), 只需要在项目里面导入这些starters, 所有相关的依赖都会导入进来 一般需要什么功能就导入什么场景的启动器 主程序类,主入口类1234567891011/** * SpringBootApplication 来标注一个主程序类, 说明这是一个Spring Boot 应用 */@SpringBootApplicationpublic class SpringbootSampleApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootSampleApplication.class, args); &#125;&#125; @SpringBootApplication: Spring Boot应用标注在某个类上来说明这个类是Spring Boot的主配置类, Spring Boot就应该运行这个类的main方法来启动Spring Boot应用 12345678910111213141516171819/***SpringBootApplication 部分源码*/@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan( excludeFilters = &#123;@Filter( type = FilterType.CUSTOM, classes = &#123;TypeExcludeFilter.class&#125;), @Filter( type = FilterType.CUSTOM, classes = &#123;AutoConfigurationExcludeFilter.class&#125;)&#125;)public @interface SpringBootApplication &#123; @SpringBootConfiguration : Spring Boot的配置类 @EnableAutoConfiguration: 开启自动配置功能 1234567@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(&#123;AutoConfigurationImportSelector.class&#125;)public @interface EnableAutoConfiguration &#123; @AutoConfiguraPackage:将主配置类(@SpringBootApplication标注的类)的所在包以及下面所有子包里面的所有组件都扫描到Spring容器中, 这一点要特别注意!!! 举个例子 Spring Boot 目录结构 这里主要说resources文件夹下面的结构 static: 保存所以静态资源, 如js css images等等 templates: 保存所以模板页面, Spring Boot默认jar包使用嵌入式tomcat, 默认不支持JSP页面, 可以使用模板引擎(freemarker, 或者Spring Boot推荐的thymeleaf) application.properties : Spring Boot 的配置文件, 可以修改一些默认配置 Spring Boot配置文件1.配置文件Spring Boot 使用一个全局的配置文件, 配置文件名是固定的 application.properties application.yml (推荐使用) 配置文件的作用在于: 修改Spring Boot自动配置的默认值 2.YAML基本语法 键值对的形式, 以空格来控制层级关系 k:(空格)v 空格不能省, 大小写敏感 推荐使用YMAL来做配置文件, 例子 12server: port: 8081 YAML value的写法值得需要注意的地方 字符串默认不用加上单引号或者双引号 如果加上双引号, 则不会转义字符串里面的特殊字符, 如 “wt \\n wt” 输出的是 “wt \\n wt” 如果加上单引号, 则会转义字符串里面的特殊字符, 如 “wt \\n wt” 输出的是 “wt 换行 wt” 数组, 使用-值表示数组中的一个元素12345&gt; pets:&gt; - cats&gt; - dog&gt; - pig&gt; 3. YAML配置文件值注入 此处举例子进行说明 首先编写YAML文件 1234567person: name: wt age: 22 maps: &#123;k1: v1, k2: v2&#125; dog: name: my_dog age: 2 导入依赖 123456&lt;!--导入配置文件处理器--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; JAVABean 1234567891011121314151617181920/** * 将配置文件中配置的每一个属性的值,映射到这个组件中 * @ConfigurationProperties 告诉Spring Boot 将本类中的所有属性和配置文件中的相关配置进行绑定 * prefix = \"person\" 配置文件中哪一个属性进行绑定, 以person开头的值 * * 只有这个组件是容器中的组件,才能使用容器提供的@ConfigurationProperties功能 */@Component@ConfigurationProperties(prefix = \"person\")public class Person &#123; private String name; private Integer age; private Map&lt;String, Object&gt; maps; private Dog dog; ---&#125; JAVABean 123456public class Dog &#123; private String name; private Integer age; ---&#125; 测试 123456789101112131415@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootSampleApplicationTests &#123; @Autowired Person person; @Test public void contextLoads() &#123; System.out.println(person); &#125;&#125;输出 Person&#123;name='wt', age=22, maps=&#123;k1=v1, k2=v2&#125;, dog=Dog&#123;name='my_dog', age=2&#125;&#125; 4. @Value 和 @ConfigurationProperties的区别 @ConfigurationProperties @Value 功能 批量注入配置属性文件的属性 一个个指定 松散绑定(松散语法) 支持 不支持 SpringEL 不支持 支持 JR303数据校验 支持 不支持 复杂类型 支持 不支持 如果说, 我们只是需要在某个业务逻辑中获取一下配置文件的某个值, 则使用@Value, 可读取YAML文件1234567891011@RestControllerpublic class HelloController &#123; @Value(\"$&#123;person.name&#125;\") private String name; @RequestMapping(\"/sayHello\") public String sayHello() &#123; return \"hello \" + name; &#125;&#125; 如果需要与一个JAVABean进行映射,则使用@ConfigurationProperties来批量注入 5.介绍两个有用且相关的注解, @PropertySource 和 @ImportResource首先先介绍@PropertySource, 此处有一点值得注意的是,官网里面有这样一句话:YAML files cannot be loaded by using the @PropertySource annotation. So, in the case that you need to load values that way, you need to use a properties file. 意思就是说, @PropertySource只能用于.properties文件而不能用于YMAL文件, 所以, 此处我们还是举个例子来进行说明. 创建person.properties 文件 123456person.name=wtperson.age=22person.maps.k1=v1person.maps.k2=v2person.dog.name=my_dogperson.dog.age=2 修改Person类, 添加@PropertySource(value = {“classpath:person.properties”}) 123456789101112131415161718/** * 将配置文件中配置的每一个属性的值,映射到这个组件中 * @ConfigurationProperties 告诉Spring Boot 将本类中的所有属性和配置文件中的相关配置进行绑定 * prefix = \"person\" 配置文件中哪一个属性进行绑定 默认从全局配置获取值, 以person开头的值 * * 只有这个组件是容器中的组件,才能使用容器提供的@ConfigurationProperties功能 */@PropertySource(value = &#123;\"classpath:person.properties\"&#125;)@Component@ConfigurationProperties(prefix = \"person\")public class Person &#123; private String name; private Integer age; private Map&lt;String, Object&gt; maps; private Dog dog; --- 接下来, 介绍@ImportResource, 它的作用是引入Spring 配置文件, Spring Boot里面没有Spring的配置文件, 我们自己写的配置文件是不能被自动识别的, 此时, 便要使用@ImportResource, 放在一个配置类上举个例子 创建一个XML文件 1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"helloService\" class=\"com.example.springbootsample.service.HelloService\"&gt;&lt;/bean&gt;&lt;/beans&gt; 添加@ImportResource到主配置类中 123456789101112/** * SpringBootApplication 来标注一个主程序类, 说明这是一个Spring Boot 应用 */@ImportResource(locations = &#123;\"classpath:beans.xml\"&#125;)@SpringBootApplicationpublic class SpringbootSampleApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbootSampleApplication.class, args); &#125;&#125; 测试 12345678910111213141516171819@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringbootSampleApplicationTests &#123; @Autowired Person person; @Autowired ApplicationContext ioc; @Test public void testHelloService() &#123; boolean b = ioc.containsBean(\"helloService\"); System.out.println(b); &#125;&#125;输出true 然而, Spring Boot 不推荐使用XML, 而是使用全注解的方式来为容器添加组件 创建一个配置类 使用@Bean来给容器添加组件 12345678910111213141516171819202122package com.example.springbootsample.config;import com.example.springbootsample.service.HelloService;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * @Configuration 指明当前类是一个配置类, 来替代之前的Spring配置文件 * * 在Spring配置文件中, 这是用&lt;bean&gt;&lt;/bean&gt;标签来添加组件的 * */@Configurationpublic class MyAppConfiguration &#123; // 将方法的返回值添加到容器中, 容器中默认的组件id就是这个方法名helloService01 @Bean public HelloService helloService01() &#123; System.out.println(\"添加组件\"); return new HelloService(); &#125;&#125; 6.配置文件占位符 随机数 1$&#123;random.value&#125; $&#123;random.int&#125; $&#123;random.long&#125; $&#123;random.int[1024, 65536]&#125; 使用占位符获取之前配置的值, 如果没有, 则使用 : 指定默认值 123456person.name=wt$&#123;random.uuid&#125;person.age=22person.maps.k1=v1person.maps.k2=$&#123;person.hello:hello&#125;person.dog.name=my_dogperson.dog.age=2 7.ProfileProfile 是 Spring 对不同环境提供不同配置功能的支持, 可以通过激活或者指定参数等方式快速切换环境, 默认使用application.properties 多Profile文件形式: 格式: application-{profile}.properties/yml 如: application-dev.properties 激活指定的profile 在默认的配置文件中, 指定spring.profiles.ative=dev, 则会使用application-dev.properties 命令行的方式: –spring.profiles.ative=dev, 可以在java -jar 命令后面写入, 可以在idea tomcat那里配置 虚拟机参数-Dspring.profile.active=dev YAML可以用文档块来指定不同的环境, 如 123456789101112131415server: port: 8081spring: profiles: active: dev #激活dev环境, 如果不指明, 则使用默认的8081端口---server: port: 8082spring: profiles: dev---server: port: 8083spring: profiles: prod 8.配置文件的加载位置Spring Boot 启动会扫描以下位置的application.properties 或 application.yml文件作为spring boot 的配置文件, 优先级从高到底, 高优先级的内容会覆盖低优先级内容 file:./config/ file:./ classpath:/config/ classpath:/ 值得注意的是, Spring Boot会从这四个位置全部加载主配置文件, 且互补配置, 即, 高优先级有的, 使用高优先级, 高优先级没有的而低优先级有的, 则使用低优先级 9.外部配置文件的加载位置此处内容较多, 建议参考 官方文档中的Externalized Configuration 10.配置原理配置文件的配置内容, 参考官方文档附录Common application properties 11、@Conditional派生注解（Spring注解版原生的@Conditional作用）作用：必须是@Conditional指定的条件成立，才给容器中添加组件，配置配里面的所有内容才生效； @Conditional扩展注解 作用（判断是否满足当前指定条件） @ConditionalOnJava 系统的java版本是否符合要求 @ConditionalOnBean 容器中存在指定Bean； @ConditionalOnMissingBean 容器中不存在指定Bean； @ConditionalOnExpression 满足SpEL表达式指定 @ConditionalOnClass 系统中有指定的类 @ConditionalOnMissingClass 系统中没有指定的类 @ConditionalOnSingleCandidate 容器中只有一个指定的Bean，或者这个Bean是首选Bean @ConditionalOnProperty 系统中指定的属性是否有指定的值 @ConditionalOnResource 类路径下是否存在指定资源文件 @ConditionalOnWebApplication 当前是web环境 @ConditionalOnNotWebApplication 当前不是web环境 @ConditionalOnJndi JNDI存在指定项 12, 打印自动配置类启用报告在配置文件中加上debug=true属性控制台便会打印相关的信息1234567891011121314151617181920212223242526272829============================CONDITIONS EVALUATION REPORT============================Positive matches: (自动配置类启用的)----------------- AopAutoConfiguration matched: - @ConditionalOnClass found required classes 'org.springframework.context.annotation.EnableAspectJAutoProxy', 'org.aspectj.lang.annotation.Aspect', 'org.aspectj.lang.reflect.Advice', 'org.aspectj.weaver.AnnotatedElement' (OnClassCondition) - @ConditionalOnProperty (spring.aop.auto=true) matched (OnPropertyCondition) AopAutoConfiguration.CglibAutoProxyConfiguration matched: - @ConditionalOnProperty (spring.aop.proxy-target-class=true) matched (OnPropertyCondition) ---Negative matches: (自动配置类未启用的, 没有匹配成功)---------------- ActiveMQAutoConfiguration: Did not match: - @ConditionalOnClass did not find required class 'javax.jms.ConnectionFactory' (OnClassCondition) AopAutoConfiguration.JdkDynamicAutoProxyConfiguration: Did not match: - @ConditionalOnProperty (spring.aop.proxy-target-class=false) did not find property 'proxy-target-class' (OnPropertyCondition) ---","categories":[],"tags":[{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://wt-git-repository.github.io/tags/Spring-Boot/"}]},{"title":"JAVA内存区域与内存溢出异常","slug":"JAVA内存区域与内存溢出异常","date":"2019-03-06T12:58:35.000Z","updated":"2019-07-08T01:15:52.669Z","comments":true,"path":"2019/03/06/JAVA内存区域与内存溢出异常/","link":"","permalink":"https://wt-git-repository.github.io/2019/03/06/JAVA内存区域与内存溢出异常/","excerpt":"JAVA内存区域与内存溢出异常JAVA虚拟机管理下的内存包括如下几个运行时数据区域 程序计数器: 一块较小的内存空间,是当前线程所执行的字节码的行号指示器 每个线程都有独立的一个程序计数器,使得各个线程的切换可以恢复到正确的执行位置","text":"JAVA内存区域与内存溢出异常JAVA虚拟机管理下的内存包括如下几个运行时数据区域 程序计数器: 一块较小的内存空间,是当前线程所执行的字节码的行号指示器 每个线程都有独立的一个程序计数器,使得各个线程的切换可以恢复到正确的执行位置 JAVA虚拟机栈:描述的是JAVA方法执行的内存模型 每个方法在执行的同时都会创建一个栈帧(存储方法的相关信息) 一个方法从调用到执行的过程中就相当于一个栈帧在虚拟机栈中的入栈和出栈的过程 许多人喜欢把JAVA内存分为堆内存和栈内存(当然这不是研究的分法,只是与对象内存分配关系最密切的内存区域就是这两块),其中栈内存指的就是虚拟机栈中的局部变量表部分 虚拟机栈的局部变量表存放编译期可知的各种基本数据类型,对象引用以及returnAddress类型 这里指的注意的是:局部变量表所需的内存空间在编译期间完成分配,当进入一个方法时,这个方法需要在帧中分配多大的局部变量空间是完全确定的,这个方法运行期间不会改变局部变量表的大小 对于上一点,在JVM规范中定义了两种异常状况:① 如果线程请求的栈深度大于虚拟机所允许的深度,将抛出StackOverflowError的错误;② 如果虚拟机栈可以动态扩展,只是扩展时无法申请到足够的内存,就会抛出OutOfMemoryError异常. 本独方法栈: 与虚拟机栈的作用类似, 区别在于本地方法栈为虚拟机使用到的Native方法服务 该栈也会抛出StackOverflowError与OutOfMemoryError异常 JAVA堆: 存放对象实例,被所有线程共享 JVM所管理的内存中最大的一块 JAVA堆是垃圾收集器管理的主要区域, 很多时候被称作为”GC堆” 方法区 用于存储已被虚拟机加载的类方法,常量,静态变量,即时编译器编译后的代码等数据 运行时常量池 方法区的一部分,Class文件除了描述信息之外,还有一个常量池,用于存放编译期生成的各种字面量和符号引用,这部分内容将在类加载后进入方法区的运行时常量池存放 具备动态性,不要求常量一定是在编译器中产生,也可以在运行期间将新的常量放入池中, 如String类中的intern()方法 内存饱满是也会抛出OutOfMemoryError方法 直接内存 HotSpot虚拟机对象对象的创建 对象创建 (在语言层面上,创建对象通常相当于一个new关键字) 当虚拟机遇到一条new指令时,首先会去检查这个指令的参数是否能在常量池中定位到一个类的符号引用,并且检查这个符号引用代表的类是否已被加载或解析或初始化过,如果没有,那必须先执行相应的类加载过程 检查通过后,虚拟机为新生对象分配内存,对象所需要的内存的大小在类加载完成后便可完全确定 为对象分配内存就相当于把一块确定大小的内存从JAVA堆中划分出来 执行完new指令后,接着执行方法,把对象按照程序员的意愿进行初始化,这样一个真正可用的对象才算完全产生出来 对象的内存布局 在HotSpot虚拟机中,对象在内存中存储的布局可分为三块区域:对象头,实例数据和对齐填充 对象头包括两部分信息:① 存储对象自身的运行时数据, ② 类型指针(指向类元数据的指针, 虚拟机通过这个指针来确定这个对象是哪个类的实例, 如果对象是数组, 则对象头中还必须有一块用于记录数组长度的数据) 实例数据:对象头真正存储的有效信息,也是代码中所定义的各种类型的字段内容","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]},{"title":"docker| 数据管理","slug":"docker-数据管理","date":"2019-03-06T12:57:57.000Z","updated":"2019-07-08T01:15:52.700Z","comments":true,"path":"2019/03/06/docker-数据管理/","link":"","permalink":"https://wt-git-repository.github.io/2019/03/06/docker-数据管理/","excerpt":"数据管理在容器中管理数据的方式主要有两种 数据卷挂载主机目录 数据卷 定义：数据卷是一个可以供一个或者多个容器使用的特殊目录 特点 可以在容器之间共享和重用对数据卷的修改会马上生效对数据卷的更新不会影响镜像数据卷会一直存在，即使容器被删除","text":"数据管理在容器中管理数据的方式主要有两种 数据卷挂载主机目录 数据卷 定义：数据卷是一个可以供一个或者多个容器使用的特殊目录 特点 可以在容器之间共享和重用对数据卷的修改会马上生效对数据卷的更新不会影响镜像数据卷会一直存在，即使容器被删除 创建一个数据卷 命令 sudo docker volume create [volume_name] 查看所有的数据卷 命令 sudo docker volume ls 查看指定数据卷的信息-命令 sudo docker volume inspect [volume_name] 启动一个加载数据卷的容器 命令 sudo docker run -d \\-P 81:80 \\–name web \\–mount source=[volume_name],target=[target_name] \\[Image_name] -d后台运行 -P端口映射 –mount将[volume_name]数据卷加载到容器的[target_name]目录下 [Image_name]镜像名 app.py主体程序 查看数据卷的具体信息 命令 sudo docker inspect [container_name] 删除镜像 命令 sudo docker volume rm [volume_name] 挂载主机目录挂载一个主机目录作为数据卷 命令 sudo docker run -d \\-P 81:80 \\–name web \\-v /src/webapp:/opt/webapp [Image_name] Docker挂载目录默认的权限是读写，ro是只读","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://wt-git-repository.github.io/tags/docker/"}]},{"title":"docker| 操作容器","slug":"docker-操作容器","date":"2019-03-06T12:57:36.000Z","updated":"2019-07-08T01:15:52.699Z","comments":true,"path":"2019/03/06/docker-操作容器/","link":"","permalink":"https://wt-git-repository.github.io/2019/03/06/docker-操作容器/","excerpt":"操作容器启动容器 启动容器有两种方式 基于镜像新建一个容器并启动 在终止状态（stopped）的容器重新启动 新建启动并在后台运行 命令 sudo docker run －d –name [name] 镜像名[:标签] -d：后台运行，启动后会进入容器 –name可为容器取名，等同与容器ID 输出结果可以用 docker logs 查看","text":"操作容器启动容器 启动容器有两种方式 基于镜像新建一个容器并启动 在终止状态（stopped）的容器重新启动 新建启动并在后台运行 命令 sudo docker run －d –name [name] 镜像名[:标签] -d：后台运行，启动后会进入容器 –name可为容器取名，等同与容器ID 输出结果可以用 docker logs 查看 命令:docker logs [OPTIONS] [container ID or NAMES]-f：跟踪日志输出 使用docker run创建容器是，Docker在后台运行的标准操作包括:检查本地是否存在指定的镜像，不存在就从公有仓库下载利用镜像创建并启动一个容器分配一个文件系统，并在只读的镜像层外面挂载一层可读写层从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去从地址池配置一个 ip 地址给容器执行用户指定的应用程序执行完毕后容器被终止 终止运行中的容器 命令 sudo docker container stop &lt;Container_ID&gt; 查看容器信息 命令 sudo docker container ls 加-a 可以查看被终止的容器信息 启动被终止的容器 命令 sudo docker container start &lt;Container_ID&gt; 重启容器 命令 sudo docker container restart 进入容器 命令 docker exec -it &lt;Container_ID&gt; bash exit命令退出 导出容器 命令 docker export &lt;Container_ID&gt; &gt; &lt;local_file_name&gt;例如：docker export 7691a814370e &gt; ubuntu.tar 导入容器 命令 cat &lt;local_file_name&gt; | docker import - &lt;仓库名&gt;:例如：cat ubuntu.tar | docker import - test/ubuntu:v1.0ordocker import http://example.com/exampleimage.tgz example/imagerepo 删除容器 命令 sudo docker rm -v [container_name] -v删除容器的同时移除数据卷","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://wt-git-repository.github.io/tags/docker/"}]},{"title":"docker| 基本概念","slug":"docker-基本概念","date":"2019-03-06T12:56:53.000Z","updated":"2019-07-08T01:15:52.698Z","comments":true,"path":"2019/03/06/docker-基本概念/","link":"","permalink":"https://wt-git-repository.github.io/2019/03/06/docker-基本概念/","excerpt":"dockerwhat is docker Docker是基于Go语言开发实现的，是一种对进程进行封装隔离，属于操作系统层面的虚拟化技术 由于隔离的进程独立于宿主和其它隔离的进程，Docker也因此被称为容器 The discrimination between virtual machines and docker 传统虚拟机技术是虚拟出一套硬件，在其运行一个完整的操作系统，然后在这个系统运行所需的应用进程 Docker的应用进程是直接运行为宿主的内核上，容器内没有自己的内核，更没有硬件虚拟 Docker容器比传统的虚拟机更为轻便","text":"dockerwhat is docker Docker是基于Go语言开发实现的，是一种对进程进行封装隔离，属于操作系统层面的虚拟化技术 由于隔离的进程独立于宿主和其它隔离的进程，Docker也因此被称为容器 The discrimination between virtual machines and docker 传统虚拟机技术是虚拟出一套硬件，在其运行一个完整的操作系统，然后在这个系统运行所需的应用进程 Docker的应用进程是直接运行为宿主的内核上，容器内没有自己的内核，更没有硬件虚拟 Docker容器比传统的虚拟机更为轻便 The discrimination between virtual machines and docker The advantage of Docker 容器不需要硬件模拟或者运行操作系统等额外开外，故相比与传统的虚拟机技术，一个相同配置的主机，可以运行更多数量的容器 容器直接运行于宿主内核，无需启动完整的操作系统，因此可以做到毫秒级的启动 容器可以提供一致的开发环境 使用容器可以定制镜像来实现持续集成，交付和部署 Docker Image Docker镜像(Image)，就相当于是一个root文件系统 镜像使用的是分层存储 例如：官方镜像 ubuntu:16.04 就包含了完整的一套 Ubuntu 16.04 最小系统的 root 文件系统 Docker Container 镜像(Image)和容器(Container)的关系，就相当于面向对象程序设计的类和实例一样 镜像是静态的定义 容器是镜像运行时的实体 容器可以被创建，启动，停止，删除，暂停等等 容器是一个运行在隔离环境下的进程，与其它进程不同的是，容器可以拥有只属于自己的root文件系统,自己的网络配置，自己的进程空间以及自己的ID空间等等 如同镜像一样，容器也是使用分层存储，每一个容器运行时，以镜像为基础层，在其上创建一个当前容器的存储层，可称为容器运行时而准备的存储层，又称容器存储层 容器消亡时，容器存储层也随之消亡，故，容器不应向存储层写入任何数据需保持无状态化 所以的文件写入操作，都应该使用数据卷,或者绑定宿主目录,在这些位置的读写会跳过容器存储层，直接对宿主或者网络存储发生读写 容器消亡，数据卷不会消亡，使用数据卷后，容器删除或者重新运行之后，数据不会丢失 Docker Register 一个 Docker Registry 中可以包含多个仓库（Repository） 每个仓库可以包含多个标签（Tag）,每个标签对应一个镜像，如版本标签,如果忽略标签，则使用latest作为默认标签 Docker Hub 阿里云加速器","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://wt-git-repository.github.io/tags/docker/"}]},{"title":"docker| 镜像使用","slug":"docker-镜像使用","date":"2019-03-06T12:56:30.000Z","updated":"2019-07-08T01:15:52.701Z","comments":true,"path":"2019/03/06/docker-镜像使用/","link":"","permalink":"https://wt-git-repository.github.io/2019/03/06/docker-镜像使用/","excerpt":"使用镜像从 Docker 镜像仓库获取镜像的命令 命令格式 sudo docker pull [选项] [url : 端口号]/镜像名[:标签] 此处的url 默认为Docker Hub 标签可忽略，默认为latest 举个例子 $ docker pull ubuntu:16.04","text":"使用镜像从 Docker 镜像仓库获取镜像的命令 命令格式 sudo docker pull [选项] [url : 端口号]/镜像名[:标签] 此处的url 默认为Docker Hub 标签可忽略，默认为latest 举个例子 $ docker pull ubuntu:16.04 运行容器（容器是镜像的一个实例） 输入命令 sudo docker run -it -rm 镜像名[:标签] bash -it：这里包括两个参数 -i：交互式参数-t：终端 –rm：这个参数是说容器退出之后随之将其删除 bash启动交互式shell exit命令退出 列出镜像 命令 sudo docker images 列表包含了 仓库名（镜像名）、标签、镜像 ID、创建时间 以及 所占用的空间 镜像 ID 则是镜像的唯一标识 一个镜像可以对应多个标签 docker images 列表中的镜像体积总和并非是所有镜像实际硬盘消耗 因为不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多 镜像体积 宿主标识的镜像空间和Docker Hub上的不同，Docker Hub上的是经过压缩的 命令：docker system df可查看镜像、容器、数据卷所占用的空间 虚悬镜像 这个镜像既没有仓库名，也没有标签，均为 删除镜像 命令 sudo docker image rm &lt;Image_ID&gt; ID一般取前3个字符以上，只要足够区分于别的镜像便可成功删除 可以添加 -f 参数,可删除正在运行的容器 docker container prune 命令可以清理所有处于终止状态的容器","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://wt-git-repository.github.io/tags/docker/"}]},{"title":"Dockerfile 相关指令","slug":"Dockerfile-相关指令","date":"2019-03-06T12:55:54.000Z","updated":"2019-07-08T01:15:52.669Z","comments":true,"path":"2019/03/06/Dockerfile-相关指令/","link":"","permalink":"https://wt-git-repository.github.io/2019/03/06/Dockerfile-相关指令/","excerpt":"Dockerfile指令全解FROM 指定基础镜像注：scratch是空白镜像如：FROM mysql","text":"Dockerfile指令全解FROM 指定基础镜像注：scratch是空白镜像如：FROM mysql RUN 执行命令每一个run指令都会新建一层，并在其上执行这些命令，执行接收，commit这一层的修改，便构成了新的镜像如:RUN apt-get update一般来说，只需构建一层便可举个例子FROM debian:jessie RUN buildDeps=’gcc libc6-dev make’ \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get install -y $buildDeps \\ 构建镜像 在Dockerfile文件所在的目录执行命令docker build [选项] &lt;上下文路径/URL/-&gt;例如 docker build -t nginx:v3 .其中 －t nginx:v3 是指定镜像名称 最后一个 . 是指当前目录","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://wt-git-repository.github.io/tags/docker/"}]},{"title":"JAVA 编程规约","slug":"JAVA-编程规约","date":"2019-03-06T12:54:51.000Z","updated":"2019-07-08T01:15:52.669Z","comments":true,"path":"2019/03/06/JAVA-编程规约/","link":"","permalink":"https://wt-git-repository.github.io/2019/03/06/JAVA-编程规约/","excerpt":"编程规约(一) 命名风格 代码中的命名均不能以 下划线或美元符号 开始， 也不能以 下划线或美元符号 结束 代码中的命名禁止使用拼音与中文混合的方式， 应当使用正确的英文拼写，杜绝完全不规范的缩写，避免望文不知义, 为了达到代码自解释的目标，任何自定义编程元素在命名时，使用尽量完整的单词组合来表达其意。注 : alibaba/taobao等国际通用的名称，可视同英文","text":"编程规约(一) 命名风格 代码中的命名均不能以 下划线或美元符号 开始， 也不能以 下划线或美元符号 结束 代码中的命名禁止使用拼音与中文混合的方式， 应当使用正确的英文拼写，杜绝完全不规范的缩写，避免望文不知义, 为了达到代码自解释的目标，任何自定义编程元素在命名时，使用尽量完整的单词组合来表达其意。注 : alibaba/taobao等国际通用的名称，可视同英文 类名使用UpperCamelCase风格，但以下情形例外：DO/ BO / DTO/ VO/ AO/ PO/ UID等。如UserDO 方法名、参数名、成员变量、局部变量都统一使用lowerCamelCase风格，必须遵从驼峰形式。 常量命名全部大写，单词间用下划线隔开，力求语义表达完整清楚，不要嫌名字长。 抽象类命名使用Abstract或Base开头 异常类命名使用Exception结尾 测试类命名以它要测试的类的名称开始，以Test结尾。 定义整形数组int[] arrayDemo POJO类中布尔类型的变量，都不要加is前缀，否则部分框架解析会引起序列化错误。（重要） 包名统一使用小写, 单数形式 类名如果有复数含义，类名可以使用复数形式 如果模块、接口、类、方法使用了设计模式，在命名时需体现出具体模式。如 public classOrder Factory； public class LoginProxy； 接口类的方法和属性不要添加任何的修饰符号(public 也不能添加), 以求保持代码的简洁性能，而且接口类的变量均为与接口方法相关的常量． 接口和实现类的命名有两套规则 对于Service和DAO类, 暴露出来的服务一定是接口，而其中的实现类命名以Impl为后缀 枚举类名建议带上Enum后缀，枚举成员名称需要全大写，单词间用下划线隔开 各层命名规约整合 Service/DAO层方法命名规约 获取单个对象的方法用get做前缀获取多个对象的方法用list做前缀，复数形式结尾如：listObjects获取统计值的方法用count做前缀插入的方法用save/insert做前缀删除的方法用remove/delete做前缀修改的方法用update做前缀 领域模型命名规约 数据对象：xxxDO，xxx即为数据表名数据传输对象：xxxDTO，xxx为业务领域相关的名称展示对象：xxxVO，xxx一般为网页名称POJO是DO/DTO/BO/VO的统称，禁止命名成xxxPOJO (二) 常量定义 不允许任何魔法值（即未经预先定义的常量）直接出现在代码中 在long或者Long赋值时，数值后使用大写的L如: public Long data = 2L; 不要使用一个常量类维护所有常量，要按常量功能进行归类，分开维护。 常量的复用层次有五层：跨应用共享常量、应用内共享常量、子工程内共享常量、包内共享常量、类内共享常量。 跨应用共享常量：放置在二方库中，通常是client.jar中的constant目录下。应用内共享常量：放置在一方库中，通常是子模块中的constant目录下。子工程内部共享常量：即在当前子工程的constant目录下。包内共享常量：即在当前包下单独的constant目录下。类内共享常量：直接在类内部private static final定义。 如果变量值仅在一个固定范围内变化用enum类型来定义注：注意区分常量类和枚举类的区别 (三) 代码格式对于代码格式的规范，阿里巴巴给出了一个正例，这个例子基本囊括了所以基本的规范操作1234567891011121314151617public class Main &#123; public static void main(String[] args) &#123; // 注释的双斜线与注释内容之间有且仅有一个空格。 // 缩进四格 String say = \"hello\"; // 运算符左右必须有一个空格 int flag = 0; // if语句与括号之间必须有一个空格, 左大括号前加空格不换行， 左大括号后换行 if (flag == 0) &#123; System.out.println(say); //右大括号前换行，右大括号有else， 不用换行 &#125; else &#123; System.out.println(\"ok\"); //右大括号结束必须换行 &#125; &#125;&#125; 补充 单行字符数限制不超过120个，超出需要换行，换行时遵循如下原则 第二行相对第一行缩进4个空格，从第三行开始，不再继续缩进运算符与下文一起换行方法调用的点符号与下文一起换行方法调用中的多个参数需要换行时，在逗号后进行在括号前不要换行 方法参数在定义和传入时，多个参数逗号后边必须加空格，如method(args1, args2, args3); IDE的textfileencoding设置为UTF-8;IDE中文件的换行符使用Unix格式，不要使用Windows格式。 单个方法的总行数不超过80行 不同逻辑、不同语义、不同业务的代码之间插入一个空行分隔开来以提升可读性。 (四) OOP(面向对象程序设计)规约 避免通过一个类的对象引用访问此类的静态变量或静态方法，无谓增加编译器解析成本，直接用类名来访问即可。 所有的覆写方法，必须加@Override注解。 相同参数类型，相同业务含义，才可以使用Java的可变参数，避免使用Object 不能使用过时的类或方法, 接口过时必须加@Deprecated注解，作为调用方来说，有义务去考证过时方法的新实现是什么 Object的equals方法容易抛空指针异常，应使用常量或确定有值的对象来调用equals，如”test”.equals(object)此处，阿里巴巴推荐使用java.util.Objects#equals（JDK7引入的工具类） 所有的相同类型的包装类对象之间值的比较，全部使用equals方法比较。 所有的POJO类属性必须使用包装数据类型 定义DO/DTO/VO等POJO类时，不要设定任何属性默认值 POJO类必须写toString方法 序列化类新增属性时，请不要修改serialVersionUID字段，避免反序列失败；如果完全不兼容升级，避免反序列化混乱，那么请修改serialVersionUID值。 构造方法里面禁止加入任何业务逻辑，如果有初始化逻辑，请放在init方法中。 禁止在POJO类中，同时存在对应属性xxx的isXxx()和getXxx()方法 使用索引访问用String的split方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛IndexOutOfBoundsException的风险。 当一个类有多个构造方法，或者多个同名方法，这些方法应该按顺序放置在一起，便于阅读，此条规则优先于第16条规则 类内方法定义的顺序依次是：公有方法或保护方法&gt; 私有方法&gt; getter/setter方法说明：公有方法是类的调用者和维护者最关心的方法，首屏展示最好；保护方法虽然只是子类关心，也可能是“模板设计模式”下的核心方法；而私有方法外部一般不需要特别关心，是一个黑盒实现；因为承载的信息价值较低，所有Service和DAO的getter/setter方法放在类体最后 setter方法中，参数名称与类成员变量名称一致，this.成员名=参数名。在getter/setter方法中，不要增加业务逻辑，增加排查问题的难度。 循环体内，字符串的连接方式，使用StringBuilder的append方法进行扩展，避免资源浪费 final可以声明类、成员变量、方法、以及本地变量，下列情况使用final关键字 不允许被继承的类，如：String类。不允许修改引用的域对象。不允许被重写的方法，如：POJO类的setter方法。 不允许运行过程中重新赋值的局部变量.避免上下文重复使用一个变量，使用final描述可以强制重新定义一个变量，方便更好地进行重构。 慎用Object的clone方法来拷贝对象，因为clone方法默认的是浅拷贝 类成员与方法访问控制从严 如果不允许外部直接通过new来创建对象，那么构造方法必须是private。工具类不允许有public或default构造方法类非static成员变量并且与子类共享，必须是protected类非static成员变量并且仅在本类使用，必须是private类static成员变量如果仅在本类使用，必须是private若是static成员变量，考虑是否为final类成员方法只供类内部调用，必须是private类成员方法只对继承类公开，那么限制为protected 注:任何类、方法、参数、变量，严控访问范围。过于宽泛的访问范围，不利于模块解耦。 (五) 集合处理 当equals()方法被override时，hashCode()也要被override。按照一般hashCode()方法的实现来说，相等的对象，它们的hash code一定相等。 因为Set存储的是不重复的对象，依据hashCode和equals进行判断，所以Set存储的对象必须重写这两个方法如果自定义对象作为Map的键，那么必须重写hashCode和equalsString重写了hashCode和equals方法，所以我们可以非常愉快地使用String对象作为key来使用 ArrayList的subList结果不可强转成ArrayList，否则会抛出ClassCastException subList返回的是ArrayList的内部类SubList，并不是ArrayList而是ArrayList的一个视图，对于SubList子列表的所有操作最终会反映到原列表上在subList场景中，高度注意对原集合元素的增加或删除，均会导致子列表的遍历、增加、删除产生ConcurrentModificationException异常。 使用集合转数组的方法，必须使用集合的toArray(T[]array)，传入的是类型完全一样的数组，大小就是list.size() 直接使用toArray无参方法存在问题，此方法返回值只能是Object[]类，若强转其它类型数组将出现ClassCastException错误。正例:String[] array = new String[list.size()];array = list.toArray(array); 使用工具类Arrays.asList()把数组转换成集合时，不能使用其修改集合相关的方法，它的add/remove/clear方法会抛出UnsupportedOperationException异常 asList的返回对象是一个Arrays内部类，并没有实现集合的修改方法。Arrays.asList体现的是适配器模式，只是转换接口，后台的数据仍是数组String[] str = new String[] { “you”, “wu” };Listlist = Arrays.asList(str);第一种情况：list.add(“yangguanbao”); 运行时异常。第二种情况：str[0]= “gujin”;那么list.get(0)也会随之修改。 泛型通配符&lt;? extendsT&gt;来接收返回的数据，此写法的泛型集合不能使用add方法，而&lt;? superT&gt;不能使用get方法，作为接口调用赋值时易出错 繁往外读取内容的，适合用&lt;? extendsT&gt;经常往里插入的，适合用&lt;? superT&gt; 不要在foreach循环里进行元素的remove/add操作。remove元素请使用Iterator方式，如果并发操作，需要对Iterator对象加锁。iterator.remove. 在JDK7版本及以上，Comparator实现类要满足如下三个条件，不然Arrays.sort，Collections.sort会报IllegalArgumentException异常。 x，y的比较结果和y，x的比较结果相反x&gt;y，y&gt;z，则x&gt;zx=y，则x，z比较结果和y，z比较结果相同 集合泛型定义时，在JDK7及以上，使用diamond语法或全省略。说明：菱形泛型，即diamond，直接使用&lt;&gt;来指代前边已经指定的类型。 1234// &lt;&gt; diamond方式HashMap&lt;String, String&gt; userCache = new HashMap&lt;&gt;(16);// 全省略方式ArrayList&lt;User&gt; users = new ArrayList(10); 集合初始化时，指定集合初始值大小。 initialCapacity=(需要存储的元素个数/负载因子) + 1。注意负载因子（即loader factor）默认为0.75，如果暂时无法确定初始值大小，请设置为16（即默认值）。 使用entrySet遍历Map类集合KV，而不是keySet方式进行遍历 keySet其实是遍历了2次，一次是转为Iterator对象，另一次是从hashMap中取出key所对应的value。而entrySet只是遍历了一次就把key和value都放到了entry中，效率更高。如果是JDK8，使用Map.foreach方法 高度注意Map类集合K/V能不能存储null值的情况 利用Set元素唯一的特性，可以快速对一个集合进行去重操作，避免使用List的contains方法进行遍历、对比、去重操作。 合理利用好集合的有序性(sort)和稳定性(order)，避免集合的无序性(unsort)和不稳定性(unorder)带来的负面影响 有序性是指遍历的结果是按某种比较规则依次排列的稳定性指集合每次遍历的元素次序是一定的如：ArrayList是order/unsort；HashMap是unorder/unsort；TreeSet是order/sort (六) 并发处理 获取单例对象需要保证线程安全，其中的方法也要保证线程安全 资源驱动类、工具类、单例工厂类都需要注意。 创建线程或线程池时请指定有意义的线程名称，方便出错时回溯 线程资源必须通过线程池提供，不允许在应用中自行显式创建线程 线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式 SimpleDateFormat是线程不安全的类，一般不要定义为static变量，如果定义为static，必须加锁，或者使用DateUtils工具类。 如果是JDK8的应用，可以使用Instant代替Date，LocalDateTime代替Calendar，DateTimeFormatter代替SimpleDateFormat，官方给出的解释：simplebeautifulstrongimmutablethread-safe 高并发时，同步调用应该去考量锁的性能损耗。能用无锁数据结构，就不要用锁；能锁区块，就不要锁整个方法体；能用对象锁，就不要用类锁 尽可能使加锁的代码块工作量尽可能的小，避免在锁代码块中调用RPC方法 对多个资源、数据库表、对象同时加锁时，需要保持一致的加锁顺序，否则可能会造成死锁 举个例子：线程一需要对表A、B、C依次全部加锁后才可以进行更新操作，那么线程二的加锁顺序也必须是A、B、C，否则可能出现死锁。 并发修改同一记录时，避免更新丢失，需要加锁。要么在应用层加锁，要么在缓存加锁，要么在数据库层使用乐观锁，使用version作为更新依据 如果每次访问冲突概率小于20%，推荐使用乐观锁，否则使用悲观锁。乐观锁的重试次数不得小于3次 多线程并行处理定时任务时，Timer运行多个TimeTask时，只要其中之一没有捕获抛出的异常，其它任务便会自动终止运行，使用ScheduledExecutorService则没有这个问题 使用CountDownLatch进行异步转同步操作，每个线程退出前必须调用countDown方法，线程执行代码注意catch异常，确保countDown方法被执行到，避免主线程无法执行至await方法，直到超时才返回结果 注意，子线程抛出异常堆栈，不能在主线程try-catch到 避免Random实例被多线程使用，虽然共享该实例是线程安全的，但会因竞争同一seed导致的性能下降 在JDK7之后，可以直接使用APIThreadLocalRandom，而在JDK7之前，需要编码保证每个线程持有一个实例 在并发场景下，通过双重检查锁（double-checkedlocking）实现延迟初始化的优化问题隐患(可参考The”Double-CheckedLockingisBroken” Declaration)，推荐解决方案中较为简单一种（适用于JDK5及以上版本），将目标属性声明为volatile型 HashMap在容量不够进行resize时由于高并发可能出现死链，导致CPU飙升，在开发过程中可以使用其它数据结构或加锁来规避此风险 ThreadLocal无法解决共享对象的更新问题，ThreadLocal对象建议使用static修饰。这个变量是针对一个线程内所有操作共享的，所以设置为静态变量，所有此类实例共享此静态变量，也就是说在类第一次被使用时装载，只分配一块存储空间，所有此类的对象(只要是这个线程内定义的)都可以操控这个变量(六) 控制语句 在一个switch块内，每个case要么通过break/return等来终止，要么注释说明程序将继续执行到哪一个case为止；在一个switch块内，都必须包含一个default语句并且放在最后，即使空代码 在if/else/for/while/do语句中必须使用大括号。 在高并发场景中，避免使用”等于”判断作为中断或退出的条件 表达异常的分支时，少用if-else方式，这种方式可以改写成12345if (condition) &#123;...return obj;&#125;// 接着写else的业务逻辑代码; 如果非得使用if()…else if()…else…方式表达逻辑，【强制】避免后续代码维护困难，请勿超过3层超过3层的if-else的逻辑判断代码可以使用卫语句、策略模式、状态模式等来实现 其中卫语句示例如下：1234567891011121314public void today() &#123;if (isBusy()) &#123;System.out.println(“change time.”);return;&#125;if (isFree()) &#123;System.out.println(“go to travel.”);return;&#125;System.out.println(“stay at home to learn Alibaba Java Coding Guidelines.”);return;&#125; 除常用方法（如getXxx/isXxx）等外，不要在条件判断中执行其它复杂的语句，将复杂逻辑判断的结果赋值给一个有意义的布尔变量名，以提高可读性。如： 1finalboolean existed = (file.open(fileName, &quot;w&quot;) != null) &amp;&amp;(...) || (...);finalboolean existed = (file.open(fileName, &quot;w&quot;) != null) &amp;&amp;(...) || (...); 循环体中的语句要考量性能，以下操作尽量移至循环体外处理，如定义对象、变量、获取数据库连接，进行不必要的try-catch操作（这个try-catch是否可以移至循环体外）。 避免采用取反逻辑运算符 – ! 接口入参保护，这种场景常见的是用作批量操作的接口 下列情形，需要进行参数校验 调用频次低的方法执行时间开销很大的方法。此情形中，参数校验时间几乎可以忽略不计，但如果因为参数错误导致中间执行回退，或者错误，那得不偿失需要极高稳定性和可用性的方法对外提供的开放接口，不管是RPC/API/HTTP接口敏感权限入口 下列情形，不需要进行参数校验 极有可能被循环调用的方法。但在方法说明里必须注明外部参数检查要求底层调用频度比较高的方法。毕竟是像纯净水过滤的最后一道，参数错误不太可能到底层才会暴露问题。一般DAO层与Service层都在同一个应用中，部署在同一台服务器中，所以DAO的参数校验，可以省略被声明成private只会被自己代码所调用的方法，如果能够确定调用方法的代码传入参数已经做过检查或者肯定不会有问题，此时可以不校验参数 (八) 代码规约 类、类属性、类方法的注释必须使用Javadoc规范，使用/*内容/格式，不得使用//xxx方式。 在IDE编辑窗口中，Javadoc方式会提示相关注释，生成Javadoc可以正确输出相应注释；在IDE中，工程调用方法时，不进入方法即可悬浮提示方法、参数、返回值的意义，提高阅读效率 所有的抽象方法（包括接口中的方法）必须要用Javadoc注释、除了返回值、参数、异常说明外，还必须指出该方法做什么事情，实现什么功能。 所有的类都必须添加创建者和创建日期 方法内部单行注释，在被注释语句上方另起一行，使用//注释。方法内部多行注释使用/ /注释，注意与代码对齐 所有的枚举类型字段必须要有注释，说明每个数据项的用途 与其“半吊子”英文来注释，不如用中文注释把问题说清楚。专有名词与关键字保持英文原文即可 代码修改的同时，注释也要进行相应的修改，尤其是参数、返回值、异常、核心逻辑等的修改 注释的要求： 第一、能够准确反应设计思想和代码逻辑第二、能够描述业务含义，使别的程序员能够迅速了解到代码背后的信息第三、注释力求精简准确、表达到位, 避免泛滥 特殊注释标记，请注明标记人与标记时间。注意及时处理这些标记，通过标记扫描，经常清理此类标记。线上故障有时候就是来源于这些标记处的代码 待办事宜（TODO）:（标记人，标记时间，[预计处理时间]）错误，不能工作（FIXME）:（标记人，标记时间，[预计处理时间]） (九)其它 在使用正则表达式时，利用好其预编译功能，可以有效加快正则匹配速度 说明：不要在方法体内定义：Patternpattern= Pattern.compile(“规则”) velocity调用POJO类的属性时，建议直接使用属性名取值即可，模板引擎会自动按规范调用POJO的getXxx()，如果是boolean基本数据类型变量（boolean命名不需要加is前缀），会自动调用isXxx()方法 后台输送给页面的变量必须加$!{var}——中间的感叹号 如果var等于null或者不存在，那么${var}会直接显示在页面上 注意Math.random()这个方法返回是double类型，注意取值的范围0≤x&lt;1（能够取到零值，注意除零异常），如果想获取整数类型的随机数,直接使用Random对象的nextInt或者nextLong方法 获取当前毫秒数System.currentTimeMillis();而不是newDate().getTime(); 如果想获取更加精确的纳秒级时间值，使用System.nanoTime()的方式。在JDK8中，针对统计时间等场景，推荐使用Instant类 不要在视图模板中加入任何复杂的逻辑 根据MVC理论，视图的职责是展示，不要抢模型和控制器的活 任何数据结构的构造或初始化，都应指定大小，避免数据结构无限增长吃光内存 及时清理不再使用的代码段或配置信息","categories":[],"tags":[{"name":"JAVA","slug":"JAVA","permalink":"https://wt-git-repository.github.io/tags/JAVA/"}]}]}